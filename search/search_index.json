{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Recent articles \u00b6 Bad algorithms The anatomy of a successful research software project Coming soon \u00b6 How not to provide data Future possibilities \u00b6 Why computers should never be used to do maths Blockchain authentication","title":"Recent articles"},{"location":"#recent-articles","text":"Bad algorithms The anatomy of a successful research software project","title":"Recent articles"},{"location":"#coming-soon","text":"How not to provide data","title":"Coming soon"},{"location":"#future-possibilities","text":"Why computers should never be used to do maths Blockchain authentication","title":"Future possibilities"},{"location":"about/","text":"About me \u00b6 Physicist turned investment banker turned academic researcher. Mission: software and practices that promote reproducibility, reusability and efficiency in academic research Currently: Research Fellow at the University of Leeds, using microsimulation and agent-based models to investigate the interaction between Police resourcing (supply) and crime (demand). Selected open-source projects \u00b6 neworder : a high-performance framework for dynamic microsimulation models humanleague : a microsynthesis package using quasirandom sampling and iterative propotional fitting (IPF) ukcensusapi : UK Census Data queries and downloads from python or R ukpopulation : download, cache, collate, filter, manipulate and extrapolate UK population and household estimates/projections Qualifications \u00b6 Publications \u00b6 Top tip, Viz , c.2002","title":"About me"},{"location":"about/#about-me","text":"Physicist turned investment banker turned academic researcher. Mission: software and practices that promote reproducibility, reusability and efficiency in academic research Currently: Research Fellow at the University of Leeds, using microsimulation and agent-based models to investigate the interaction between Police resourcing (supply) and crime (demand).","title":"About me"},{"location":"about/#selected-open-source-projects","text":"neworder : a high-performance framework for dynamic microsimulation models humanleague : a microsynthesis package using quasirandom sampling and iterative propotional fitting (IPF) ukcensusapi : UK Census Data queries and downloads from python or R ukpopulation : download, cache, collate, filter, manipulate and extrapolate UK population and household estimates/projections","title":"Selected open-source projects"},{"location":"about/#qualifications","text":"","title":"Qualifications"},{"location":"about/#publications","text":"Top tip, Viz , c.2002","title":"Publications"},{"location":"anatomy/","text":"The anatomy of a successful research software project \u00b6 Motivations \u00b6 If you publish research that requires you to write a significant amount of code to produce results, then you probably (hopefully) put the code into source control in a public repository. Many journals now require, or at the very least, encourage this practice. But if you think this alone is enough to make your results reproducible by your peers, then think again. Why is this? Here's a few possible reasons another researcher might not be able to first replicate your results and then go on to use your software to extend the research: it isn't clear how you run/use the code the code isn't packaged and thus difficult to install correctly, if at all the code doesn't work on their machine the code runs, but the results are different the code works - the results are the same - but it's not at all clear how you would go about using the package to advance or modify the published work Even if a researcher successfully replicates your results, uses, and then and cites your software (somehow), subsequent readers of their research will likely confront the same problems. Often, documentation and testing are overlooked or given a low priority because they \"slow you down\". To this, I say more haste less speed . You'll save more time in future development than you'll lose by producing some documentation and a test framework. Whilst all developers test their code as they write it, not all developers keep those tests and run them regularly. How many times have you changed some code that's had unintended consequences in another part of your software, creating a bug that's gone unnoticed because you didn't keep the original tests, and now you have no idea exactly what is broken or quite why it no longer works? It is also important that your software is open to scrutiny by others. This essentially exposes your methodology at a level of detail generally impractical to describe fully in a paper. If there are any flaws in your methodology, or even bugs, then they are open for peer review. Conversely, aspects of your implementation may be of benefit to other researchers, even if the overall package isn't as relevant to them. All of the potential problems listed above can be solved to a lesser or greater extent using free and open tools that integrate into your project. Where I mention a specific product, it's not necessarily a recommendation, it just happens to be the tool I tend to use. And in most cases other (and perhaps better) products are available. Here's how it all fits together: One you integrate these tools into your ecosystem, and once set up require little or no maintenance. They'll save you enormous amounts of time and effort by providing forewarning of any issues, as well as providing a framework enabling developers to collaborate effectively, and users to feed back into the project by reporting bugs or suggesting improvements. Some, or perhaps all, of what follows may be obvious to some readers and it's largely common sense, but I'm describing the methodology from the ground up for completeness, and for those who are new to the concept of providing software research tools to a wide audience. Developer(s) \u00b6 Even if you work on your own, entertain the possibility that someone, somewhere, may one day want to collaborate on the project. Consider also that there may be long periods where you're not actively developing the software, during which you'll likely forget some of the detail of how it works, what the dependencies are, or how you test it's working correctly. This means documentation, even if it's just some brief notes for yourself. But you may as well write it for a wider audience. I've seen plenty of projects on github that I would've like to contribute to, but didn't as there weren't clear instructions on basic usage or instructions on to how to set up a development environment, and a well-documented test framework. If you want collaborators, you're more likely to get them if they are comfortable they can add value without breaking anything. The obvious place to put this documentation is in the repo itself, starting with README.md . Github (for example) renders markdown into nicely (if somewhat spartanly) formatted web pages, is simple to master, and is well worth getting up to speed in if you haven't already: see e.g. markdown cheatsheet . Source control \u00b6 In the corporate world, teams of developers work very closely together, and often exclusively on a single project, and in this context it often makes sense for them to all work directly in the same repo - when things go wrong they are quickly apparent and quickly resolved. In academia, however, the situation is very different: collaborations are much looser and people will be dipping in an out of various projects. Your collaborators may be people you barely know, geographically separated, and (most crucially) not reporting to the same boss as you! The best model for this situation is the \"fork and pull request\" practice. In other words, you control the \"root\" repo for the project, and only you (or a select few) have permission to commit to this repository. If somebody else wants to collaborate, they \"fork\" your repo, essentially taking a copy of it, do their development in their repo, and when ready they submit a \"pull request\" (PR) which invites you to review their changes and decide if you want to merge their contribution into your repo. Github also has tools in place to ensure their changes don't break anything (see Testing ). This practice can also be used within a single repo, with people developing on branches and submitting PRs merging to the master branch. A note here about good practice - it's generally not a good idea to commit directly to master , which should be the golden copy of your code. Development should be carried out on branches and merged to master when complete and fully tested. In practice, when I'm the only developer on a project I tend to commit small and uncontroversial \"atomic\" changes directly to master , but always use branches for larger and/or potentially breaking changes. Once a branch has been merged to master, it is no longer required and should be deleted. Issues and project management \u00b6 Github has the facility to create \"issues\" against a repo which are very useful for reporting bugs and tracking development. Issues can be categorised, assigned to people, assigned a status, assigned to a particular release, and discussed and commented on. Whilst the functionality is reasonably basic compared to other (often paid for) products, it has the advantage of being completely integrated into the project, and there are tools available to enhance the functionality, such as Zenhub , which adds full Agile project management via a browser plugin. For academic projects with a relatively small number of collaborators, this is probably more than sufficient. Commit messages can be automatically linked to specific issues simply by referencing the issue number prefixed with a '#', which is useful for tracking work done on a particular issue. Additionally, you can automatically close issues via a commit that addresses the issue, by including text like \"fixes #47\" or \"closes #47\" in the body. Looking at the issues boards of other repos is a good (rough) guide to see how robust the software is, how many issues are reported by users (a lot could indicate either wide usage or buggy software, or both!) and how responsive the developers are at addressing them. Testing \u00b6 It goes without saying that you should test your code, have a test harness of some form, and append the test code for any new features to the test suite. And obviously you always run the tests before pushing to the github repo, and when you do your CI (see next section ) runs more comprehensive tests. This section is more concerned more about how you go about automating this process, how to test in a wider range of environments, and how to determine if your tests are giving you a good indication of how robust your software is. Continuous integration \u00b6 Simply put, you register with a continuous integration (CI) provider (e.g. travis , appveyor , circleci ) and point them to your github project and specify some configuration for each provider so they know how to build and test your project. Then, every time you commit, the CI will build and test your code for you and tell you if there's a problem. The first thing this does is give you an assurance that your repo is complete and self-contained - if you've forgotten to commit a file, or if there's a dependency you haven't explicitly specified but happens to be installed on your machine, you'll know pretty soon. Ok, so you've been convinced that regression tests are a good thing (if you weren't already), and you're developing a python project on your linux machine, which has python 3.7 on it, and your tests are fairly comprehensive to you're confident it works... but only on linux, and only with python 3.7. This is where CI comes in - you can configure it to build on multiple platforms and multiple software versions. Different CI providers support different platforms so you may well need to register with multiple providers to cover the major platforms, e.g. Windows, OSX and linux. In terms of software versions aim for 3 - e.g. for R, they recommend testing against the previous release, the current release, and the release in development. Obviously if you do find a problem with a specific platform/version, you may then need to get hold of a machine with that configuration to track down the problem, but often a painstaking trawl through the CI log files is enough to pinpoint the problem. Writing configuration files for CI providers (typically in yaml) is something of a dark art. There are plenty of examples to be found on github and elsewhere though. Test coverage \u00b6 As software evolves, the usage patterns change and the code gets changed or added to but often the tests don't. Thus its easy to stumble into a situation where the code that's being tested is not a good representation of the code that's executed by users. Again there are online tools that integrate with your repo and report area of your codebase that aren't being adequately tested. Static analysis \u00b6 No amount of runtime testing can guarantee to weed out all code errors, so it's important to analyse your code for dubious constructs that cause subtle bugs. This is commonly known as static analysis , since its done without actually running the code. For compiled code, this is doubly important, as there could be undefined behaviour lurking in the code which may (by pure luck) appear to work 99% of the time but cause the application to error or even crash seemingly randomly, for example by accidentally reading over the end of an array. As with CI, online tools are available that can be integrated with the github repository and will automatically analyse each commit and report any issues. They also typically grade your code for quality, which you can advertise on your repo: a high grade will again encourage people to trust (and therefore use) your package, and may also help attract contributors. Codacy is one such provider, and it combines static analysis tools for a variety of languages, sometimes with multiple tools for one language. Often static analysis tools will seem overly pedantic and seem to report a lot of false positives, but sometimes they are highlighting a subtle issue that requires some thought to understand. Otherwise, spurious and overly pedantic issues can be suppressed. Versioning and releases \u00b6 So far all we've done is lay some foundations for good software development but havent really got to grips with the problems outlined at the start. Now we can start getting to the point of this methodology. Journal articles generally have a pretty slow turnaround. If you submit a manuscript containing a reference to some software in a github repository, then it will be at least 3 months and possibly a lot longer before that article is published. In the meantime, unless you've completely stopped development, the software in your repo could have changed substantially, and may not even be able to produce the results you described in the article, so it will be very difficult for any other researcher to replicate your results, short of bisecting the revision history, hoping for some informative commit comments, and guessing the approximate date you ran the code that generated the results you published. A time-consuming and potentially fruitless process for anyone. There is a simple solution, you simply version your software, and refer to a specific version in any published work. git has a concept called a tag, which just identifies a specific commit in a repo with a convenient name. In turn you can use this tag to create a release on github, which creates a zipped copy of the code at the tag, and allows you to create some release notes. Examining commit comments and closed issues since the previous release is a good starting point for making release notes. There are a number of different standards for versioning software, perhaps the most common is semantic versioning (aka semver) which is described here . So if you quoted your package and a version in your article, you've gone a long way to making your work reproducible by others. But there are easier and less error-prone ways of installing software than downloading a zip file, extracting it and manually copying it to a particular location. If you want to upgrade or remove it then will you remember exactly what you did to \"install\" it in the first place? Documentation \u00b6 I've already mentioned the README, and this is a great place to introduce your package and put some brief notes on installation and basic usage. But its important also to provide more detailed examples and, ideally, API documentation. Depending on the size/complexity of your package you might be able to put this all in README.md but it probably makes sense to split it over multiple markdown files. Github markdown rendering is fairly spartan and has limited support for things like tables of contents and equations, if your documentation requires them. If you want very professional-looking documentation it is worth looking into rendering packages that convert markdown to styled html, such as Sphinx or MkDocs , which give you things like themes, a search tool, and tables of contents etc, straight out of the box. The generated documentation can be hosted (free) on github as project pages, or externally on sites such as readthedocs . I personally use mkdocs (in fact this site uses it), which is relatively easy to get up and running, supports configurable themes and supports thing like embedding (\\latex) (for equations) and (python) macros to programmatically generate content. Examples \u00b6 Providing worked examples is arguably the most important part of the documentation as its likely to be the first port of call for new users. The documentation macro feature (in e.g. mkdocs ) is particularly useful for examples, as you can pull in the actual code (rather than having to remember to manually copy/paste the example code in the documentation every time it changes). You can even go as far as actually running example code, inserting the output into the documentation as its generated. This has the added bonus of ensuring your example code works with the current version of the package. You are effectively testing your documentation, a process known as doctest . Some languages (e.g R, rust) support this natively. API documentation \u00b6 Once users are actively integrating your package into their work, they will likely want more detail on how to use specific classes or functions. This is where API documentation comes in. You want to automate this as much as you can so it reflects the current API and manually generating it and keeping it up-to-date is a manual, laborious (and thus error-prone) process. Some documentation packages, or plugins for the packages, purport to be able to automatically generate documentation from your package, but you will probably find that your mileage may vary. In the worst case you can write a script to translate the raw package documentation ( docstrings in python) into markdown. Not ideal, but still better than a purely manual process. Package repositories \u00b6 If you want users to be able to install your software easily, which translates pretty much to \"if you want users\", then you're going to need to package your software and upload it to a public repository. How straightforward this process is depends very much on the repository. In my experience, python package index (PyPI) is the easiest and CRAN (the R repository) the most onerous. PyPI seemingly accepts pretty much anything (including broken packages), whereas CRAN is extremely strict about the package contents and documentation. In between you have things like conda-forge, which will test the package installs on a number of different platforms before it accepts it. Obviously it's imperative that you ensure that you have fully tested the version you are publishing. Container repositories \u00b6 An alternative way of delivering software is to provide it in the form of a container, a.k.a. a docker image. This is in essence a lightweight but full (linux) operating system image pre-installed with your software and all its dependencies. This can be extremely useful if your package has a web API, i.e. an \"app service\" and allows it to be easily and quickly deployed almost anywhere. It is also useful for providing an environment containing ready-to-run examples that aren't part of the installed package. You provide a recipe in the form of a Dockerfile which is essentially a sequence of commands on how to install - from a base OS - all your package's dependencies, build it, set up an environment, and then run it. Another advantage is if you your software requires large datasets from various sources, you can provide a pre-packaged docker volume containing all the data (as long as its publicly available and you have got any necessary permissions to replicate it, of course). Docker hub is one such repository, which (as always) integrates with github and can be configured to automatically generate images for you, including tags (and thus releases). Citations \u00b6 Citations nowadays include a unique Document Origin Identifier (DOI), and this by no means applies only to academic papers. It's also used for both software and data. Thus, assigning a DOI to a specific release of your software constitutes a reference which anyone should be able to trace. What happens if you publish a paper referencing your software, including a version, and then subsequently move the repo elsewhere or even delete it, or pull the release from public repositories? This will leave readers of your article unable to find your work. The beauty of a DOI (and the provider thereof) is that it doesn't matter (so much). Zenodo is a service that (you guessed it) integrates with github and takes a complete snapshot copy of your software, and thus persists even if the original repo or package no longer exists. I would suggest you make it as easy as possible for people to cite you, providing a bibtex-style reference and/or a link to Zenodo (or similar) in your documentation can help. After all, it's people (including yourself, see below ) citing your software that makes it maintaining it worthwhile. Licencing \u00b6 Another aspect of acknowledgement your work is others using your source code, as opposed to your software. This is where licencing comes in. It is important you understand your rights as a a developer of open-source software, and also the rights of users of your software. By licencing your package you protect yourself from public liability and also (typically) legally require anyone to acknowledge you if they base their own software on yours. Most open source licences are quite permissive, even to the point where a third party is allowed to sell your software, as long as your were acknowledged. Github, for example, provides a number of licence templates you can select from, select the one which best suits your situation. Communication \u00b6 For promotion \u00b6 Users and practitioners need to be aware that your software exists. A simple and relatively easy way of promoting it is to publish an article in, for example, the Journal of Open Source Software or the Journal of Open Research Software , neither of which require a full paper, just simply a brief summary of what the package does. Their review process focussed on the software and if you've followed the recommendations here you should have little trouble being accepted. The impact, however, may not be great (who trawls there journals in the hope a package they could use suddenly appears?), so if you can also cite it yourself in an applied paper this will help greatly. Likewise, conferences are a good place to plug your development work, as well as simply encouraging collaborators and colleagues to use (and then cite) it. For development \u00b6 After configuring the tools in your ecosystem to listen for changes in repo, and respond automatically to them, they will typically email you if there's a problem. These days this isn't so reliable as most people are bombarded by emails and read few of them. Perhaps a better way is to use a messaging service (such as slack , as always others are available). Slack can be used as a central point for any issues coming from the services around your package, and of course can also be a place where developers communicate. Slack has many custom integrations which you can add to your channel, such as being able to create and amend github issues directly, and where custom integrations are not available you cant fairly easily create your own via webhooks. The slack channel(s) can also be a good place for communication with end users, which brings me on to... Support \u00b6 It's difficult to know how many people actively use your package (short of incorporating spyware into it). Whilst package repositories will produce a count (or rate) of downloads, I suspect at least some are bots, and of the others, downloading is one thing but actually using is another. But lets assume you have active users, and you have good documentation. You may find that a lot of the support questions could be answered by simply reading the documentation (who's not guilty of this?), or are not specific enough for you to replicate the problem they report. Some form of triage is useful if you get a lot of support questions. You can use an \"issue template\" where users must complete certain questions, describing in detail what they were trying to do, what went wrong, and on which version and platform. This deters people who realise they should look again at the documentation, and stops people raising vague issues along the lines of \"I tried using X but it didn't work\", that simply don't contain enough information to replicate. At the very least, you should make it clear that you cannot/will not help anyone unless they can come up with a reproducible example. After all, how do you fix a bug if you can't actually replicate the bug? Takeaways \u00b6 This may seem like an awful lot of effort to go to, but it does save potentially a lot of future headaches, and it doesn't need to all be done at once. Start with CI, and add other services/features as and when they become useful. Good practice in software- and data-driven research is becoming ever more important and is being taken more and more seriously by more traditional academics. The growth of Research Software Engineering as a discipline in it's own right speaks to this. If I had to summarise what I've written into some very brief key lessons, they would be: if you want other people to use and cite your software, you have do do a lot more than just writing the software, but there is payback all the tools you need are there, you just need to figure out how to use them (if you don't know already) automate the sh!t out of everything you can. It will be less error prone and save you (potentially lots of) time going forward. Scripts are your friend anticipate failure, test everything. The sooner you know something is broken, the better Room for improvement \u00b6 In what I've described, the developer has the ability to release directly from their own development environment to public package repositories. This is error-prone and open to abuse. I could accidentally (or deliberately) create a public release of my package that's not in source control and thus circumvent all the good work around testing and versioning. A better approach would be to release via continuous deployment (CD), a logical extension of CI. In the commercial world there's typically a clear segregation of roles (dev, infrastructure, production/users) but in academia you are often all 3 of them. Implementing CD would at least eliminate accidents, another approach would be for package repositories to only accept releases from verified sources, e.g. releases created on github. In practice \u00b6 To see (most of) this in action \"in the wild\", take a look around the neworder project.","title":"Anatomy of a successful research software project"},{"location":"anatomy/#the-anatomy-of-a-successful-research-software-project","text":"","title":"The anatomy of a successful research software project"},{"location":"anatomy/#motivations","text":"If you publish research that requires you to write a significant amount of code to produce results, then you probably (hopefully) put the code into source control in a public repository. Many journals now require, or at the very least, encourage this practice. But if you think this alone is enough to make your results reproducible by your peers, then think again. Why is this? Here's a few possible reasons another researcher might not be able to first replicate your results and then go on to use your software to extend the research: it isn't clear how you run/use the code the code isn't packaged and thus difficult to install correctly, if at all the code doesn't work on their machine the code runs, but the results are different the code works - the results are the same - but it's not at all clear how you would go about using the package to advance or modify the published work Even if a researcher successfully replicates your results, uses, and then and cites your software (somehow), subsequent readers of their research will likely confront the same problems. Often, documentation and testing are overlooked or given a low priority because they \"slow you down\". To this, I say more haste less speed . You'll save more time in future development than you'll lose by producing some documentation and a test framework. Whilst all developers test their code as they write it, not all developers keep those tests and run them regularly. How many times have you changed some code that's had unintended consequences in another part of your software, creating a bug that's gone unnoticed because you didn't keep the original tests, and now you have no idea exactly what is broken or quite why it no longer works? It is also important that your software is open to scrutiny by others. This essentially exposes your methodology at a level of detail generally impractical to describe fully in a paper. If there are any flaws in your methodology, or even bugs, then they are open for peer review. Conversely, aspects of your implementation may be of benefit to other researchers, even if the overall package isn't as relevant to them. All of the potential problems listed above can be solved to a lesser or greater extent using free and open tools that integrate into your project. Where I mention a specific product, it's not necessarily a recommendation, it just happens to be the tool I tend to use. And in most cases other (and perhaps better) products are available. Here's how it all fits together: One you integrate these tools into your ecosystem, and once set up require little or no maintenance. They'll save you enormous amounts of time and effort by providing forewarning of any issues, as well as providing a framework enabling developers to collaborate effectively, and users to feed back into the project by reporting bugs or suggesting improvements. Some, or perhaps all, of what follows may be obvious to some readers and it's largely common sense, but I'm describing the methodology from the ground up for completeness, and for those who are new to the concept of providing software research tools to a wide audience.","title":"Motivations"},{"location":"anatomy/#developers","text":"Even if you work on your own, entertain the possibility that someone, somewhere, may one day want to collaborate on the project. Consider also that there may be long periods where you're not actively developing the software, during which you'll likely forget some of the detail of how it works, what the dependencies are, or how you test it's working correctly. This means documentation, even if it's just some brief notes for yourself. But you may as well write it for a wider audience. I've seen plenty of projects on github that I would've like to contribute to, but didn't as there weren't clear instructions on basic usage or instructions on to how to set up a development environment, and a well-documented test framework. If you want collaborators, you're more likely to get them if they are comfortable they can add value without breaking anything. The obvious place to put this documentation is in the repo itself, starting with README.md . Github (for example) renders markdown into nicely (if somewhat spartanly) formatted web pages, is simple to master, and is well worth getting up to speed in if you haven't already: see e.g. markdown cheatsheet .","title":"Developer(s)"},{"location":"anatomy/#source-control","text":"In the corporate world, teams of developers work very closely together, and often exclusively on a single project, and in this context it often makes sense for them to all work directly in the same repo - when things go wrong they are quickly apparent and quickly resolved. In academia, however, the situation is very different: collaborations are much looser and people will be dipping in an out of various projects. Your collaborators may be people you barely know, geographically separated, and (most crucially) not reporting to the same boss as you! The best model for this situation is the \"fork and pull request\" practice. In other words, you control the \"root\" repo for the project, and only you (or a select few) have permission to commit to this repository. If somebody else wants to collaborate, they \"fork\" your repo, essentially taking a copy of it, do their development in their repo, and when ready they submit a \"pull request\" (PR) which invites you to review their changes and decide if you want to merge their contribution into your repo. Github also has tools in place to ensure their changes don't break anything (see Testing ). This practice can also be used within a single repo, with people developing on branches and submitting PRs merging to the master branch. A note here about good practice - it's generally not a good idea to commit directly to master , which should be the golden copy of your code. Development should be carried out on branches and merged to master when complete and fully tested. In practice, when I'm the only developer on a project I tend to commit small and uncontroversial \"atomic\" changes directly to master , but always use branches for larger and/or potentially breaking changes. Once a branch has been merged to master, it is no longer required and should be deleted.","title":"Source control"},{"location":"anatomy/#issues-and-project-management","text":"Github has the facility to create \"issues\" against a repo which are very useful for reporting bugs and tracking development. Issues can be categorised, assigned to people, assigned a status, assigned to a particular release, and discussed and commented on. Whilst the functionality is reasonably basic compared to other (often paid for) products, it has the advantage of being completely integrated into the project, and there are tools available to enhance the functionality, such as Zenhub , which adds full Agile project management via a browser plugin. For academic projects with a relatively small number of collaborators, this is probably more than sufficient. Commit messages can be automatically linked to specific issues simply by referencing the issue number prefixed with a '#', which is useful for tracking work done on a particular issue. Additionally, you can automatically close issues via a commit that addresses the issue, by including text like \"fixes #47\" or \"closes #47\" in the body. Looking at the issues boards of other repos is a good (rough) guide to see how robust the software is, how many issues are reported by users (a lot could indicate either wide usage or buggy software, or both!) and how responsive the developers are at addressing them.","title":"Issues and project management"},{"location":"anatomy/#testing","text":"It goes without saying that you should test your code, have a test harness of some form, and append the test code for any new features to the test suite. And obviously you always run the tests before pushing to the github repo, and when you do your CI (see next section ) runs more comprehensive tests. This section is more concerned more about how you go about automating this process, how to test in a wider range of environments, and how to determine if your tests are giving you a good indication of how robust your software is.","title":"Testing"},{"location":"anatomy/#continuous-integration","text":"Simply put, you register with a continuous integration (CI) provider (e.g. travis , appveyor , circleci ) and point them to your github project and specify some configuration for each provider so they know how to build and test your project. Then, every time you commit, the CI will build and test your code for you and tell you if there's a problem. The first thing this does is give you an assurance that your repo is complete and self-contained - if you've forgotten to commit a file, or if there's a dependency you haven't explicitly specified but happens to be installed on your machine, you'll know pretty soon. Ok, so you've been convinced that regression tests are a good thing (if you weren't already), and you're developing a python project on your linux machine, which has python 3.7 on it, and your tests are fairly comprehensive to you're confident it works... but only on linux, and only with python 3.7. This is where CI comes in - you can configure it to build on multiple platforms and multiple software versions. Different CI providers support different platforms so you may well need to register with multiple providers to cover the major platforms, e.g. Windows, OSX and linux. In terms of software versions aim for 3 - e.g. for R, they recommend testing against the previous release, the current release, and the release in development. Obviously if you do find a problem with a specific platform/version, you may then need to get hold of a machine with that configuration to track down the problem, but often a painstaking trawl through the CI log files is enough to pinpoint the problem. Writing configuration files for CI providers (typically in yaml) is something of a dark art. There are plenty of examples to be found on github and elsewhere though.","title":"Continuous integration"},{"location":"anatomy/#test-coverage","text":"As software evolves, the usage patterns change and the code gets changed or added to but often the tests don't. Thus its easy to stumble into a situation where the code that's being tested is not a good representation of the code that's executed by users. Again there are online tools that integrate with your repo and report area of your codebase that aren't being adequately tested.","title":"Test coverage"},{"location":"anatomy/#static-analysis","text":"No amount of runtime testing can guarantee to weed out all code errors, so it's important to analyse your code for dubious constructs that cause subtle bugs. This is commonly known as static analysis , since its done without actually running the code. For compiled code, this is doubly important, as there could be undefined behaviour lurking in the code which may (by pure luck) appear to work 99% of the time but cause the application to error or even crash seemingly randomly, for example by accidentally reading over the end of an array. As with CI, online tools are available that can be integrated with the github repository and will automatically analyse each commit and report any issues. They also typically grade your code for quality, which you can advertise on your repo: a high grade will again encourage people to trust (and therefore use) your package, and may also help attract contributors. Codacy is one such provider, and it combines static analysis tools for a variety of languages, sometimes with multiple tools for one language. Often static analysis tools will seem overly pedantic and seem to report a lot of false positives, but sometimes they are highlighting a subtle issue that requires some thought to understand. Otherwise, spurious and overly pedantic issues can be suppressed.","title":"Static analysis"},{"location":"anatomy/#versioning-and-releases","text":"So far all we've done is lay some foundations for good software development but havent really got to grips with the problems outlined at the start. Now we can start getting to the point of this methodology. Journal articles generally have a pretty slow turnaround. If you submit a manuscript containing a reference to some software in a github repository, then it will be at least 3 months and possibly a lot longer before that article is published. In the meantime, unless you've completely stopped development, the software in your repo could have changed substantially, and may not even be able to produce the results you described in the article, so it will be very difficult for any other researcher to replicate your results, short of bisecting the revision history, hoping for some informative commit comments, and guessing the approximate date you ran the code that generated the results you published. A time-consuming and potentially fruitless process for anyone. There is a simple solution, you simply version your software, and refer to a specific version in any published work. git has a concept called a tag, which just identifies a specific commit in a repo with a convenient name. In turn you can use this tag to create a release on github, which creates a zipped copy of the code at the tag, and allows you to create some release notes. Examining commit comments and closed issues since the previous release is a good starting point for making release notes. There are a number of different standards for versioning software, perhaps the most common is semantic versioning (aka semver) which is described here . So if you quoted your package and a version in your article, you've gone a long way to making your work reproducible by others. But there are easier and less error-prone ways of installing software than downloading a zip file, extracting it and manually copying it to a particular location. If you want to upgrade or remove it then will you remember exactly what you did to \"install\" it in the first place?","title":"Versioning and releases"},{"location":"anatomy/#documentation","text":"I've already mentioned the README, and this is a great place to introduce your package and put some brief notes on installation and basic usage. But its important also to provide more detailed examples and, ideally, API documentation. Depending on the size/complexity of your package you might be able to put this all in README.md but it probably makes sense to split it over multiple markdown files. Github markdown rendering is fairly spartan and has limited support for things like tables of contents and equations, if your documentation requires them. If you want very professional-looking documentation it is worth looking into rendering packages that convert markdown to styled html, such as Sphinx or MkDocs , which give you things like themes, a search tool, and tables of contents etc, straight out of the box. The generated documentation can be hosted (free) on github as project pages, or externally on sites such as readthedocs . I personally use mkdocs (in fact this site uses it), which is relatively easy to get up and running, supports configurable themes and supports thing like embedding (\\latex) (for equations) and (python) macros to programmatically generate content.","title":"Documentation"},{"location":"anatomy/#examples","text":"Providing worked examples is arguably the most important part of the documentation as its likely to be the first port of call for new users. The documentation macro feature (in e.g. mkdocs ) is particularly useful for examples, as you can pull in the actual code (rather than having to remember to manually copy/paste the example code in the documentation every time it changes). You can even go as far as actually running example code, inserting the output into the documentation as its generated. This has the added bonus of ensuring your example code works with the current version of the package. You are effectively testing your documentation, a process known as doctest . Some languages (e.g R, rust) support this natively.","title":"Examples"},{"location":"anatomy/#api-documentation","text":"Once users are actively integrating your package into their work, they will likely want more detail on how to use specific classes or functions. This is where API documentation comes in. You want to automate this as much as you can so it reflects the current API and manually generating it and keeping it up-to-date is a manual, laborious (and thus error-prone) process. Some documentation packages, or plugins for the packages, purport to be able to automatically generate documentation from your package, but you will probably find that your mileage may vary. In the worst case you can write a script to translate the raw package documentation ( docstrings in python) into markdown. Not ideal, but still better than a purely manual process.","title":"API documentation"},{"location":"anatomy/#package-repositories","text":"If you want users to be able to install your software easily, which translates pretty much to \"if you want users\", then you're going to need to package your software and upload it to a public repository. How straightforward this process is depends very much on the repository. In my experience, python package index (PyPI) is the easiest and CRAN (the R repository) the most onerous. PyPI seemingly accepts pretty much anything (including broken packages), whereas CRAN is extremely strict about the package contents and documentation. In between you have things like conda-forge, which will test the package installs on a number of different platforms before it accepts it. Obviously it's imperative that you ensure that you have fully tested the version you are publishing.","title":"Package repositories"},{"location":"anatomy/#container-repositories","text":"An alternative way of delivering software is to provide it in the form of a container, a.k.a. a docker image. This is in essence a lightweight but full (linux) operating system image pre-installed with your software and all its dependencies. This can be extremely useful if your package has a web API, i.e. an \"app service\" and allows it to be easily and quickly deployed almost anywhere. It is also useful for providing an environment containing ready-to-run examples that aren't part of the installed package. You provide a recipe in the form of a Dockerfile which is essentially a sequence of commands on how to install - from a base OS - all your package's dependencies, build it, set up an environment, and then run it. Another advantage is if you your software requires large datasets from various sources, you can provide a pre-packaged docker volume containing all the data (as long as its publicly available and you have got any necessary permissions to replicate it, of course). Docker hub is one such repository, which (as always) integrates with github and can be configured to automatically generate images for you, including tags (and thus releases).","title":"Container repositories"},{"location":"anatomy/#citations","text":"Citations nowadays include a unique Document Origin Identifier (DOI), and this by no means applies only to academic papers. It's also used for both software and data. Thus, assigning a DOI to a specific release of your software constitutes a reference which anyone should be able to trace. What happens if you publish a paper referencing your software, including a version, and then subsequently move the repo elsewhere or even delete it, or pull the release from public repositories? This will leave readers of your article unable to find your work. The beauty of a DOI (and the provider thereof) is that it doesn't matter (so much). Zenodo is a service that (you guessed it) integrates with github and takes a complete snapshot copy of your software, and thus persists even if the original repo or package no longer exists. I would suggest you make it as easy as possible for people to cite you, providing a bibtex-style reference and/or a link to Zenodo (or similar) in your documentation can help. After all, it's people (including yourself, see below ) citing your software that makes it maintaining it worthwhile.","title":"Citations"},{"location":"anatomy/#licencing","text":"Another aspect of acknowledgement your work is others using your source code, as opposed to your software. This is where licencing comes in. It is important you understand your rights as a a developer of open-source software, and also the rights of users of your software. By licencing your package you protect yourself from public liability and also (typically) legally require anyone to acknowledge you if they base their own software on yours. Most open source licences are quite permissive, even to the point where a third party is allowed to sell your software, as long as your were acknowledged. Github, for example, provides a number of licence templates you can select from, select the one which best suits your situation.","title":"Licencing"},{"location":"anatomy/#communication","text":"","title":"Communication"},{"location":"anatomy/#for-promotion","text":"Users and practitioners need to be aware that your software exists. A simple and relatively easy way of promoting it is to publish an article in, for example, the Journal of Open Source Software or the Journal of Open Research Software , neither of which require a full paper, just simply a brief summary of what the package does. Their review process focussed on the software and if you've followed the recommendations here you should have little trouble being accepted. The impact, however, may not be great (who trawls there journals in the hope a package they could use suddenly appears?), so if you can also cite it yourself in an applied paper this will help greatly. Likewise, conferences are a good place to plug your development work, as well as simply encouraging collaborators and colleagues to use (and then cite) it.","title":"For promotion"},{"location":"anatomy/#for-development","text":"After configuring the tools in your ecosystem to listen for changes in repo, and respond automatically to them, they will typically email you if there's a problem. These days this isn't so reliable as most people are bombarded by emails and read few of them. Perhaps a better way is to use a messaging service (such as slack , as always others are available). Slack can be used as a central point for any issues coming from the services around your package, and of course can also be a place where developers communicate. Slack has many custom integrations which you can add to your channel, such as being able to create and amend github issues directly, and where custom integrations are not available you cant fairly easily create your own via webhooks. The slack channel(s) can also be a good place for communication with end users, which brings me on to...","title":"For development"},{"location":"anatomy/#support","text":"It's difficult to know how many people actively use your package (short of incorporating spyware into it). Whilst package repositories will produce a count (or rate) of downloads, I suspect at least some are bots, and of the others, downloading is one thing but actually using is another. But lets assume you have active users, and you have good documentation. You may find that a lot of the support questions could be answered by simply reading the documentation (who's not guilty of this?), or are not specific enough for you to replicate the problem they report. Some form of triage is useful if you get a lot of support questions. You can use an \"issue template\" where users must complete certain questions, describing in detail what they were trying to do, what went wrong, and on which version and platform. This deters people who realise they should look again at the documentation, and stops people raising vague issues along the lines of \"I tried using X but it didn't work\", that simply don't contain enough information to replicate. At the very least, you should make it clear that you cannot/will not help anyone unless they can come up with a reproducible example. After all, how do you fix a bug if you can't actually replicate the bug?","title":"Support"},{"location":"anatomy/#takeaways","text":"This may seem like an awful lot of effort to go to, but it does save potentially a lot of future headaches, and it doesn't need to all be done at once. Start with CI, and add other services/features as and when they become useful. Good practice in software- and data-driven research is becoming ever more important and is being taken more and more seriously by more traditional academics. The growth of Research Software Engineering as a discipline in it's own right speaks to this. If I had to summarise what I've written into some very brief key lessons, they would be: if you want other people to use and cite your software, you have do do a lot more than just writing the software, but there is payback all the tools you need are there, you just need to figure out how to use them (if you don't know already) automate the sh!t out of everything you can. It will be less error prone and save you (potentially lots of) time going forward. Scripts are your friend anticipate failure, test everything. The sooner you know something is broken, the better","title":"Takeaways"},{"location":"anatomy/#room-for-improvement","text":"In what I've described, the developer has the ability to release directly from their own development environment to public package repositories. This is error-prone and open to abuse. I could accidentally (or deliberately) create a public release of my package that's not in source control and thus circumvent all the good work around testing and versioning. A better approach would be to release via continuous deployment (CD), a logical extension of CI. In the commercial world there's typically a clear segregation of roles (dev, infrastructure, production/users) but in academia you are often all 3 of them. Implementing CD would at least eliminate accidents, another approach would be for package repositories to only accept releases from verified sources, e.g. releases created on github.","title":"Room for improvement"},{"location":"anatomy/#in-practice","text":"To see (most of) this in action \"in the wild\", take a look around the neworder project.","title":"In practice"},{"location":"bad-algorithms/","text":"Bad algorithms \u00b6 I want to have some fun(?) implementing insanely slow algorithms for simple operations, but first here's a classic example of how not to implement an algorithm to set the scene. Exponential complexity \u00b6 The Fibonacci series is an integer sequence beginning \\(0, 1\\) where every successive digit is the sum of the previous two: \\[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55...\\] The definition \\[F\\big(n\\big) = \\left\\{\\begin{matrix} & n & n \\in {0,1} \\\\ & F\\big(n-1\\big) + F\\big(n-2\\big) & n > 1 \\end{matrix}\\right.\\] lends itself directly to a recursive implementation, e.g. in python: def fib_recursive ( n ): if n < 2 : return n return fib_recursive ( n - 1 ) + fib_recursive ( n - 2 ) For each call to fib_recursive up to two further calls to the function are made and thus the call stack will grow exponentially. For large \\(n\\) this will cause the program to run very slooooow if it doesn't run out of resources entirely and crash. This table illustrates the execution time for a small range of \\(n\\) : n F(n) Time(ms) 10 55 0.0851154 15 610 0.911951 20 6765 9.92441 25 75025 111.743 30 832040 1224.36 In graphical form it's clear that execution time in terms of \\(n\\) is linear on a log scale and thus growing exponentially : If you compute the slope, it's about ~1.618, a.k.a the Golden ratio \\(\\phi\\) which just happens to be the value the that the ratio of successive values in the sequence tends to. As you can see, for even fairly small \\(n\\) the function is taking prohibitively long to run. The number of times the function is actually called for a given value of \\(n\\) is a close relation of the sequence itself (unsurprisingly): \\[C\\big(n\\big) = \\left\\{\\begin{matrix} & 1 & n \\in {0,1} \\\\ & 1 + C\\big(n-1\\big) + C\\big(n-2\\big) & n > 1 \\end{matrix}\\right.\\] which is larger than \\(F\\) but grows at exactly the same rate, \\(\\phi\\) . In algorithmic complexity terms, this is known as exponential time complexity and should be avoided at all costs. The point I'm trying to make here is that the obvious (but perhaps naiive) implementation of a mathematical function is not always the best approach, and a good (research or otherwise) software engineer should know this. So let's try again. Python has a feature called generators which are ideal for computing sequences. The generator holds hold some state, and the next time you call it, it just picks up where it left off (the yield statement - this construct is known as a coroutine ). You could implement is like this: def fibgen (): a , b = 0 , 1 while True : yield a a , b = a + b , a def fib_generator ( n ): fg = fibgen () for _ in range ( n ): next ( fg ) return next ( fg ) And the execution time is vastly improved: n F(n) Time(ms) 10 55 0.00524521 20 6765 0.00333786 30 832040 0.00429153 40 102334155 0.00476837 50 12586269025 0.00572205 60 1548008755920 0.00691414 70 190392490709135 0.00762939 80 23416728348467685 0.00858307 90 2880067194370816120 0.00977516 The data is a little noisier this time because the execution time is so much faster, but there's a clear linear trend now: So, we've started with a worse-than-polynomial \\(O(\\phi^n)\\) algorithm and turned it into an \\(O(n)\\) one. Great! Worse-than-exponential algorithms \u00b6 You don't hear much about such things, I suppose largely because they're utterly impractical. Apart from travelling salesman-type problems which have a brute-force \\(O(n!)\\) complexity, I haven't really come across any problems that require such algorithms, but I'm sure plenty must exist. So, perversely I decided to explore how I could contrive to solve simple problems with hopelessly inefficient algorithms and try and work out how long operations would take using them. Obviously actually running, and even testing, them is largely impractical. Hyperoperations \u00b6 Starting two integers \\(a\\) and \\(b\\) , we formulate arithmetic operations on them as a chain of binary operations starting with the basic operation of incrementing (\"succession\"): Level Operation 0 Succession \\((a, b) \\rightarrow a + 1\\) 1 Addition \\((a, b) \\rightarrow a + b\\) 2 Multiplication \\((a, b) \\rightarrow ab\\) 3 Exponentiation \\((a, b) \\rightarrow a^b \\equiv a \\uparrow b\\) 4 Tetration \\((a, b) \\rightarrow \\underbrace{a^{a^{\\unicode{x22F0}^a}}}_b \\equiv a \\uparrow \\uparrow b\\) 4 Pentation \\((a, b) \\rightarrow a \\uparrow^3 b\\) 4 Hexation \\((a, b) \\rightarrow a \\uparrow^4 b\\) So each level is just doing \\(b\\) iterations of the previous level. And you can keep going forever but you need some unusual mathematical notation. The table above introduces Knuth's up-arrow notation. I can code up all of these operations in terms of iterated calls to the succession function: def suc ( n ): return n + 1 def add ( m , n ): for i in range ( n ): m = suc ( m ) return m def mul ( m , n ): m_ = m for i in range ( 1 , n ): m = add ( m , m_ ) return m def pow ( m , n ): m_ = m for i in range ( 1 , n ): m = mul ( m , m_ ) return m def tet ( m , n ): m_ = m for i in range ( 1 , n ): m = pow ( m_ , m ) # note args switched return m def pen ( m , n ): m_ = m for i in range ( 1 , n ): m = tet ( m_ , m ) return m def hex ( m , n ): m_ = m for i in range ( 1 , n ): m = pen ( m_ , m ) return m which gives addition linear complexity, multiplication quadratic, exponentiation exponential, and then we get into unknown territory with pentation and hexation. I'm not sure why I bothered, as I'm not going to be able to run them anyway. There's also the amusing side effect that adding 1 to 1000000 is much quicker than vice versa. And, since the only arithmetic operation in all of this is succession, the execution time will be proportional to the value of the result. This is good because in most cases it won't be possible to actually perform the computation, but I can still get a good idea of how long it would take. Addition \u00b6 The results for addition, unsurprisingly, show a clear linear trend, so it's \\(O(n)\\) : Multiplication \u00b6 This time with the \\(y\\) axis as the square root of the execution time, again we see the expected linear trend, so we've gone from the inbuilt \\(O(1)\\) to \\(O(n^2)\\) : Exponentiation \u00b6 And for exponentiation, you can see the runtime is increasing rapidly with \\(n\\) : n pow(n,n) Time(ms) 4 256 0.310421 5 3125 2.46119 6 46656 30.9212 7 823543 311.033 8 16777216 4973.39 9 387420489 112083 and since the number of operations grows with \\(n^n\\) , we plot the execution time against \\(\\ln(t)/\\ln(n)\\) : Already the performance is worse than the brute-force Travelling Salesman at \\(O(n^n) > O(n!)\\) and is trying my patience, but now I've worked out my algorithms give the right answer and perform as badly as expected, and also that this laptop can do about 3 million succession operations per second. So how does tetration perform? Hyperexponential complexity \u00b6 We can think of the previous example as \"hyperquadratic\" complexity \\(O(n\\uparrow\\uparrow2)\\) , analgous to quadratic complexity in the family of polynomial complexities. What about tetration? Here's some (extrapolated) execution times: \\(2^{2^{2^2}} = 2\\uparrow\\uparrow4 = 65\\,536\\) took about 20ms to calculate. \\(3^{3^3} = 3\\uparrow\\uparrow3 = 7\\,625\\,597\\,484\\,987\\) will take approximately 29 days to compute. \\(4^{4^4} = 4\\uparrow\\uparrow3 \\approx 1.34 \\times 10^{154} \\) will take around \\(\\mathbf{10^{130}}\\) times the age of the universe to compute. But if you're in a hurry, python -c \"print(4**4**4)\" will give you the answer instantaneously \ud83d\ude09. I'd better stop there. I've replaced perfectly good constant-time algorithms with pathologically inefficient linear, quadratic, exponential (and worse) ones. Utterly pointless, but sort of interesting all the same. I'll get my coat... If you want to see a mind-boggling example of a hyperoperation at a level that is itself an iterated hyperoperation, check out Graham's number .","title":"Bad algorithms"},{"location":"bad-algorithms/#bad-algorithms","text":"I want to have some fun(?) implementing insanely slow algorithms for simple operations, but first here's a classic example of how not to implement an algorithm to set the scene.","title":"Bad algorithms"},{"location":"bad-algorithms/#exponential-complexity","text":"The Fibonacci series is an integer sequence beginning \\(0, 1\\) where every successive digit is the sum of the previous two: \\[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55...\\] The definition \\[F\\big(n\\big) = \\left\\{\\begin{matrix} & n & n \\in {0,1} \\\\ & F\\big(n-1\\big) + F\\big(n-2\\big) & n > 1 \\end{matrix}\\right.\\] lends itself directly to a recursive implementation, e.g. in python: def fib_recursive ( n ): if n < 2 : return n return fib_recursive ( n - 1 ) + fib_recursive ( n - 2 ) For each call to fib_recursive up to two further calls to the function are made and thus the call stack will grow exponentially. For large \\(n\\) this will cause the program to run very slooooow if it doesn't run out of resources entirely and crash. This table illustrates the execution time for a small range of \\(n\\) : n F(n) Time(ms) 10 55 0.0851154 15 610 0.911951 20 6765 9.92441 25 75025 111.743 30 832040 1224.36 In graphical form it's clear that execution time in terms of \\(n\\) is linear on a log scale and thus growing exponentially : If you compute the slope, it's about ~1.618, a.k.a the Golden ratio \\(\\phi\\) which just happens to be the value the that the ratio of successive values in the sequence tends to. As you can see, for even fairly small \\(n\\) the function is taking prohibitively long to run. The number of times the function is actually called for a given value of \\(n\\) is a close relation of the sequence itself (unsurprisingly): \\[C\\big(n\\big) = \\left\\{\\begin{matrix} & 1 & n \\in {0,1} \\\\ & 1 + C\\big(n-1\\big) + C\\big(n-2\\big) & n > 1 \\end{matrix}\\right.\\] which is larger than \\(F\\) but grows at exactly the same rate, \\(\\phi\\) . In algorithmic complexity terms, this is known as exponential time complexity and should be avoided at all costs. The point I'm trying to make here is that the obvious (but perhaps naiive) implementation of a mathematical function is not always the best approach, and a good (research or otherwise) software engineer should know this. So let's try again. Python has a feature called generators which are ideal for computing sequences. The generator holds hold some state, and the next time you call it, it just picks up where it left off (the yield statement - this construct is known as a coroutine ). You could implement is like this: def fibgen (): a , b = 0 , 1 while True : yield a a , b = a + b , a def fib_generator ( n ): fg = fibgen () for _ in range ( n ): next ( fg ) return next ( fg ) And the execution time is vastly improved: n F(n) Time(ms) 10 55 0.00524521 20 6765 0.00333786 30 832040 0.00429153 40 102334155 0.00476837 50 12586269025 0.00572205 60 1548008755920 0.00691414 70 190392490709135 0.00762939 80 23416728348467685 0.00858307 90 2880067194370816120 0.00977516 The data is a little noisier this time because the execution time is so much faster, but there's a clear linear trend now: So, we've started with a worse-than-polynomial \\(O(\\phi^n)\\) algorithm and turned it into an \\(O(n)\\) one. Great!","title":"Exponential complexity"},{"location":"bad-algorithms/#worse-than-exponential-algorithms","text":"You don't hear much about such things, I suppose largely because they're utterly impractical. Apart from travelling salesman-type problems which have a brute-force \\(O(n!)\\) complexity, I haven't really come across any problems that require such algorithms, but I'm sure plenty must exist. So, perversely I decided to explore how I could contrive to solve simple problems with hopelessly inefficient algorithms and try and work out how long operations would take using them. Obviously actually running, and even testing, them is largely impractical.","title":"Worse-than-exponential algorithms"},{"location":"bad-algorithms/#hyperoperations","text":"Starting two integers \\(a\\) and \\(b\\) , we formulate arithmetic operations on them as a chain of binary operations starting with the basic operation of incrementing (\"succession\"): Level Operation 0 Succession \\((a, b) \\rightarrow a + 1\\) 1 Addition \\((a, b) \\rightarrow a + b\\) 2 Multiplication \\((a, b) \\rightarrow ab\\) 3 Exponentiation \\((a, b) \\rightarrow a^b \\equiv a \\uparrow b\\) 4 Tetration \\((a, b) \\rightarrow \\underbrace{a^{a^{\\unicode{x22F0}^a}}}_b \\equiv a \\uparrow \\uparrow b\\) 4 Pentation \\((a, b) \\rightarrow a \\uparrow^3 b\\) 4 Hexation \\((a, b) \\rightarrow a \\uparrow^4 b\\) So each level is just doing \\(b\\) iterations of the previous level. And you can keep going forever but you need some unusual mathematical notation. The table above introduces Knuth's up-arrow notation. I can code up all of these operations in terms of iterated calls to the succession function: def suc ( n ): return n + 1 def add ( m , n ): for i in range ( n ): m = suc ( m ) return m def mul ( m , n ): m_ = m for i in range ( 1 , n ): m = add ( m , m_ ) return m def pow ( m , n ): m_ = m for i in range ( 1 , n ): m = mul ( m , m_ ) return m def tet ( m , n ): m_ = m for i in range ( 1 , n ): m = pow ( m_ , m ) # note args switched return m def pen ( m , n ): m_ = m for i in range ( 1 , n ): m = tet ( m_ , m ) return m def hex ( m , n ): m_ = m for i in range ( 1 , n ): m = pen ( m_ , m ) return m which gives addition linear complexity, multiplication quadratic, exponentiation exponential, and then we get into unknown territory with pentation and hexation. I'm not sure why I bothered, as I'm not going to be able to run them anyway. There's also the amusing side effect that adding 1 to 1000000 is much quicker than vice versa. And, since the only arithmetic operation in all of this is succession, the execution time will be proportional to the value of the result. This is good because in most cases it won't be possible to actually perform the computation, but I can still get a good idea of how long it would take.","title":"Hyperoperations"},{"location":"bad-algorithms/#addition","text":"The results for addition, unsurprisingly, show a clear linear trend, so it's \\(O(n)\\) :","title":"Addition"},{"location":"bad-algorithms/#multiplication","text":"This time with the \\(y\\) axis as the square root of the execution time, again we see the expected linear trend, so we've gone from the inbuilt \\(O(1)\\) to \\(O(n^2)\\) :","title":"Multiplication"},{"location":"bad-algorithms/#exponentiation","text":"And for exponentiation, you can see the runtime is increasing rapidly with \\(n\\) : n pow(n,n) Time(ms) 4 256 0.310421 5 3125 2.46119 6 46656 30.9212 7 823543 311.033 8 16777216 4973.39 9 387420489 112083 and since the number of operations grows with \\(n^n\\) , we plot the execution time against \\(\\ln(t)/\\ln(n)\\) : Already the performance is worse than the brute-force Travelling Salesman at \\(O(n^n) > O(n!)\\) and is trying my patience, but now I've worked out my algorithms give the right answer and perform as badly as expected, and also that this laptop can do about 3 million succession operations per second. So how does tetration perform?","title":"Exponentiation"},{"location":"bad-algorithms/#hyperexponential-complexity","text":"We can think of the previous example as \"hyperquadratic\" complexity \\(O(n\\uparrow\\uparrow2)\\) , analgous to quadratic complexity in the family of polynomial complexities. What about tetration? Here's some (extrapolated) execution times: \\(2^{2^{2^2}} = 2\\uparrow\\uparrow4 = 65\\,536\\) took about 20ms to calculate. \\(3^{3^3} = 3\\uparrow\\uparrow3 = 7\\,625\\,597\\,484\\,987\\) will take approximately 29 days to compute. \\(4^{4^4} = 4\\uparrow\\uparrow3 \\approx 1.34 \\times 10^{154} \\) will take around \\(\\mathbf{10^{130}}\\) times the age of the universe to compute. But if you're in a hurry, python -c \"print(4**4**4)\" will give you the answer instantaneously \ud83d\ude09. I'd better stop there. I've replaced perfectly good constant-time algorithms with pathologically inefficient linear, quadratic, exponential (and worse) ones. Utterly pointless, but sort of interesting all the same. I'll get my coat... If you want to see a mind-boggling example of a hyperoperation at a level that is itself an iterated hyperoperation, check out Graham's number .","title":"Hyperexponential complexity"},{"location":"blockchain-auth/","text":"Blockchain authentication \u00b6 Cryptography, of which authentication is a part, is a thing where you don't ever roll your own. If you do, people will just laugh at you. (After they've hacked you, probably). At the risk of people laughing at me, this isn't a here goes... Crypto background \u00b6 Public-key cryptography \u00b6 ECDSA Cryptographic hashes \u00b6 SHA256 Blockchain background \u00b6 Crypto in blockchain \u00b6 \"Smart\" contracts \u00b6 Motivations \u00b6 Example Application \u00b6 I have a Raspberry Pi with a \"Sense HAT\" which contains sensors for temperature, pressure and humidity. I've written a little API so that clients can request this information over http, but the ambient conditions in my office are top secret information so I don't want everyone knowing them. I've got another Raspberry Pi that wants to know what the ambient conditions are (2 metres away), but since this is highly sensitive information it wants to ensure that the response it gets is genuine. My Raspberry Pis have agreed (bilaterally) to share this data between them, but don't fully trust each other (to not be hacked) so each want to be able to unilaterally (and permanently) stop any communications if they suspect foul play. I'm also concerned that they could be hacked, so I'd I could probab","title":"Blockchain authentication"},{"location":"blockchain-auth/#blockchain-authentication","text":"Cryptography, of which authentication is a part, is a thing where you don't ever roll your own. If you do, people will just laugh at you. (After they've hacked you, probably). At the risk of people laughing at me, this isn't a here goes...","title":"Blockchain authentication"},{"location":"blockchain-auth/#crypto-background","text":"","title":"Crypto background"},{"location":"blockchain-auth/#public-key-cryptography","text":"ECDSA","title":"Public-key cryptography"},{"location":"blockchain-auth/#cryptographic-hashes","text":"SHA256","title":"Cryptographic hashes"},{"location":"blockchain-auth/#blockchain-background","text":"","title":"Blockchain background"},{"location":"blockchain-auth/#crypto-in-blockchain","text":"","title":"Crypto in blockchain"},{"location":"blockchain-auth/#smart-contracts","text":"","title":"\"Smart\" contracts"},{"location":"blockchain-auth/#motivations","text":"","title":"Motivations"},{"location":"blockchain-auth/#example-application","text":"I have a Raspberry Pi with a \"Sense HAT\" which contains sensors for temperature, pressure and humidity. I've written a little API so that clients can request this information over http, but the ambient conditions in my office are top secret information so I don't want everyone knowing them. I've got another Raspberry Pi that wants to know what the ambient conditions are (2 metres away), but since this is highly sensitive information it wants to ensure that the response it gets is genuine. My Raspberry Pis have agreed (bilaterally) to share this data between them, but don't fully trust each other (to not be hacked) so each want to be able to unilaterally (and permanently) stop any communications if they suspect foul play. I'm also concerned that they could be hacked, so I'd I could probab","title":"Example Application"},{"location":"difficult-data/","text":"How Not to Supply Data \u00b6 If you provide packaged, versioned, well-documented software as part of your academic output, then you make your work much more easily reproducible (see my previous post ). However, if that software needs some input data to do its thing, then you also need to apply the same principles to the data : somebody with a different dataset wont be able to produce your results, despite having identical software. Step 1 is to you cite all your data sources, but often the raw data will need some form of preprocessing to be useful. And how often is this transparent? You download a spreadsheet, copy and paste some data into a new spreadsheet, apply some formulas, change some names, then save it as a csv file. This is completely opaque and if any misinterpretations or errors creep in then they are hidden. In an ideal world data providers would provide their data in directly usable formats, but how can they anticipate all the possible needs of their consumers? Realistically, some preprocessing will be required Too often data providers conflate visualisation (i.e. presentation to human viewers) and the data itself, which makes the data harder to extract programatically. Too often data providers supply data in proprietary formats... Example 1: Population data \u00b6 Say we need population projections by for England by local authority, sex and (5-year) age group. We'd like it in some non-proprietary format that we could load straight into (e.g.) python or R and do something with it. A csv file will do just fine for this. ONS produce this data, and here's the 2018 numbers from their website . It's an Excel spreadsheet: The obvious question that springs to mind is: what audience is this aimed at? It's got all sorts of features like frozen rows, formatted text, comma-number separators than suggest it's been designed for humans to pore over. Really? But who gets insight directly from huge tables of numbers, surely they'd visualise it graphically in some way? It seems to be in some no-man's land between user-friendliness and process-friendliness. To get the exact dataset I want I could do the following: delete the first six rows of the Males worksheet delete all the rows where the AGE GROUP column value is \"All ages\", otherwise I'll double-count the numbers delete all the rows where the AREA column value is a country, a region or a county, to avoid more double-counting create a new column called SEX and fill it with (e.g.) \"M\" remove the comma-separator formatting from the numbers and the \"freeze columns\" setting copy the remaining contents into a new worksheet repeat all the steps above in the Females worksheet, but adding (e.g.) \"F\" in the new SEX column copy this to the new sheet, below the male data save the new worksheet as a csv file As data goes, this isn't particularly messy or unstructured, yet I have to go though some fairly tedious, manual and error-prone steps to get the data exactly as I need it. And if I'm using this data to generate some published results, do I really want people to have to repeat these steps to replicate my work? I could detail the manual preprocessing steps in the text, but it detracts somewhat from the impact of the paper. Another possibility is I could publish the preprocessed data and reference it in my paper. But if it transpires there is an error in the original data, which gets corrected, then neither I, not my readers, get the correction. Perhaps a better solution would be to automate the steps I took, and packages like pandas (for python) are perfect for this, although there's a few hoops to jump though: import pandas as pd import requests # ONS spreadsheet # 2018 SNPP 5y url = \"https://www.ons.gov.uk/file?uri= %2f peoplepopulationandcommunity %2f populationandmigration %2f populationprojections %2f datasets %2f localauthoritiesinenglandtable2 %2f 2018based/table2.xls\" # reading url directly gives 403, have to use requests with an acceptable user-agent string # data = pd.read_excel(url) # fails # which requests (by default) response = requests . get ( url ) if response . status_code != 200 : print ( \" %d error\" ) exit () then we process the data dataset = pd . DataFrame () for tab in [ \"Persons\" ]: # [\"Males\", \"Females\"]: data = pd . read_excel ( response . content , sheet_name = tab , header = 6 ) # get totals for checking I filtered correctly totals = data [( data [ \"AGE GROUP\" ] == \"All ages\" ) & ( data [ \"CODE\" ] == \"E92000001\" )] . reset_index ( drop = True ) # remove duplicate age counts data = data [ data [ \"AGE GROUP\" ] != \"All ages\" ] # remove non-LAD counts data = data [ data [ \"CODE\" ] . str . match ( r '^E0[6789]*' )] # get totals for each year annual_sums = data . drop ([ \"CODE\" , \"AREA\" , \"AGE GROUP\" ], axis = 1 ) . sum () annual_totals = totals . drop ([ \"CODE\" , \"AREA\" , \"AGE GROUP\" ], axis = 1 ) . iloc [ 0 ]) #assert annual_sums.equals(annual_totals) #print(data.drop([\"CODE\", \"AREA\", \"AGE GROUP\"], axis=1).sum() - totals.drop([\"CODE\", \"AREA\", \"AGE GROUP\"], axis=1).iloc[0]) data [ \"SEX\" ] = tab [ 0 ] dataset = dataset . append ( data ) print ( dataset ) explain that this breaks if the URL becomes stale, so still not a perfect solution. APIs Nomisweb... https://www.nomisweb.co.uk/api/v01/dataset/NM_31_1.data.tsv?geography=1811939329...1811939332,1811939334...1811939336,1811939338...1811939497,1811939499...1811939501,1811939503,1811939505...1811939507,1811939509...1811939517,1811939519,1811939520,1811939524...1811939570,1811939575...1811939599,1811939601...1811939628,1811939630...1811939634,1811939636...1811939647,1811939649,1811939655...1811939664,1811939667...1811939680,1811939682,1811939683,1811939685,1811939687...1811939704,1811939707,1811939708,1811939710,1811939712...1811939717,1811939719,1811939720,1811939722...1811939730,1811939757...1811939767&date=latestMINUS1&sex=5,6&age=1...19&measures=20100 Schemas DOIs for data https://dataingovernment.blog.gov.uk/2020/11/10/we-want-your-feedback-on-using-the-doi-standard-in-government/ Still relying on supplier not to change data/format. At least you can say it not your fault","title":"How Not to Supply Data"},{"location":"difficult-data/#how-not-to-supply-data","text":"If you provide packaged, versioned, well-documented software as part of your academic output, then you make your work much more easily reproducible (see my previous post ). However, if that software needs some input data to do its thing, then you also need to apply the same principles to the data : somebody with a different dataset wont be able to produce your results, despite having identical software. Step 1 is to you cite all your data sources, but often the raw data will need some form of preprocessing to be useful. And how often is this transparent? You download a spreadsheet, copy and paste some data into a new spreadsheet, apply some formulas, change some names, then save it as a csv file. This is completely opaque and if any misinterpretations or errors creep in then they are hidden. In an ideal world data providers would provide their data in directly usable formats, but how can they anticipate all the possible needs of their consumers? Realistically, some preprocessing will be required Too often data providers conflate visualisation (i.e. presentation to human viewers) and the data itself, which makes the data harder to extract programatically. Too often data providers supply data in proprietary formats...","title":"How Not to Supply Data"},{"location":"difficult-data/#example-1-population-data","text":"Say we need population projections by for England by local authority, sex and (5-year) age group. We'd like it in some non-proprietary format that we could load straight into (e.g.) python or R and do something with it. A csv file will do just fine for this. ONS produce this data, and here's the 2018 numbers from their website . It's an Excel spreadsheet: The obvious question that springs to mind is: what audience is this aimed at? It's got all sorts of features like frozen rows, formatted text, comma-number separators than suggest it's been designed for humans to pore over. Really? But who gets insight directly from huge tables of numbers, surely they'd visualise it graphically in some way? It seems to be in some no-man's land between user-friendliness and process-friendliness. To get the exact dataset I want I could do the following: delete the first six rows of the Males worksheet delete all the rows where the AGE GROUP column value is \"All ages\", otherwise I'll double-count the numbers delete all the rows where the AREA column value is a country, a region or a county, to avoid more double-counting create a new column called SEX and fill it with (e.g.) \"M\" remove the comma-separator formatting from the numbers and the \"freeze columns\" setting copy the remaining contents into a new worksheet repeat all the steps above in the Females worksheet, but adding (e.g.) \"F\" in the new SEX column copy this to the new sheet, below the male data save the new worksheet as a csv file As data goes, this isn't particularly messy or unstructured, yet I have to go though some fairly tedious, manual and error-prone steps to get the data exactly as I need it. And if I'm using this data to generate some published results, do I really want people to have to repeat these steps to replicate my work? I could detail the manual preprocessing steps in the text, but it detracts somewhat from the impact of the paper. Another possibility is I could publish the preprocessed data and reference it in my paper. But if it transpires there is an error in the original data, which gets corrected, then neither I, not my readers, get the correction. Perhaps a better solution would be to automate the steps I took, and packages like pandas (for python) are perfect for this, although there's a few hoops to jump though: import pandas as pd import requests # ONS spreadsheet # 2018 SNPP 5y url = \"https://www.ons.gov.uk/file?uri= %2f peoplepopulationandcommunity %2f populationandmigration %2f populationprojections %2f datasets %2f localauthoritiesinenglandtable2 %2f 2018based/table2.xls\" # reading url directly gives 403, have to use requests with an acceptable user-agent string # data = pd.read_excel(url) # fails # which requests (by default) response = requests . get ( url ) if response . status_code != 200 : print ( \" %d error\" ) exit () then we process the data dataset = pd . DataFrame () for tab in [ \"Persons\" ]: # [\"Males\", \"Females\"]: data = pd . read_excel ( response . content , sheet_name = tab , header = 6 ) # get totals for checking I filtered correctly totals = data [( data [ \"AGE GROUP\" ] == \"All ages\" ) & ( data [ \"CODE\" ] == \"E92000001\" )] . reset_index ( drop = True ) # remove duplicate age counts data = data [ data [ \"AGE GROUP\" ] != \"All ages\" ] # remove non-LAD counts data = data [ data [ \"CODE\" ] . str . match ( r '^E0[6789]*' )] # get totals for each year annual_sums = data . drop ([ \"CODE\" , \"AREA\" , \"AGE GROUP\" ], axis = 1 ) . sum () annual_totals = totals . drop ([ \"CODE\" , \"AREA\" , \"AGE GROUP\" ], axis = 1 ) . iloc [ 0 ]) #assert annual_sums.equals(annual_totals) #print(data.drop([\"CODE\", \"AREA\", \"AGE GROUP\"], axis=1).sum() - totals.drop([\"CODE\", \"AREA\", \"AGE GROUP\"], axis=1).iloc[0]) data [ \"SEX\" ] = tab [ 0 ] dataset = dataset . append ( data ) print ( dataset ) explain that this breaks if the URL becomes stale, so still not a perfect solution. APIs Nomisweb... https://www.nomisweb.co.uk/api/v01/dataset/NM_31_1.data.tsv?geography=1811939329...1811939332,1811939334...1811939336,1811939338...1811939497,1811939499...1811939501,1811939503,1811939505...1811939507,1811939509...1811939517,1811939519,1811939520,1811939524...1811939570,1811939575...1811939599,1811939601...1811939628,1811939630...1811939634,1811939636...1811939647,1811939649,1811939655...1811939664,1811939667...1811939680,1811939682,1811939683,1811939685,1811939687...1811939704,1811939707,1811939708,1811939710,1811939712...1811939717,1811939719,1811939720,1811939722...1811939730,1811939757...1811939767&date=latestMINUS1&sex=5,6&age=1...19&measures=20100 Schemas DOIs for data https://dataingovernment.blog.gov.uk/2020/11/10/we-want-your-feedback-on-using-the-doi-standard-in-government/ Still relying on supplier not to change data/format. At least you can say it not your fault","title":"Example 1: Population data"},{"location":"float/","text":"Floating-Point Arithmetic \u00b6 \"Computers are an unsuiable tool for doing mathematical computations, and should not be used for such applications\" Famous quote XKCD sums it up quite nicely: Any software developer who writes numerical code needs to have some understanding of how floating-point numbers are represented, because they are a major source of software bugs and issues. Almost all hardware and software architectures implement the IEEE754 standard. In this chapter I shall explain how a floating-point number is represented using this standard, what the limitations of the format are, and how these can cause some quite nasty software bugs, where the source of errors arise, and how to minimise the impact of these limitations. Some people have a tendency to dismiss floating-point issues as \"its a rounding error, you get them with floating-point\" but that belies a lack fo understanding as to when simply dismisses the potential problem Consider this motivating example of a very simple floating-point expression: Python 3.7.5 (default, Nov 20 2019, 09:21:52) [GCC 9.2.1 20191008] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> 0.5-0.4-0.1 -2.7755575615628914e-17 >>> Feel free to try this in your language/platform of choice, chances are you will get the same wrong answer. So why can't a computer get such a simple sum correct? You can just dismiss it as a rounding error and get on with your life. Unless of course you have derivative traders screaming at you because their risk numbers are all over the place and they can't hedge their positions. Turns out the pricing model is a nonlinear optimisation (requiring matrix inversions) with an objective function that's itself a Monte-Carlo simulation. Then you can't. The IEEE754 Standard \u00b6 Here we will only consider the double precision format, as it's by far the most widely used. Single, half and quad precision all suffer from the same issues. Double-precision floating-point numbers are represented in 64 bits as follows: a single sign bit s , with zero representing positive and 1 negative. an 11-bit exponent e a 52-bit mantissa m The value 1 looks like this in binary: \\[\\underbrace{\\boxed{0}}_s\\underbrace{\\boxed{01111111111}}_e\\underbrace{\\boxed{0000000000000000000000000000000000000000000000000000}}_m\\] In other words, \\[ x = \\big(-1\\big)^{s}.2^{e-1023}.(1 + m/(2^{52})) \\] The exponent needs some explanation. It is biased by 1023, in other words a value of 1023 represents an exponent of one. It has two special values that are interpreted differently, namely 0 and 2047, which we shall come to later. A value of 1 represents an exponent of \\(2^{-1022}\\) or approximately \\(2.23\\times10^{-308}\\) and a value of 2046 is \\(2^{1023}\\) or approximately \\(8.99\\times10^{307}\\) . Thus the format can represent number over a range of well over 600 decimal orders of magnitude. The mantissa is interpreted as a binary number in the range \\(\\big[1,2\\big)\\) , with the leading 1 implied. The bits thus represent the fraction after the decimal point. This is why double precision is often referred to as having 53 bits of precision when the length of the mantissa is actually 52. Something of a moot point when one of those bits is fixed. Here are the bit representations of some other numbers n bits 0 \\(\\boxed{0}\\boxed{00000000000}\\boxed{0000000000000000000000000000000000000000000000000000}\\) 2 \\(\\boxed{0}\\boxed{10000000000}\\boxed{0000000000000000000000000000000000000000000000000000}\\) -1 \\(\\boxed{1}\\boxed{01111111111}\\boxed{0000000000000000000000000000000000000000000000000000}\\) 1000000 \\(\\boxed{0}\\boxed{10000010010}\\boxed{1110100001001000000000000000000000000000000000000000}\\) -0.0005 \\(\\boxed{1}\\boxed{01111110100}\\boxed{0000011000100100110111010010111100011010100111111100}\\) It is important to understand that the number is represented in binary format, not decimal or any other base. If you consider decimal, the fractions that can be represented exactly in a finite number of decimal digits are with denominator componsed of the prime factors of 10, namely 2 or 5, and products thereof. Thus n/2, n/4, n/5, n/8, n/10, n/16, n/20, n/25 etc., can all be represented exactly in finite precision. For binary, only fractions that have a denominator that is a power of 2 can be represented exactly. Number systems such as sexagesimal evolved largely because it divides easily for a range of denominators. It should be noted that binary (along with any other prime base) is the worst for this. Some other points to note: all integers that fit into 53 or fewer binary bits (not including the sign bit) can be represented exactly in this format. there are representations for both positive and negative zero. Epsilon \u00b6 In this context, epsilon is a measure of the precsion of the format. Specifically, it is the smallest number you can add to to a value of 1 to get a value that is not 1. You can show that it's equivalent to the length of the mantissa. In binary, it is the number we add to increase \\[ \\boxed{0}\\boxed{01111111111}\\boxed{0000000000000000000000000000000000000000000000000000} \\] to \\[ \\boxed{0}\\boxed{01111111111}\\boxed{0000000000000000000000000000000000000000000000000001} \\] We're simply increasing the least significant bit. The difference between the two (see the equation above), is \\(1/2^{52}\\) , or about \\(2.22\\times10^{-16}\\) in decimal. This number is approximately 1.00000000000000022. Another way of looking at this is that we can represent \\(2^{52}\\) equally spaced values over the interval of the exponent, which is \\(\\big[1,2\\big)\\) . Thus the difference between successive values is \\(\\big(2-1\\big)/2^{52}\\) . Now consider what the smallest value we can subtract from one and not get a different number, we are now in the exponent range \\(\\big[0.5,\\big)\\) , and thus the difference between successive values is halved. This we can represent a value of (approximately) 0.999999999999999889. Similarly, for larger numbers the exponent range is greater, and thus the distance between successive value is proportionately larger, but there are always \\(2^{52}\\) discrete values available for a given exponent. We'll can define the spacing around any value as follows: \\[ \\delta\\big(x\\big) = 2^{\\lfloor{log_2\\big(x\\big)}\\rfloor - 52} \\] Now consider the assignment from string literals: Python 3.7 . 5 ( default , Nov 20 2019 , 09 : 21 : 52 ) [ GCC 9.2 . 1 20191008 ] on linux Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information . >>> x = 0.4 >>> print ( \" %.18f \" % x ) 0.400000000000000022 >>> x = 0.1 >>> print ( \" %.18f \" % x ) 0.100000000000000006 >>> As you can see neither 0.4 nor 0.1 are represented exactly, and we can be get error bounds: Desired x Representation of x 0.4 \\(0.4 \\pm \\delta\\big(0.4\\big)\\) 0.1 \\(0.1 \\pm \\delta\\big(0.1\\big)\\) Looking at the binary representations of the values we can see that the only difference is in the exponent: \\(\\boxed{0}\\boxed{01111111101}\\boxed{1001100110011001100110011001100110011001100110011010}\\) \\(\\boxed{0}\\boxed{01111111011}\\boxed{1001100110011001100110011001100110011001100110011010}\\) This makes sense since the numbers are exactly two binary orders of magnitude different. Thus the errors in approximating 0.1 must be exactly a quarter of that approximating 0.4. Ordering \u00b6 +/- zero inf nan References \u00b6 What every computer scientist should know about floating-point: https://www.itu.dk/~sestoft/bachelor/IEEE754_article.pdf","title":"Floating-Point Arithmetic"},{"location":"float/#floating-point-arithmetic","text":"\"Computers are an unsuiable tool for doing mathematical computations, and should not be used for such applications\" Famous quote XKCD sums it up quite nicely: Any software developer who writes numerical code needs to have some understanding of how floating-point numbers are represented, because they are a major source of software bugs and issues. Almost all hardware and software architectures implement the IEEE754 standard. In this chapter I shall explain how a floating-point number is represented using this standard, what the limitations of the format are, and how these can cause some quite nasty software bugs, where the source of errors arise, and how to minimise the impact of these limitations. Some people have a tendency to dismiss floating-point issues as \"its a rounding error, you get them with floating-point\" but that belies a lack fo understanding as to when simply dismisses the potential problem Consider this motivating example of a very simple floating-point expression: Python 3.7.5 (default, Nov 20 2019, 09:21:52) [GCC 9.2.1 20191008] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> 0.5-0.4-0.1 -2.7755575615628914e-17 >>> Feel free to try this in your language/platform of choice, chances are you will get the same wrong answer. So why can't a computer get such a simple sum correct? You can just dismiss it as a rounding error and get on with your life. Unless of course you have derivative traders screaming at you because their risk numbers are all over the place and they can't hedge their positions. Turns out the pricing model is a nonlinear optimisation (requiring matrix inversions) with an objective function that's itself a Monte-Carlo simulation. Then you can't.","title":"Floating-Point Arithmetic"},{"location":"float/#the-ieee754-standard","text":"Here we will only consider the double precision format, as it's by far the most widely used. Single, half and quad precision all suffer from the same issues. Double-precision floating-point numbers are represented in 64 bits as follows: a single sign bit s , with zero representing positive and 1 negative. an 11-bit exponent e a 52-bit mantissa m The value 1 looks like this in binary: \\[\\underbrace{\\boxed{0}}_s\\underbrace{\\boxed{01111111111}}_e\\underbrace{\\boxed{0000000000000000000000000000000000000000000000000000}}_m\\] In other words, \\[ x = \\big(-1\\big)^{s}.2^{e-1023}.(1 + m/(2^{52})) \\] The exponent needs some explanation. It is biased by 1023, in other words a value of 1023 represents an exponent of one. It has two special values that are interpreted differently, namely 0 and 2047, which we shall come to later. A value of 1 represents an exponent of \\(2^{-1022}\\) or approximately \\(2.23\\times10^{-308}\\) and a value of 2046 is \\(2^{1023}\\) or approximately \\(8.99\\times10^{307}\\) . Thus the format can represent number over a range of well over 600 decimal orders of magnitude. The mantissa is interpreted as a binary number in the range \\(\\big[1,2\\big)\\) , with the leading 1 implied. The bits thus represent the fraction after the decimal point. This is why double precision is often referred to as having 53 bits of precision when the length of the mantissa is actually 52. Something of a moot point when one of those bits is fixed. Here are the bit representations of some other numbers n bits 0 \\(\\boxed{0}\\boxed{00000000000}\\boxed{0000000000000000000000000000000000000000000000000000}\\) 2 \\(\\boxed{0}\\boxed{10000000000}\\boxed{0000000000000000000000000000000000000000000000000000}\\) -1 \\(\\boxed{1}\\boxed{01111111111}\\boxed{0000000000000000000000000000000000000000000000000000}\\) 1000000 \\(\\boxed{0}\\boxed{10000010010}\\boxed{1110100001001000000000000000000000000000000000000000}\\) -0.0005 \\(\\boxed{1}\\boxed{01111110100}\\boxed{0000011000100100110111010010111100011010100111111100}\\) It is important to understand that the number is represented in binary format, not decimal or any other base. If you consider decimal, the fractions that can be represented exactly in a finite number of decimal digits are with denominator componsed of the prime factors of 10, namely 2 or 5, and products thereof. Thus n/2, n/4, n/5, n/8, n/10, n/16, n/20, n/25 etc., can all be represented exactly in finite precision. For binary, only fractions that have a denominator that is a power of 2 can be represented exactly. Number systems such as sexagesimal evolved largely because it divides easily for a range of denominators. It should be noted that binary (along with any other prime base) is the worst for this. Some other points to note: all integers that fit into 53 or fewer binary bits (not including the sign bit) can be represented exactly in this format. there are representations for both positive and negative zero.","title":"The IEEE754 Standard"},{"location":"float/#epsilon","text":"In this context, epsilon is a measure of the precsion of the format. Specifically, it is the smallest number you can add to to a value of 1 to get a value that is not 1. You can show that it's equivalent to the length of the mantissa. In binary, it is the number we add to increase \\[ \\boxed{0}\\boxed{01111111111}\\boxed{0000000000000000000000000000000000000000000000000000} \\] to \\[ \\boxed{0}\\boxed{01111111111}\\boxed{0000000000000000000000000000000000000000000000000001} \\] We're simply increasing the least significant bit. The difference between the two (see the equation above), is \\(1/2^{52}\\) , or about \\(2.22\\times10^{-16}\\) in decimal. This number is approximately 1.00000000000000022. Another way of looking at this is that we can represent \\(2^{52}\\) equally spaced values over the interval of the exponent, which is \\(\\big[1,2\\big)\\) . Thus the difference between successive values is \\(\\big(2-1\\big)/2^{52}\\) . Now consider what the smallest value we can subtract from one and not get a different number, we are now in the exponent range \\(\\big[0.5,\\big)\\) , and thus the difference between successive values is halved. This we can represent a value of (approximately) 0.999999999999999889. Similarly, for larger numbers the exponent range is greater, and thus the distance between successive value is proportionately larger, but there are always \\(2^{52}\\) discrete values available for a given exponent. We'll can define the spacing around any value as follows: \\[ \\delta\\big(x\\big) = 2^{\\lfloor{log_2\\big(x\\big)}\\rfloor - 52} \\] Now consider the assignment from string literals: Python 3.7 . 5 ( default , Nov 20 2019 , 09 : 21 : 52 ) [ GCC 9.2 . 1 20191008 ] on linux Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information . >>> x = 0.4 >>> print ( \" %.18f \" % x ) 0.400000000000000022 >>> x = 0.1 >>> print ( \" %.18f \" % x ) 0.100000000000000006 >>> As you can see neither 0.4 nor 0.1 are represented exactly, and we can be get error bounds: Desired x Representation of x 0.4 \\(0.4 \\pm \\delta\\big(0.4\\big)\\) 0.1 \\(0.1 \\pm \\delta\\big(0.1\\big)\\) Looking at the binary representations of the values we can see that the only difference is in the exponent: \\(\\boxed{0}\\boxed{01111111101}\\boxed{1001100110011001100110011001100110011001100110011010}\\) \\(\\boxed{0}\\boxed{01111111011}\\boxed{1001100110011001100110011001100110011001100110011010}\\) This makes sense since the numbers are exactly two binary orders of magnitude different. Thus the errors in approximating 0.1 must be exactly a quarter of that approximating 0.4.","title":"Epsilon"},{"location":"float/#ordering","text":"+/- zero inf nan","title":"Ordering"},{"location":"float/#references","text":"What every computer scientist should know about floating-point: https://www.itu.dk/~sestoft/bachelor/IEEE754_article.pdf","title":"References"},{"location":"sourdough/","text":"Sourdough Bread \u00b6 Refreshing the Mother \u00b6 Making the preferment \u00b6 (aka spoge) Making the dough \u00b6 Autolyse \u00b6 Knead \u00b6 Fold and First Proof \u00b6 Shape \u00b6 Overnight Retardation \u00b6 Bake \u00b6 Eat \u00b6","title":"Sourdough Bread"},{"location":"sourdough/#sourdough-bread","text":"","title":"Sourdough Bread"},{"location":"sourdough/#refreshing-the-mother","text":"","title":"Refreshing the Mother"},{"location":"sourdough/#making-the-preferment","text":"(aka spoge)","title":"Making the preferment"},{"location":"sourdough/#making-the-dough","text":"","title":"Making the dough"},{"location":"sourdough/#autolyse","text":"","title":"Autolyse"},{"location":"sourdough/#knead","text":"","title":"Knead"},{"location":"sourdough/#fold-and-first-proof","text":"","title":"Fold and First Proof"},{"location":"sourdough/#shape","text":"","title":"Shape"},{"location":"sourdough/#overnight-retardation","text":"","title":"Overnight Retardation"},{"location":"sourdough/#bake","text":"","title":"Bake"},{"location":"sourdough/#eat","text":"","title":"Eat"},{"location":"output/fib_generator_table/","text":"n F(n) Time(ms) 10 55 0.00524521 20 6765 0.00333786 30 832040 0.00429153 40 102334155 0.00476837 50 12586269025 0.00572205 60 1548008755920 0.00691414 70 190392490709135 0.00762939 80 23416728348467685 0.00858307 90 2880067194370816120 0.00977516","title":"Fib generator table"},{"location":"output/fib_recursive_table/","text":"n F(n) Time(ms) 10 55 0.0851154 15 610 0.911951 20 6765 9.92441 25 75025 111.743 30 832040 1224.36","title":"Fib recursive table"},{"location":"output/hyper_exp_table/","text":"n pow(n,n) Time(ms) 4 256 0.310421 5 3125 2.46119 6 46656 30.9212 7 823543 311.033 8 16777216 4973.39 9 387420489 112083","title":"Hyper exp table"}]}