{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Recent articles \u00b6 The anatomy of a successful research software project Coming soon \u00b6 How not to provide data Future possibilities \u00b6 Why computers should never be used to do maths Blockchain authentication How (long it takes me) to make sourdough bread Insanely slow algorithms Embedding \\(\\LaTeX\\) \u00b6 \\[ e^{i\\pi}+1=0 \\] Code \u00b6 def fibonacci ( n ): if n < 2 : return n return fibonacci ( n - 1 ) + fibonacci ( n - 2 )","title":"Recent articles"},{"location":"#recent-articles","text":"The anatomy of a successful research software project","title":"Recent articles"},{"location":"#coming-soon","text":"How not to provide data","title":"Coming soon"},{"location":"#future-possibilities","text":"Why computers should never be used to do maths Blockchain authentication How (long it takes me) to make sourdough bread Insanely slow algorithms","title":"Future possibilities"},{"location":"#embedding-latex","text":"\\[ e^{i\\pi}+1=0 \\]","title":"Embedding \\(\\LaTeX\\)"},{"location":"#code","text":"def fibonacci ( n ): if n < 2 : return n return fibonacci ( n - 1 ) + fibonacci ( n - 2 )","title":"Code"},{"location":"about/","text":"About me \u00b6 Physicist turned investment banker turned academic researcher. Mission: software and practices that promote reproducibility, reusability and efficiency in academic research Currently: Research Fellow at the University of Leeds, using microsimulation and agent-based models to investigate the interaction between Police resourcing (supply) and crime (demand). Selected open-source projects \u00b6 neworder : a high-performance framework for dynamic microsimulation models humanleague : a microsynthesis package using quasirandom sampling and iterative propotional fitting (IPF) ukcensusapi : UK Census Data queries and downloads from python or R ukpopulation : download, cache, collate, filter, manipulate and extrapolate UK population and household estimates/projections Qualifications \u00b6 Publications \u00b6 Top tip, Viz , c.2002","title":"About me"},{"location":"about/#about-me","text":"Physicist turned investment banker turned academic researcher. Mission: software and practices that promote reproducibility, reusability and efficiency in academic research Currently: Research Fellow at the University of Leeds, using microsimulation and agent-based models to investigate the interaction between Police resourcing (supply) and crime (demand).","title":"About me"},{"location":"about/#selected-open-source-projects","text":"neworder : a high-performance framework for dynamic microsimulation models humanleague : a microsynthesis package using quasirandom sampling and iterative propotional fitting (IPF) ukcensusapi : UK Census Data queries and downloads from python or R ukpopulation : download, cache, collate, filter, manipulate and extrapolate UK population and household estimates/projections","title":"Selected open-source projects"},{"location":"about/#qualifications","text":"","title":"Qualifications"},{"location":"about/#publications","text":"Top tip, Viz , c.2002","title":"Publications"},{"location":"anatomy/","text":"The anatomy of a successful research software project \u00b6 Motivations \u00b6 If you publish research that requires you to write a significant amount of code to produce results, then you probably (hopefully) put the code into source control in a public repository. Many journals now require, or at the very least, encourage this practice. But if you think this alone is enough to make your results reproducible by your peers, then think again. Why is this? Here's a few possible reasons another researcher might not be able to first replicate your results and then go on to use your software to extend the research: it isn't clear how you run/use the code the code isn't packaged and thus difficult to install correctly, if at all the code doesn't work on their machine the code works, but the results are different the code works, the results are the same, but it's not at all clear how you would go about using the code to advance or modify the published work Even if a researcher successfully replicates your results, uses, and then and cites your software (somehow), subsequent readers of their research will likely confront the same problems. Often, documentation and testing are overlooked or given a low priority because they \"slow you down\". To this, I say more haste less speed . You'll save more time in future development than you'll lose by producing some documentation and a test framework. Whilst all developers test their code as they write it, not all developers keep those tests and run them regularly. How many times have you changed some code that's had unintended consequences in another part of your software, creating a bug that's gone unnoticed because you didn't keep the original tests, and now you have no idea exactly what is broken or quite why it no longer works? It is also important that your software is open to scrutiny by others. This essentially exposes your methodology at a level of detail generally impractical to describe fully in a paper. If there are any flaws in your methodology, or even bugs, then they are open for peer review. Conversely, aspects of your implementation may be of benefit to other researchers, even if the overall package isn't as relevant to them. All of the potential problems listed above can be solved to a lesser or greater extent using free and open tools that integrate into your project. Where I mention a specific product, it's not necessarily a recommendation, it just happens to be the tool I tend to use. And in most cases other (and perhaps better) products are available. Here's how it all fits together: One you integrate these tools into your ecosystem, and once set up require little or no maintenance. They'll save you enormous amounts of time and effort by providing forewarning of any issues, as well as providing a framework enabling developers to collaborate effectively, and users to feed back into the project by reporting bugs or suggesting improvements. Some, or perhaps all, of what follows may be obvious to some readers and it's largely common sense, but I'm describing the methodology from the ground up for completeness, and for those who are new to the concept of providing software research tools to a wide audience. Developer(s) \u00b6 Even if you work on your own, entertain the possibility that someone, somewhere, may one day want to collaborate on the project. Consider also that there may be long periods where you're not actively developing the software, during which you'll likely forget some of the detail of how it works, what the dependencies are, or how you test it's working correctly. This means documentation, even if it's just some brief notes for yourself. But you may as well write it for a wider audience. I've seen plenty of projects on github that I would've like to contribute to, but didn't as there weren't clear instructions on basic usage or instructions on to how to set up a development environment, and a well-documented test framework. If you want collaborators, you're more likely to get them if they are comfortable they can add value without breaking anything. The obvious place to put this documentation is in the repo itself, starting with README.md . Github (for example) renders markdown into nicely (if somewhat spartanly) formatted web pages, is simple to master, and is well worth getting up to speed in if you haven't already: see e.g. markdown cheatsheet . Source control \u00b6 In the corporate world, teams of developers work very closely together, and often exclusively on a single project, and in this context it often makes sense for them to all work directly in the same repo - when things go wrong they are quickly apparent and quickly resolved. In academia, however, the situation is very different: collaborations are much looser and people will be dipping in an out of various projects. Your collaborators may be people you barely know, geographically separated, and (most crucially) not reporting to the same boss as you! The best model for this situation is the \"fork and pull request\" practice. In other words, you control the \"root\" repo for the project, and only you (or a select few) have permission to commit to this repository. If somebody else wants to collaborate, they \"fork\" your repo, essentially taking a copy of it, do their development in their repo, and when ready they submit a \"pull request\" (PR) which invites you to review their changes and decide if you want to merge their contribution into your repo. Github also has tools in place to ensure their changes don't break anything (see Testing ). This practice can also be used within a single repo, with people developing on branches and submitting PRs merging to the master branch. A note here about good practice - it's generally not a good idea to commit directly to master , which should be the golden copy of your code. Development should be carried out on branches and merged to master when complete and fully tested. In practice, when I'm the only developer on a project I tend to commit small and uncontroversial \"atomic\" changes directly to master , but always use branches for larger and/or potentially breaking changes. Once a branch has been merged to master, it is no longer required and should be deleted. Issues and project management \u00b6 Github has the facility to create \"issues\" against a repo which are very useful for reporting bugs and tracking development. Issues can be categorised, assigned to people, assigned a status, assigned to a particular release, and discussed and commented on. Whilst the functionality is reasonably basic compared to other (often paid for) products, it has the advantage of being completely integrated into the project, and there are tools available to enhance the functionality, such as Zenhub , which adds full Agile project management via a browser plugin. For academic projects with a relatively small number of collaborators, this is probably more than sufficient. Commit messages can be automatically linked to specific issues simply by referencing the issue number prefixed with a '#', which is useful for tracking work done on a particular issue. Additionally, you can automatically close issues via a commit that addresses the issue, by including text like \"fixes #47\" or \"closes #47\" in the body. Looking at the issues boards of other repos is a good (rough) guide to see how robust the software is, how many issues are reported by users (a lot could indicate either wide usage or buggy software, or both!) and how responsive the developers are at addressing them. Testing \u00b6 It goes without saying that you should test your code, have a test harness of some form, and append the test code for any new features to the test suite. And obviously you always run the tests before pushing to the github repo, and when you do your CI (see next section ) runs more comprehensive tests. This section is more concerned more about how you go about automating this process, how to test in a wider range of environments, and how to determine if your tests are giving you a good indication of how robust your software is. Continuous integration \u00b6 Simply put, you register with a continuous integration (CI) provider (e.g. travis , appveyor , circleci ) and point them to your github project and specify some configuration for each provider so they know how to build and test your project. Then, every time you commit, the CI will build and test your code for you and tell you if there's a problem. The first thing this does is give you an assurance that your repo is complete and self-contained - if you've forgotten to commit a file, or if there's a dependency you haven't explicitly specified but happens to be installed on your machine, you'll know pretty soon. Ok, so you've been convinced that regression tests are a good thing (if you weren't already), and you're developing a python project on your linux machine, which has python 3.7 on it, and your tests are fairly comprehensive to you're confident it works... but only on linux, and only with python 3.7. This is where CI comes in - you can configure it to build on multiple platforms and multiple software versions. Different CI providers support different platforms so you may well need to register with multiple providers to cover the major platforms, e.g. Windows, OSX and linux. In terms of software versions aim for 3 - e.g. for R, they recommend testing against the previous release, the current release, and the release in development. Obviously if you do find a problem with a specific platform/version, you may then need to get hold of a machine with that configuration to track down the problem, but often a painstaking trawl through the CI log files is enough to pinpoint the problem. Writing configuration files for CI providers (typically in yaml) is something of a dark art. There are plenty of examples to be found on github and elsewhere though. Test coverage \u00b6 As software evolves, the usage patterns change and the code gets changed or added to but often the tests don't. Thus its easy to stumble into a situation where the code that's being tested is not a good representation of the code that's executed by users. Again there are online tools that integrate with your repo and report area of your codebase that aren't being adequately tested. Static analysis \u00b6 No amount of runtime testing can guarantee to weed out all code errors, so it's important to analyse your code for dubious constructs that cause subtle bugs. This is commonly known as static analysis , since its done without actually running the code. For compiled code, this is doubly important, as there could be undefined behaviour lurking in the code which may (by pure luck) appear to work 99% of the time but cause the application to error or even crash seemingly randomly, for example by accidentally reading over the end of an array. As with CI, online tools are available that can be integrated with the github repository and will automatically analyse each commit and report any issues. They also typically grade your code for quality, which you can advertise on your repo: a high grade will again encourage people to trust (and therefore use) your package, and may also help attract contributors. Codacy is one such provider, and it combines static analysis tools for a variety of languages, sometimes with multiple tools for one language. Often static analysis tools will seem overly pedantic and seem to report a lot of false positives, but sometimes they are highlighting a subtle issue that requires some thought to understand. Otherwise, spurious and overly pedantic issues can be suppressed. Versioning and releases \u00b6 So far all we've done is lay some foundations for good software development but havent really got to grips with the problems outlined at the start. Now we can start getting to the point of this methodology. Journal articles generally have a pretty slow turnaround. If you submit a manuscript containing a reference to some software in a github repository, then it will be at least 3 months and possibly a lot longer before that article is published. In the meantime, unless you've completely stopped development, the software in your repo could have changed substantially, and may not even be able to produce the results you described in the article, so it will be very difficult for any other researcher to replicate your results, short of bisecting the revision history, hoping for some informative commit comments, and guessing the approximate date you ran the code that generated the results you published. A time-consuming and potentially fruitless process for anyone. There is a simple solution, you simply version your software, and refer to a specific version in any published work. git has a concept called a tag, which just identifies a specific commit in a repo with a convenient name. In turn you can use this tag to create a release on github, which creates a zipped copy of the code at the tag, and allows you to create some release notes. Examining commit comments and closed issues since the previous release is a good starting point for making release notes. There are a number of different standards for versioning software, perhaps the most common is semantic versioning (aka semver) which is described here . So if you quoted your package and a version in your article, you've gone a long way to making your work reproducible by others. But there are easier and less error-prone ways of installing software than downloading a zip file, extracting it and manually copying it to a particular location. If you want to upgrade or remove it then will you remember exactly what you did to \"install\" it in the first place? Documentation \u00b6 I've already mentioned the README, and this is a great place to introduce your package and put some brief notes on installation and basic usage. But its important also to provide more detailed examples and, ideally, API documentation. Depending on the size/complexity of your package you might be able to put this all in README.md but it probably makes sense to split it over multiple markdown files. Github markdown rendering is fairly spartan and has limited support for things like tables of contents and equations, if your documentation requires them. If you want very professional-looking documentation it is worth looking into rendering packages that convert markdown to styled html, such as Sphinx or MkDocs , which give you things like themes, a search tool, and tables of contents etc, straight out of the box. The generated documentation can be hosted (free) on github as project pages, or externally on sites such as readthedocs . I personally use mkdocs (in fact this site uses it), which is relatively easy to get up and running, supports configurable themes and supports thing like embedding (\\latex) (for equations) and (python) macros to programmatically generate content. Examples \u00b6 Providing worked examples is arguably the most important part of the documentation as its likely to be the first port of call for new users. The documentation macro feature (in e.g. mkdocs ) is particularly useful for examples, as you can pull in the actual code (rather than having to remember to manually copy/paste the example code in the documentation every time it changes). You can even go as far as actually running example code, inserting the output into the documentation as its generated. This has the added bonus of ensuring your example code works with the current version of the package. You are effectively testing your documentation, a process known as doctest . Some languages (e.g R, rust) support this natively. API documentation \u00b6 Once users are actively integrating your package into their work, they will likely want more detail on how to use specific classes or functions. This is where API documentation comes in. You want to automate this as much as you can so it reflects the current API and manually generating it and keeping it up-to-date is a manual, laborious (and thus error-prone) process. Some documentation packages, or plugins for the packages, purport to be able to automatically generate documentation from your package, but you will probably find that your mileage may vary. In the worst case you can write a script to translate the raw package documentation ( docstrings in python) into markdown. Not ideal, but still better than a purely manual process. Package repositories \u00b6 If you want users to be able to install your software easily, which translates pretty much to \"if you want users\", then you're going to need to package your software and upload it to a public repository. How straightforward this process is depends very much on the repository. In my experience, python package index (PyPI) is the easiest and CRAN (the R repository) the most onerous. PyPI seemingly accepts pretty much anything (including broken packages), whereas CRAN is extremely strict about the package contents and documentation. In between you have things like conda-forge, which will test the package installs on a number of different platforms before it accepts it. Obviously it's imperative that you ensure that you have fully tested the version you are publishing. Container repositories \u00b6 An alternative way of delivering software is to provide it in the form of a container, a.k.a. a docker image. This is in essence a lightweight but full (linux) operating system image pre-installed with your software and all its dependencies. This can be extremely useful if your package has a web API, i.e. an \"app service\" and allows it to be easily and quickly deployed almost anywhere. It is also useful for providing an environment containing ready-to-run examples that aren't part of the installed package. You provide a recipe in the form of a Dockerfile which is essentially a sequence of commands on how to install - from a base OS - all your package's dependencies, build it, set up an environment, and then run it. Another advantage is if you your software requires large datasets from various sources, you can provide a pre-packaged docker volume containing all the data (as long as its publicly available and you have got any necessary permissions to replicate it, of course). Docker hub is one such repository, which (as always) integrates with github and can be configured to automatically generate images for you, including tags (and thus releases). Citations \u00b6 Citations nowadays include a unique Document Origin Identifier (DOI), and this by no means applies only to academic papers. It's also used for both software and data. Thus, assigning a DOI to a specific release of your software constitutes a reference which anyone should be able to trace. What happens if you publish a paper referencing your software, including a version, and then subsequently move the repo elsewhere or even delete it, or pull the release from public repositories? This will leave readers of your article unable to find your work. The beauty of a DOI (and the provider thereof) is that it doesn't matter (so much). Zenodo is a service that (you guessed it) integrates with github and takes a complete snapshot copy of your software, and thus persists even if the original repo or package no longer exists. I would suggest you make it as easy as possible for people to cite you, providing a bibtex-style reference and/or a link to Zenodo (or similar) in your documentation can help. After all, it's people (including yourself, see below ) citing your software that makes it maintaining it worthwhile. Licencing \u00b6 Another aspect of acknowledgement your work is others using your source code, as opposed to your software. This is where licencing comes in. It is important you understand your rights as a a developer of open-source software, and also the rights of users of your software. By licencing your package you protect yourself from public liability and also (typically) legally require anyone to acknowledge you if they base their own software on yours. Most open source licences are quite permissive, even to the point where a third party is allowed to sell your software, as long as your were acknowledged. Github, for example, provides a number of licence templates you can select from, select the one which best suits your situation. Communication \u00b6 For promotion \u00b6 Users and practitioners need to be aware that your software exists. A simple and relatively easy way of promoting it is to publish an article in, for example, the Journal of Open Source Software or the Journal of Open Research Software , neither of which require a full paper, just simply a brief summary of what the package does. Their review process focussed on the software and if you've followed the recommendations here you should have little trouble being accepted. The impact, however, may not be great (who trawls there journals in the hope a package they could use suddenly appears?), so if you can also cite it yourself in an applied paper this will help greatly. Likewise, conferences are a good place to plug your development work, as well as simply encouraging collaborators and colleagues to use (and then cite) it. For development \u00b6 After configuring the tools in your ecosystem to listen for changes in repo, and respond automatically to them, they will typically email you if there's a problem. These days this isn't so reliable as most people are bombarded by emails and read few of them. Perhaps a better way is to use a messaging service (such as slack , as always others are available). Slack can be used as a central point for any issues coming from the services around your package, and of course can also be a place where developers communicate. Slack has many custom integrations which you can add to your channel, such as being able to create and amend github issues directly, and where custom integrations are not available you cant fairly easily create your own via webhooks. The slack channel(s) can also be a good place for communication with end users, which brings me on to... Support \u00b6 It's difficult to know how many people actively use your package (short of incorporating spyware into it). Whilst package repositories will produce a count (or rate) of downloads, I suspect at least some are bots, and of the others, downloading is one thing but actually using is another. But lets assume you have active users, and you have good documentation. You may find that a lot of the support questions could be answered by simply reading the documentation (who's not guilty of this?), or are not specific enough for you to replicate the problem they report. Some form of triage is useful if you get a lot of support questions. You can use an \"issue template\" where users must complete certain questions, describing in detail what they were trying to do, what went wrong, and on which version and platform. This deters people who realise they should look again at the documentation, and stops people raising vague issues along the lines of \"I tried using X but it didn't work\", that simply don't contain enough information to replicate. At the very least, you should make it clear that you cannot/will not help anyone unless they can come up with a reproducible example. After all, how do you fix a bug if you can't actually replicate the bug? Takeaways \u00b6 This may seem like an awful lot of effort to go to, but it does save potentially a lot of future headaches, and it doesn't need to all be done at once. Start with CI, and add other services/features as and when they become useful. Good practice in software- and data-driven research is becoming ever more important and is being taken more and more seriously by more traditional academics. The growth of Research Software Engineering as a discipline in it's own right speaks to this. If I had to summarise what I've written into some very brief key lessons, they would be: if you want other people to use and cite your software, you have do do a lot more than just writing the software, but there is payback all the tools you need are there, you just need to figure out how to use them (if you don't know already) automate the sh!t out of everything you can. It will be less error prone and save you (potentially lots of) time going forward. Scripts are your friend anticipate failure, test everything. The sooner you know something is broken, the better Room for improvement \u00b6 In what I've described, the developer has the ability to release directly from their own development environment to public package repositories. This is error-prone and open to abuse. I could accidentally (or deliberately) create a public release of my package that's not in source control and thus circumvent all the good work around testing and versioning. A better approach would be to release via continuous deployment (CD), a logical extension of CI. In the commercial world there's typically a clear segregation of roles (dev, infrastructure, production/users) but in academia you are often all 3 of them. Implementing CD would at least eliminate accidents, another approach would be for package repositories to only accept releases from verified sources, e.g. releases created on github. In practice \u00b6 To see (most of) this in action \"in the wild\", take a look around the neworder project.","title":"Anatomy of a successful research software project"},{"location":"anatomy/#the-anatomy-of-a-successful-research-software-project","text":"","title":"The anatomy of a successful research software project"},{"location":"anatomy/#motivations","text":"If you publish research that requires you to write a significant amount of code to produce results, then you probably (hopefully) put the code into source control in a public repository. Many journals now require, or at the very least, encourage this practice. But if you think this alone is enough to make your results reproducible by your peers, then think again. Why is this? Here's a few possible reasons another researcher might not be able to first replicate your results and then go on to use your software to extend the research: it isn't clear how you run/use the code the code isn't packaged and thus difficult to install correctly, if at all the code doesn't work on their machine the code works, but the results are different the code works, the results are the same, but it's not at all clear how you would go about using the code to advance or modify the published work Even if a researcher successfully replicates your results, uses, and then and cites your software (somehow), subsequent readers of their research will likely confront the same problems. Often, documentation and testing are overlooked or given a low priority because they \"slow you down\". To this, I say more haste less speed . You'll save more time in future development than you'll lose by producing some documentation and a test framework. Whilst all developers test their code as they write it, not all developers keep those tests and run them regularly. How many times have you changed some code that's had unintended consequences in another part of your software, creating a bug that's gone unnoticed because you didn't keep the original tests, and now you have no idea exactly what is broken or quite why it no longer works? It is also important that your software is open to scrutiny by others. This essentially exposes your methodology at a level of detail generally impractical to describe fully in a paper. If there are any flaws in your methodology, or even bugs, then they are open for peer review. Conversely, aspects of your implementation may be of benefit to other researchers, even if the overall package isn't as relevant to them. All of the potential problems listed above can be solved to a lesser or greater extent using free and open tools that integrate into your project. Where I mention a specific product, it's not necessarily a recommendation, it just happens to be the tool I tend to use. And in most cases other (and perhaps better) products are available. Here's how it all fits together: One you integrate these tools into your ecosystem, and once set up require little or no maintenance. They'll save you enormous amounts of time and effort by providing forewarning of any issues, as well as providing a framework enabling developers to collaborate effectively, and users to feed back into the project by reporting bugs or suggesting improvements. Some, or perhaps all, of what follows may be obvious to some readers and it's largely common sense, but I'm describing the methodology from the ground up for completeness, and for those who are new to the concept of providing software research tools to a wide audience.","title":"Motivations"},{"location":"anatomy/#developers","text":"Even if you work on your own, entertain the possibility that someone, somewhere, may one day want to collaborate on the project. Consider also that there may be long periods where you're not actively developing the software, during which you'll likely forget some of the detail of how it works, what the dependencies are, or how you test it's working correctly. This means documentation, even if it's just some brief notes for yourself. But you may as well write it for a wider audience. I've seen plenty of projects on github that I would've like to contribute to, but didn't as there weren't clear instructions on basic usage or instructions on to how to set up a development environment, and a well-documented test framework. If you want collaborators, you're more likely to get them if they are comfortable they can add value without breaking anything. The obvious place to put this documentation is in the repo itself, starting with README.md . Github (for example) renders markdown into nicely (if somewhat spartanly) formatted web pages, is simple to master, and is well worth getting up to speed in if you haven't already: see e.g. markdown cheatsheet .","title":"Developer(s)"},{"location":"anatomy/#source-control","text":"In the corporate world, teams of developers work very closely together, and often exclusively on a single project, and in this context it often makes sense for them to all work directly in the same repo - when things go wrong they are quickly apparent and quickly resolved. In academia, however, the situation is very different: collaborations are much looser and people will be dipping in an out of various projects. Your collaborators may be people you barely know, geographically separated, and (most crucially) not reporting to the same boss as you! The best model for this situation is the \"fork and pull request\" practice. In other words, you control the \"root\" repo for the project, and only you (or a select few) have permission to commit to this repository. If somebody else wants to collaborate, they \"fork\" your repo, essentially taking a copy of it, do their development in their repo, and when ready they submit a \"pull request\" (PR) which invites you to review their changes and decide if you want to merge their contribution into your repo. Github also has tools in place to ensure their changes don't break anything (see Testing ). This practice can also be used within a single repo, with people developing on branches and submitting PRs merging to the master branch. A note here about good practice - it's generally not a good idea to commit directly to master , which should be the golden copy of your code. Development should be carried out on branches and merged to master when complete and fully tested. In practice, when I'm the only developer on a project I tend to commit small and uncontroversial \"atomic\" changes directly to master , but always use branches for larger and/or potentially breaking changes. Once a branch has been merged to master, it is no longer required and should be deleted.","title":"Source control"},{"location":"anatomy/#issues-and-project-management","text":"Github has the facility to create \"issues\" against a repo which are very useful for reporting bugs and tracking development. Issues can be categorised, assigned to people, assigned a status, assigned to a particular release, and discussed and commented on. Whilst the functionality is reasonably basic compared to other (often paid for) products, it has the advantage of being completely integrated into the project, and there are tools available to enhance the functionality, such as Zenhub , which adds full Agile project management via a browser plugin. For academic projects with a relatively small number of collaborators, this is probably more than sufficient. Commit messages can be automatically linked to specific issues simply by referencing the issue number prefixed with a '#', which is useful for tracking work done on a particular issue. Additionally, you can automatically close issues via a commit that addresses the issue, by including text like \"fixes #47\" or \"closes #47\" in the body. Looking at the issues boards of other repos is a good (rough) guide to see how robust the software is, how many issues are reported by users (a lot could indicate either wide usage or buggy software, or both!) and how responsive the developers are at addressing them.","title":"Issues and project management"},{"location":"anatomy/#testing","text":"It goes without saying that you should test your code, have a test harness of some form, and append the test code for any new features to the test suite. And obviously you always run the tests before pushing to the github repo, and when you do your CI (see next section ) runs more comprehensive tests. This section is more concerned more about how you go about automating this process, how to test in a wider range of environments, and how to determine if your tests are giving you a good indication of how robust your software is.","title":"Testing"},{"location":"anatomy/#continuous-integration","text":"Simply put, you register with a continuous integration (CI) provider (e.g. travis , appveyor , circleci ) and point them to your github project and specify some configuration for each provider so they know how to build and test your project. Then, every time you commit, the CI will build and test your code for you and tell you if there's a problem. The first thing this does is give you an assurance that your repo is complete and self-contained - if you've forgotten to commit a file, or if there's a dependency you haven't explicitly specified but happens to be installed on your machine, you'll know pretty soon. Ok, so you've been convinced that regression tests are a good thing (if you weren't already), and you're developing a python project on your linux machine, which has python 3.7 on it, and your tests are fairly comprehensive to you're confident it works... but only on linux, and only with python 3.7. This is where CI comes in - you can configure it to build on multiple platforms and multiple software versions. Different CI providers support different platforms so you may well need to register with multiple providers to cover the major platforms, e.g. Windows, OSX and linux. In terms of software versions aim for 3 - e.g. for R, they recommend testing against the previous release, the current release, and the release in development. Obviously if you do find a problem with a specific platform/version, you may then need to get hold of a machine with that configuration to track down the problem, but often a painstaking trawl through the CI log files is enough to pinpoint the problem. Writing configuration files for CI providers (typically in yaml) is something of a dark art. There are plenty of examples to be found on github and elsewhere though.","title":"Continuous integration"},{"location":"anatomy/#test-coverage","text":"As software evolves, the usage patterns change and the code gets changed or added to but often the tests don't. Thus its easy to stumble into a situation where the code that's being tested is not a good representation of the code that's executed by users. Again there are online tools that integrate with your repo and report area of your codebase that aren't being adequately tested.","title":"Test coverage"},{"location":"anatomy/#static-analysis","text":"No amount of runtime testing can guarantee to weed out all code errors, so it's important to analyse your code for dubious constructs that cause subtle bugs. This is commonly known as static analysis , since its done without actually running the code. For compiled code, this is doubly important, as there could be undefined behaviour lurking in the code which may (by pure luck) appear to work 99% of the time but cause the application to error or even crash seemingly randomly, for example by accidentally reading over the end of an array. As with CI, online tools are available that can be integrated with the github repository and will automatically analyse each commit and report any issues. They also typically grade your code for quality, which you can advertise on your repo: a high grade will again encourage people to trust (and therefore use) your package, and may also help attract contributors. Codacy is one such provider, and it combines static analysis tools for a variety of languages, sometimes with multiple tools for one language. Often static analysis tools will seem overly pedantic and seem to report a lot of false positives, but sometimes they are highlighting a subtle issue that requires some thought to understand. Otherwise, spurious and overly pedantic issues can be suppressed.","title":"Static analysis"},{"location":"anatomy/#versioning-and-releases","text":"So far all we've done is lay some foundations for good software development but havent really got to grips with the problems outlined at the start. Now we can start getting to the point of this methodology. Journal articles generally have a pretty slow turnaround. If you submit a manuscript containing a reference to some software in a github repository, then it will be at least 3 months and possibly a lot longer before that article is published. In the meantime, unless you've completely stopped development, the software in your repo could have changed substantially, and may not even be able to produce the results you described in the article, so it will be very difficult for any other researcher to replicate your results, short of bisecting the revision history, hoping for some informative commit comments, and guessing the approximate date you ran the code that generated the results you published. A time-consuming and potentially fruitless process for anyone. There is a simple solution, you simply version your software, and refer to a specific version in any published work. git has a concept called a tag, which just identifies a specific commit in a repo with a convenient name. In turn you can use this tag to create a release on github, which creates a zipped copy of the code at the tag, and allows you to create some release notes. Examining commit comments and closed issues since the previous release is a good starting point for making release notes. There are a number of different standards for versioning software, perhaps the most common is semantic versioning (aka semver) which is described here . So if you quoted your package and a version in your article, you've gone a long way to making your work reproducible by others. But there are easier and less error-prone ways of installing software than downloading a zip file, extracting it and manually copying it to a particular location. If you want to upgrade or remove it then will you remember exactly what you did to \"install\" it in the first place?","title":"Versioning and releases"},{"location":"anatomy/#documentation","text":"I've already mentioned the README, and this is a great place to introduce your package and put some brief notes on installation and basic usage. But its important also to provide more detailed examples and, ideally, API documentation. Depending on the size/complexity of your package you might be able to put this all in README.md but it probably makes sense to split it over multiple markdown files. Github markdown rendering is fairly spartan and has limited support for things like tables of contents and equations, if your documentation requires them. If you want very professional-looking documentation it is worth looking into rendering packages that convert markdown to styled html, such as Sphinx or MkDocs , which give you things like themes, a search tool, and tables of contents etc, straight out of the box. The generated documentation can be hosted (free) on github as project pages, or externally on sites such as readthedocs . I personally use mkdocs (in fact this site uses it), which is relatively easy to get up and running, supports configurable themes and supports thing like embedding (\\latex) (for equations) and (python) macros to programmatically generate content.","title":"Documentation"},{"location":"anatomy/#examples","text":"Providing worked examples is arguably the most important part of the documentation as its likely to be the first port of call for new users. The documentation macro feature (in e.g. mkdocs ) is particularly useful for examples, as you can pull in the actual code (rather than having to remember to manually copy/paste the example code in the documentation every time it changes). You can even go as far as actually running example code, inserting the output into the documentation as its generated. This has the added bonus of ensuring your example code works with the current version of the package. You are effectively testing your documentation, a process known as doctest . Some languages (e.g R, rust) support this natively.","title":"Examples"},{"location":"anatomy/#api-documentation","text":"Once users are actively integrating your package into their work, they will likely want more detail on how to use specific classes or functions. This is where API documentation comes in. You want to automate this as much as you can so it reflects the current API and manually generating it and keeping it up-to-date is a manual, laborious (and thus error-prone) process. Some documentation packages, or plugins for the packages, purport to be able to automatically generate documentation from your package, but you will probably find that your mileage may vary. In the worst case you can write a script to translate the raw package documentation ( docstrings in python) into markdown. Not ideal, but still better than a purely manual process.","title":"API documentation"},{"location":"anatomy/#package-repositories","text":"If you want users to be able to install your software easily, which translates pretty much to \"if you want users\", then you're going to need to package your software and upload it to a public repository. How straightforward this process is depends very much on the repository. In my experience, python package index (PyPI) is the easiest and CRAN (the R repository) the most onerous. PyPI seemingly accepts pretty much anything (including broken packages), whereas CRAN is extremely strict about the package contents and documentation. In between you have things like conda-forge, which will test the package installs on a number of different platforms before it accepts it. Obviously it's imperative that you ensure that you have fully tested the version you are publishing.","title":"Package repositories"},{"location":"anatomy/#container-repositories","text":"An alternative way of delivering software is to provide it in the form of a container, a.k.a. a docker image. This is in essence a lightweight but full (linux) operating system image pre-installed with your software and all its dependencies. This can be extremely useful if your package has a web API, i.e. an \"app service\" and allows it to be easily and quickly deployed almost anywhere. It is also useful for providing an environment containing ready-to-run examples that aren't part of the installed package. You provide a recipe in the form of a Dockerfile which is essentially a sequence of commands on how to install - from a base OS - all your package's dependencies, build it, set up an environment, and then run it. Another advantage is if you your software requires large datasets from various sources, you can provide a pre-packaged docker volume containing all the data (as long as its publicly available and you have got any necessary permissions to replicate it, of course). Docker hub is one such repository, which (as always) integrates with github and can be configured to automatically generate images for you, including tags (and thus releases).","title":"Container repositories"},{"location":"anatomy/#citations","text":"Citations nowadays include a unique Document Origin Identifier (DOI), and this by no means applies only to academic papers. It's also used for both software and data. Thus, assigning a DOI to a specific release of your software constitutes a reference which anyone should be able to trace. What happens if you publish a paper referencing your software, including a version, and then subsequently move the repo elsewhere or even delete it, or pull the release from public repositories? This will leave readers of your article unable to find your work. The beauty of a DOI (and the provider thereof) is that it doesn't matter (so much). Zenodo is a service that (you guessed it) integrates with github and takes a complete snapshot copy of your software, and thus persists even if the original repo or package no longer exists. I would suggest you make it as easy as possible for people to cite you, providing a bibtex-style reference and/or a link to Zenodo (or similar) in your documentation can help. After all, it's people (including yourself, see below ) citing your software that makes it maintaining it worthwhile.","title":"Citations"},{"location":"anatomy/#licencing","text":"Another aspect of acknowledgement your work is others using your source code, as opposed to your software. This is where licencing comes in. It is important you understand your rights as a a developer of open-source software, and also the rights of users of your software. By licencing your package you protect yourself from public liability and also (typically) legally require anyone to acknowledge you if they base their own software on yours. Most open source licences are quite permissive, even to the point where a third party is allowed to sell your software, as long as your were acknowledged. Github, for example, provides a number of licence templates you can select from, select the one which best suits your situation.","title":"Licencing"},{"location":"anatomy/#communication","text":"","title":"Communication"},{"location":"anatomy/#for-promotion","text":"Users and practitioners need to be aware that your software exists. A simple and relatively easy way of promoting it is to publish an article in, for example, the Journal of Open Source Software or the Journal of Open Research Software , neither of which require a full paper, just simply a brief summary of what the package does. Their review process focussed on the software and if you've followed the recommendations here you should have little trouble being accepted. The impact, however, may not be great (who trawls there journals in the hope a package they could use suddenly appears?), so if you can also cite it yourself in an applied paper this will help greatly. Likewise, conferences are a good place to plug your development work, as well as simply encouraging collaborators and colleagues to use (and then cite) it.","title":"For promotion"},{"location":"anatomy/#for-development","text":"After configuring the tools in your ecosystem to listen for changes in repo, and respond automatically to them, they will typically email you if there's a problem. These days this isn't so reliable as most people are bombarded by emails and read few of them. Perhaps a better way is to use a messaging service (such as slack , as always others are available). Slack can be used as a central point for any issues coming from the services around your package, and of course can also be a place where developers communicate. Slack has many custom integrations which you can add to your channel, such as being able to create and amend github issues directly, and where custom integrations are not available you cant fairly easily create your own via webhooks. The slack channel(s) can also be a good place for communication with end users, which brings me on to...","title":"For development"},{"location":"anatomy/#support","text":"It's difficult to know how many people actively use your package (short of incorporating spyware into it). Whilst package repositories will produce a count (or rate) of downloads, I suspect at least some are bots, and of the others, downloading is one thing but actually using is another. But lets assume you have active users, and you have good documentation. You may find that a lot of the support questions could be answered by simply reading the documentation (who's not guilty of this?), or are not specific enough for you to replicate the problem they report. Some form of triage is useful if you get a lot of support questions. You can use an \"issue template\" where users must complete certain questions, describing in detail what they were trying to do, what went wrong, and on which version and platform. This deters people who realise they should look again at the documentation, and stops people raising vague issues along the lines of \"I tried using X but it didn't work\", that simply don't contain enough information to replicate. At the very least, you should make it clear that you cannot/will not help anyone unless they can come up with a reproducible example. After all, how do you fix a bug if you can't actually replicate the bug?","title":"Support"},{"location":"anatomy/#takeaways","text":"This may seem like an awful lot of effort to go to, but it does save potentially a lot of future headaches, and it doesn't need to all be done at once. Start with CI, and add other services/features as and when they become useful. Good practice in software- and data-driven research is becoming ever more important and is being taken more and more seriously by more traditional academics. The growth of Research Software Engineering as a discipline in it's own right speaks to this. If I had to summarise what I've written into some very brief key lessons, they would be: if you want other people to use and cite your software, you have do do a lot more than just writing the software, but there is payback all the tools you need are there, you just need to figure out how to use them (if you don't know already) automate the sh!t out of everything you can. It will be less error prone and save you (potentially lots of) time going forward. Scripts are your friend anticipate failure, test everything. The sooner you know something is broken, the better","title":"Takeaways"},{"location":"anatomy/#room-for-improvement","text":"In what I've described, the developer has the ability to release directly from their own development environment to public package repositories. This is error-prone and open to abuse. I could accidentally (or deliberately) create a public release of my package that's not in source control and thus circumvent all the good work around testing and versioning. A better approach would be to release via continuous deployment (CD), a logical extension of CI. In the commercial world there's typically a clear segregation of roles (dev, infrastructure, production/users) but in academia you are often all 3 of them. Implementing CD would at least eliminate accidents, another approach would be for package repositories to only accept releases from verified sources, e.g. releases created on github.","title":"Room for improvement"},{"location":"anatomy/#in-practice","text":"To see (most of) this in action \"in the wild\", take a look around the neworder project.","title":"In practice"},{"location":"bad-algorithms/","text":"Bad algorithms \u00b6 The Fibonacci sequence is an integer series beginning 0, 1 where every successive digit is the sum of the previous two: \\[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55...\\] The definition \\[F\\big(n\\big) = \\left\\{\\begin{matrix} & n & n \\in {0,1} \\\\ & F\\big(n-1\\big) + F\\big(n-2\\big) & n \\in \\mathbb{Z}^{+} \\setminus{1} \\end{matrix}\\right.\\] lends itself directly to a recursive implementation, e.g. in python: def fib_recursive ( n : int ) -> int : global calls calls = calls + 1 if n < 2 : return n return fib_recursive ( n - 1 ) + fib_recursive ( n - 2 ) [file: src/fib.py ] In the python implementation, as python is a dynamically typed language, we need to check that the argument is both a valid type (integral) and a valid value (non-negative). It makes sense to do this only once - the first time the function is called - so the recursive implementation is split into a separate function. The rust implementation, thanks to static typing and an unsigned integral type, doesn't require these checks, although it may overflow if \\(n\\) is high enough and return an incorrect result. However in both cases there is a major problem with the implementation. For each recursive call to fib two further calls to the function are made and thus there is a potential for the call stack to grow exponentially. For large \\(n\\) this will cause the program to run out of resources very quickly. It turns out that, in general, for an input value of \\(n\\) , there will be about \\(\\phi^n\\) calls to fib , where \\(\\phi\\) is the Golden ratio (~1.618). For example this table illustrates the number of calls to the python implementation for a small range of \\(n\\) : //$include md output/fib_recursive_table.md Or in graphical form, the execution time: and the number of calls is clearly the dominant factor causing the execution time: As you can see, for even fairly small n the function is taking prohibitively long to run, and calling itself hundreds of millions of times. In algorithmic complexity terms, this is known as exponential time complexity and should be avoided at all costs. The point I'm trying to make here is that the obvious (but perhaps naiive) implementation of a mathematical function is not always the best approach, and a good (research or otherwise) software engineer should know this. So lets try again. Perhaps a better approach would be to compute the entire sequence up to \\(n\\) and just return the final value, in python something like: ERROR src/fib.py (python) too few/many tags (0) for '!fib-array!' And the execution time is vastly improved: { include_snippet(\"docs/output/fib_array_table.md\") }} The question is, what does this graph tell us about how the execution time of the array Fibonacci algorithm varies with \\(n\\) ?. Looking at the code we can see that there is one loop so it would be reasonable to expect that the algorithm is \\(O\\big(n\\big)\\) (it actually is). But the data is very noisy, so how do we determine that he behaviour is linear, and the slope, so that we can estimate execution time by extrapolation? Data scientists would perhaps initially suggest linear regression, others might simply suggest averaging the execution time over multiple runs, but there is a problem with this: the noise is biased. Here's why. Modern operating systems run hundreds of processes on hardware that has multiple cores and these processes are competing to consume resources. But CPUs can only run a small number of processes simulteneously, so the operating system has to be able to share the CPU resources between all the processes. Whilst operating systems are cleverly designed to hide all this and provide a smooth user experience, there are limits. I'm sure you've all experienced the slowdowns incurred when a process maximises CPU usage or the system runs low on memory. When you are profiling code, however, these effects become much more noticeable, and particularly in interpreted or garbage-collected languages where the language runtime itself could be competing with your code for CPU resources. So using actual time (often called wall time , referring to a wall clock) is a very crude measure of performance. Low-level profiling tools use much more sophisticated metrics including analysis at the hardware level, but these tools require some effort (and sometimes money) to get working, so may not be available to everyone, and may be overkill for a quick benchmark. Any program will have a theoretical minimum execution time for a given platform, when (by chance) it is not impeded by competing processes or threads, and this value is really what we are looking for when we want to determine how the values if \\(n\\) impacts execution time. So there's a simple solution, instead of averaging multiple runs, we should take the minimum of multiple runs - put another way, we need never to worry about a datapoint underestimating the execution time. As you can see from the red points in the graph below, there is a much clearer linear signal emerging using only 3 runs: TODO extrapolation is dangerous... The only drawback of the array Fibonacci algorithm is that the amount of storage required to compute the value grows with \\(n\\) , and all the intermediate results are discarded. Specifically, this algorithm has both linear time compexity and linear space complexity , since both the number of operations and the amount of storage is proportional to \\(n\\) . If we were calculating terms in the sequence for lots of very large \\(n\\) this could potentially be problematic with storage: there could potentially be a lot of redundant computation and duplication of data, plus a lot of repeated memory allocations and deallocations will impact performance. Since there is only one sequence, a better approach could be to have a single cached sequence that is extended as necessary. In C++ you could implement it like so: { include_snippet(\"src/fib.cpp\", \"fib-cached\") }} However, if you tried a similar approach in rust, you would encounter a very unhappy compiler. It's worth looking into the reasons why this is the case. Basically, this code is not threadsafe. Imagine that it's being called roughly simultaneously by two threads, each requiring different values. Let's say thread 1 calls the function asking for the 100th number in the sequence, and the cache contains only 50 values. It will resize the cache to 101 and start to compute the missing values. Now thread 2 might enter the function whilst this is happening, and want the 75th value in the sequence. It will check the cache size and decide it can simply return the approriate value. The problem is, we don't actually know whether that value has been computed yet, so the result may be wrong, or it may (by luck) be correct if thread 1 has got that far. This is an example of undefined behaviour . Programs like this are not deterministic as it's impossible to say where individual threads will be relative to each other during program execution. The results may or may not be incorrect, the program may even crash, and debugging is extremely difficult as it's often impossible to consistently reproduce an error. Undefined behaviour is perhaps the prime motive for the development of rust. The rust compiler is extremely strict and this strictness enables it to detect many situations where undefined behaviour may arise. For this reason rust is often considered a difficult language to learn, especially for those coming from more forgiving languages. All I can say is it's worth it. Even if you go back to your old, more forgiving, language you will look at your code in a new, more defensive, light. We could fix the problem in the C++ code above by blocking multiple threads entering the function (using a construct called a mutex lock). But this is likely to result in degraded performance, and more complicated code, which has a maintenance overhead. The days when it was quicker to cache simple computations (reading precomputed random numbers from a file was once a thing!) are way behind us. The increase in CPU performance over that last decades has been even greater than those memory or even disk performance. So perhaps the best solution might be simpler... going back to the definition, we see that we don't need to store every term, we only need the previous two terms in order to compute the next term. So we should be able to write an algorithm that is linear in time complexity and constant in space complexity. Worse-than-exponential algorithms \u00b6 Perversely I wanted to explore how could implement something even more insanely inefficient than an exponential time algorithm so I had an idea... References \u00b6 Dasgupta, Papadimitriou and Vazirani, Algorithms, McGraw-Hill, 2008. ISBN 978-0-07-352340-8","title":"Bad algorithms"},{"location":"bad-algorithms/#bad-algorithms","text":"The Fibonacci sequence is an integer series beginning 0, 1 where every successive digit is the sum of the previous two: \\[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55...\\] The definition \\[F\\big(n\\big) = \\left\\{\\begin{matrix} & n & n \\in {0,1} \\\\ & F\\big(n-1\\big) + F\\big(n-2\\big) & n \\in \\mathbb{Z}^{+} \\setminus{1} \\end{matrix}\\right.\\] lends itself directly to a recursive implementation, e.g. in python: def fib_recursive ( n : int ) -> int : global calls calls = calls + 1 if n < 2 : return n return fib_recursive ( n - 1 ) + fib_recursive ( n - 2 ) [file: src/fib.py ] In the python implementation, as python is a dynamically typed language, we need to check that the argument is both a valid type (integral) and a valid value (non-negative). It makes sense to do this only once - the first time the function is called - so the recursive implementation is split into a separate function. The rust implementation, thanks to static typing and an unsigned integral type, doesn't require these checks, although it may overflow if \\(n\\) is high enough and return an incorrect result. However in both cases there is a major problem with the implementation. For each recursive call to fib two further calls to the function are made and thus there is a potential for the call stack to grow exponentially. For large \\(n\\) this will cause the program to run out of resources very quickly. It turns out that, in general, for an input value of \\(n\\) , there will be about \\(\\phi^n\\) calls to fib , where \\(\\phi\\) is the Golden ratio (~1.618). For example this table illustrates the number of calls to the python implementation for a small range of \\(n\\) : //$include md output/fib_recursive_table.md Or in graphical form, the execution time: and the number of calls is clearly the dominant factor causing the execution time: As you can see, for even fairly small n the function is taking prohibitively long to run, and calling itself hundreds of millions of times. In algorithmic complexity terms, this is known as exponential time complexity and should be avoided at all costs. The point I'm trying to make here is that the obvious (but perhaps naiive) implementation of a mathematical function is not always the best approach, and a good (research or otherwise) software engineer should know this. So lets try again. Perhaps a better approach would be to compute the entire sequence up to \\(n\\) and just return the final value, in python something like: ERROR src/fib.py (python) too few/many tags (0) for '!fib-array!' And the execution time is vastly improved: { include_snippet(\"docs/output/fib_array_table.md\") }} The question is, what does this graph tell us about how the execution time of the array Fibonacci algorithm varies with \\(n\\) ?. Looking at the code we can see that there is one loop so it would be reasonable to expect that the algorithm is \\(O\\big(n\\big)\\) (it actually is). But the data is very noisy, so how do we determine that he behaviour is linear, and the slope, so that we can estimate execution time by extrapolation? Data scientists would perhaps initially suggest linear regression, others might simply suggest averaging the execution time over multiple runs, but there is a problem with this: the noise is biased. Here's why. Modern operating systems run hundreds of processes on hardware that has multiple cores and these processes are competing to consume resources. But CPUs can only run a small number of processes simulteneously, so the operating system has to be able to share the CPU resources between all the processes. Whilst operating systems are cleverly designed to hide all this and provide a smooth user experience, there are limits. I'm sure you've all experienced the slowdowns incurred when a process maximises CPU usage or the system runs low on memory. When you are profiling code, however, these effects become much more noticeable, and particularly in interpreted or garbage-collected languages where the language runtime itself could be competing with your code for CPU resources. So using actual time (often called wall time , referring to a wall clock) is a very crude measure of performance. Low-level profiling tools use much more sophisticated metrics including analysis at the hardware level, but these tools require some effort (and sometimes money) to get working, so may not be available to everyone, and may be overkill for a quick benchmark. Any program will have a theoretical minimum execution time for a given platform, when (by chance) it is not impeded by competing processes or threads, and this value is really what we are looking for when we want to determine how the values if \\(n\\) impacts execution time. So there's a simple solution, instead of averaging multiple runs, we should take the minimum of multiple runs - put another way, we need never to worry about a datapoint underestimating the execution time. As you can see from the red points in the graph below, there is a much clearer linear signal emerging using only 3 runs: TODO extrapolation is dangerous... The only drawback of the array Fibonacci algorithm is that the amount of storage required to compute the value grows with \\(n\\) , and all the intermediate results are discarded. Specifically, this algorithm has both linear time compexity and linear space complexity , since both the number of operations and the amount of storage is proportional to \\(n\\) . If we were calculating terms in the sequence for lots of very large \\(n\\) this could potentially be problematic with storage: there could potentially be a lot of redundant computation and duplication of data, plus a lot of repeated memory allocations and deallocations will impact performance. Since there is only one sequence, a better approach could be to have a single cached sequence that is extended as necessary. In C++ you could implement it like so: { include_snippet(\"src/fib.cpp\", \"fib-cached\") }} However, if you tried a similar approach in rust, you would encounter a very unhappy compiler. It's worth looking into the reasons why this is the case. Basically, this code is not threadsafe. Imagine that it's being called roughly simultaneously by two threads, each requiring different values. Let's say thread 1 calls the function asking for the 100th number in the sequence, and the cache contains only 50 values. It will resize the cache to 101 and start to compute the missing values. Now thread 2 might enter the function whilst this is happening, and want the 75th value in the sequence. It will check the cache size and decide it can simply return the approriate value. The problem is, we don't actually know whether that value has been computed yet, so the result may be wrong, or it may (by luck) be correct if thread 1 has got that far. This is an example of undefined behaviour . Programs like this are not deterministic as it's impossible to say where individual threads will be relative to each other during program execution. The results may or may not be incorrect, the program may even crash, and debugging is extremely difficult as it's often impossible to consistently reproduce an error. Undefined behaviour is perhaps the prime motive for the development of rust. The rust compiler is extremely strict and this strictness enables it to detect many situations where undefined behaviour may arise. For this reason rust is often considered a difficult language to learn, especially for those coming from more forgiving languages. All I can say is it's worth it. Even if you go back to your old, more forgiving, language you will look at your code in a new, more defensive, light. We could fix the problem in the C++ code above by blocking multiple threads entering the function (using a construct called a mutex lock). But this is likely to result in degraded performance, and more complicated code, which has a maintenance overhead. The days when it was quicker to cache simple computations (reading precomputed random numbers from a file was once a thing!) are way behind us. The increase in CPU performance over that last decades has been even greater than those memory or even disk performance. So perhaps the best solution might be simpler... going back to the definition, we see that we don't need to store every term, we only need the previous two terms in order to compute the next term. So we should be able to write an algorithm that is linear in time complexity and constant in space complexity.","title":"Bad algorithms"},{"location":"bad-algorithms/#worse-than-exponential-algorithms","text":"Perversely I wanted to explore how could implement something even more insanely inefficient than an exponential time algorithm so I had an idea...","title":"Worse-than-exponential algorithms"},{"location":"bad-algorithms/#references","text":"Dasgupta, Papadimitriou and Vazirani, Algorithms, McGraw-Hill, 2008. ISBN 978-0-07-352340-8","title":"References"},{"location":"difficult-data/","text":"How Not to Supply Data \u00b6","title":"How Not to Supply Data"},{"location":"difficult-data/#how-not-to-supply-data","text":"","title":"How Not to Supply Data"},{"location":"float/","text":"Floating-Point Arithmetic \u00b6 \"Computers are an unsuiable tool for doing mathematical computations, and should not be used for such applications\" Famous quote XKCD sums it up quite nicely: Any software developer who writes numerical code needs to have some understanding of how floating-point numbers are represented, because they are a major source of software bugs and issues. Almost all hardware and software architectures implement the IEEE754 standard. In this chapter I shall explain how a floating-point number is represented using this standard, what the limitations of the format are, and how these can cause some quite nasty software bugs, where the source of errors arise, and how to minimise the impact of these limitations. Some people have a tendency to dismiss floating-point issues as \"its a rounding error, you get them with floating-point\" but that belies a lack fo understanding as to when simply dismisses the potential problem Consider this motivating example of a very simple floating-point expression: Python 3.7.5 (default, Nov 20 2019, 09:21:52) [GCC 9.2.1 20191008] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> 0.5-0.4-0.1 -2.7755575615628914e-17 >>> Feel free to try this in your language/platform of choice, chances are you will get the same wrong answer. So why can't a computer get such a simple sum correct? You can just dismiss it as a rounding error and get on with your life. Unless of course you have derivative traders screaming at you because their risk numbers are all over the place and they can't hedge their positions. Turns out the pricing model is a nonlinear optimisation (requiring matrix inversions) with an objective function that's itself a Monte-Carlo simulation. Then you can't. The IEEE754 Standard \u00b6 Here we will only consider the double precision format, as it's by far the most widely used. Single, half and quad precision all suffer from the same issues. Double-precision floating-point numbers are represented in 64 bits as follows: a single sign bit s , with zero representing positive and 1 negative. an 11-bit exponent e a 52-bit mantissa m The value 1 looks like this in binary: \\[\\underbrace{\\boxed{0}}_s\\underbrace{\\boxed{01111111111}}_e\\underbrace{\\boxed{0000000000000000000000000000000000000000000000000000}}_m\\] In other words, \\[ x = \\big(-1\\big)^{s}.2^{e-1023}.(1 + m/(2^{52})) \\] The exponent needs some explanation. It is biased by 1023, in other words a value of 1023 represents an exponent of one. It has two special values that are interpreted differently, namely 0 and 2047, which we shall come to later. A value of 1 represents an exponent of \\(2^{-1022}\\) or approximately \\(2.23\\times10^{-308}\\) and a value of 2046 is \\(2^{1023}\\) or approximately \\(8.99\\times10^{307}\\) . Thus the format can represent number over a range of well over 600 decimal orders of magnitude. The mantissa is interpreted as a binary number in the range \\(\\big[1,2\\big)\\) , with the leading 1 implied. The bits thus represent the fraction after the decimal point. This is why double precision is often referred to as having 53 bits of precision when the length of the mantissa is actually 52. Something of a moot point when one of those bits is fixed. Here are the bit representations of some other numbers n bits 0 \\(\\boxed{0}\\boxed{00000000000}\\boxed{0000000000000000000000000000000000000000000000000000}\\) 2 \\(\\boxed{0}\\boxed{10000000000}\\boxed{0000000000000000000000000000000000000000000000000000}\\) -1 \\(\\boxed{1}\\boxed{01111111111}\\boxed{0000000000000000000000000000000000000000000000000000}\\) 1000000 \\(\\boxed{0}\\boxed{10000010010}\\boxed{1110100001001000000000000000000000000000000000000000}\\) -0.0005 \\(\\boxed{1}\\boxed{01111110100}\\boxed{0000011000100100110111010010111100011010100111111100}\\) It is important to understand that the number is represented in binary format, not decimal or any other base. If you consider decimal, the fractions that can be represented exactly in a finite number of decimal digits are with denominator componsed of the prime factors of 10, namely 2 or 5, and products thereof. Thus n/2, n/4, n/5, n/8, n/10, n/16, n/20, n/25 etc., can all be represented exactly in finite precision. For binary, only fractions that have a denominator that is a power of 2 can be represented exactly. Number systems such as sexagesimal evolved largely because it divides easily for a range of denominators. It should be noted that binary (along with any other prime base) is the worst for this. Some other points to note: all integers that fit into 53 or fewer binary bits (not including the sign bit) can be represented exactly in this format. there are representations for both positive and negative zero. Epsilon \u00b6 In this context, epsilon is a measure of the precsion of the format. Specifically, it is the smallest number you can add to to a value of 1 to get a value that is not 1. You can show that it's equivalent to the length of the mantissa. In binary, it is the number we add to increase \\[ \\boxed{0}\\boxed{01111111111}\\boxed{0000000000000000000000000000000000000000000000000000} \\] to \\[ \\boxed{0}\\boxed{01111111111}\\boxed{0000000000000000000000000000000000000000000000000001} \\] We're simply increasing the least significant bit. The difference between the two (see the equation above), is \\(1/2^{52}\\) , or about \\(2.22\\times10^{-16}\\) in decimal. This number is approximately 1.00000000000000022. Another way of looking at this is that we can represent \\(2^{52}\\) equally spaced values over the interval of the exponent, which is \\(\\big[1,2\\big)\\) . Thus the difference between successive values is \\(\\big(2-1\\big)/2^{52}\\) . Now consider what the smallest value we can subtract from one and not get a different number, we are now in the exponent range \\(\\big[0.5,\\big)\\) , and thus the difference between successive values is halved. This we can represent a value of (approximately) 0.999999999999999889. Similarly, for larger numbers the exponent range is greater, and thus the distance between successive value is proportionately larger, but there are always \\(2^{52}\\) discrete values available for a given exponent. We'll can define the spacing around any value as follows: \\[ \\delta\\big(x\\big) = 2^{\\lfloor{log_2\\big(x\\big)}\\rfloor - 52} \\] Now consider the assignment from string literals: Python 3.7 . 5 ( default , Nov 20 2019 , 09 : 21 : 52 ) [ GCC 9.2 . 1 20191008 ] on linux Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information . >>> x = 0.4 >>> print ( \" %.18f \" % x ) 0.400000000000000022 >>> x = 0.1 >>> print ( \" %.18f \" % x ) 0.100000000000000006 >>> As you can see neither 0.4 nor 0.1 are represented exactly, and we can be get error bounds: Desired x Representation of x 0.4 \\(0.4 \\pm \\delta\\big(0.4\\big)\\) 0.1 \\(0.1 \\pm \\delta\\big(0.1\\big)\\) Looking at the binary representations of the values we can see that the only difference is in the exponent: \\(\\boxed{0}\\boxed{01111111101}\\boxed{1001100110011001100110011001100110011001100110011010}\\) \\(\\boxed{0}\\boxed{01111111011}\\boxed{1001100110011001100110011001100110011001100110011010}\\) This makes sense since the numbers are exactly two binary orders of magnitude different. Thus the errors in approximating 0.1 must be exactly a quarter of that approximating 0.4. Ordering \u00b6 +/- zero inf nan References \u00b6 What every computer scientist should know about floating-point: https://www.itu.dk/~sestoft/bachelor/IEEE754_article.pdf","title":"Floating-Point Arithmetic"},{"location":"float/#floating-point-arithmetic","text":"\"Computers are an unsuiable tool for doing mathematical computations, and should not be used for such applications\" Famous quote XKCD sums it up quite nicely: Any software developer who writes numerical code needs to have some understanding of how floating-point numbers are represented, because they are a major source of software bugs and issues. Almost all hardware and software architectures implement the IEEE754 standard. In this chapter I shall explain how a floating-point number is represented using this standard, what the limitations of the format are, and how these can cause some quite nasty software bugs, where the source of errors arise, and how to minimise the impact of these limitations. Some people have a tendency to dismiss floating-point issues as \"its a rounding error, you get them with floating-point\" but that belies a lack fo understanding as to when simply dismisses the potential problem Consider this motivating example of a very simple floating-point expression: Python 3.7.5 (default, Nov 20 2019, 09:21:52) [GCC 9.2.1 20191008] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> 0.5-0.4-0.1 -2.7755575615628914e-17 >>> Feel free to try this in your language/platform of choice, chances are you will get the same wrong answer. So why can't a computer get such a simple sum correct? You can just dismiss it as a rounding error and get on with your life. Unless of course you have derivative traders screaming at you because their risk numbers are all over the place and they can't hedge their positions. Turns out the pricing model is a nonlinear optimisation (requiring matrix inversions) with an objective function that's itself a Monte-Carlo simulation. Then you can't.","title":"Floating-Point Arithmetic"},{"location":"float/#the-ieee754-standard","text":"Here we will only consider the double precision format, as it's by far the most widely used. Single, half and quad precision all suffer from the same issues. Double-precision floating-point numbers are represented in 64 bits as follows: a single sign bit s , with zero representing positive and 1 negative. an 11-bit exponent e a 52-bit mantissa m The value 1 looks like this in binary: \\[\\underbrace{\\boxed{0}}_s\\underbrace{\\boxed{01111111111}}_e\\underbrace{\\boxed{0000000000000000000000000000000000000000000000000000}}_m\\] In other words, \\[ x = \\big(-1\\big)^{s}.2^{e-1023}.(1 + m/(2^{52})) \\] The exponent needs some explanation. It is biased by 1023, in other words a value of 1023 represents an exponent of one. It has two special values that are interpreted differently, namely 0 and 2047, which we shall come to later. A value of 1 represents an exponent of \\(2^{-1022}\\) or approximately \\(2.23\\times10^{-308}\\) and a value of 2046 is \\(2^{1023}\\) or approximately \\(8.99\\times10^{307}\\) . Thus the format can represent number over a range of well over 600 decimal orders of magnitude. The mantissa is interpreted as a binary number in the range \\(\\big[1,2\\big)\\) , with the leading 1 implied. The bits thus represent the fraction after the decimal point. This is why double precision is often referred to as having 53 bits of precision when the length of the mantissa is actually 52. Something of a moot point when one of those bits is fixed. Here are the bit representations of some other numbers n bits 0 \\(\\boxed{0}\\boxed{00000000000}\\boxed{0000000000000000000000000000000000000000000000000000}\\) 2 \\(\\boxed{0}\\boxed{10000000000}\\boxed{0000000000000000000000000000000000000000000000000000}\\) -1 \\(\\boxed{1}\\boxed{01111111111}\\boxed{0000000000000000000000000000000000000000000000000000}\\) 1000000 \\(\\boxed{0}\\boxed{10000010010}\\boxed{1110100001001000000000000000000000000000000000000000}\\) -0.0005 \\(\\boxed{1}\\boxed{01111110100}\\boxed{0000011000100100110111010010111100011010100111111100}\\) It is important to understand that the number is represented in binary format, not decimal or any other base. If you consider decimal, the fractions that can be represented exactly in a finite number of decimal digits are with denominator componsed of the prime factors of 10, namely 2 or 5, and products thereof. Thus n/2, n/4, n/5, n/8, n/10, n/16, n/20, n/25 etc., can all be represented exactly in finite precision. For binary, only fractions that have a denominator that is a power of 2 can be represented exactly. Number systems such as sexagesimal evolved largely because it divides easily for a range of denominators. It should be noted that binary (along with any other prime base) is the worst for this. Some other points to note: all integers that fit into 53 or fewer binary bits (not including the sign bit) can be represented exactly in this format. there are representations for both positive and negative zero.","title":"The IEEE754 Standard"},{"location":"float/#epsilon","text":"In this context, epsilon is a measure of the precsion of the format. Specifically, it is the smallest number you can add to to a value of 1 to get a value that is not 1. You can show that it's equivalent to the length of the mantissa. In binary, it is the number we add to increase \\[ \\boxed{0}\\boxed{01111111111}\\boxed{0000000000000000000000000000000000000000000000000000} \\] to \\[ \\boxed{0}\\boxed{01111111111}\\boxed{0000000000000000000000000000000000000000000000000001} \\] We're simply increasing the least significant bit. The difference between the two (see the equation above), is \\(1/2^{52}\\) , or about \\(2.22\\times10^{-16}\\) in decimal. This number is approximately 1.00000000000000022. Another way of looking at this is that we can represent \\(2^{52}\\) equally spaced values over the interval of the exponent, which is \\(\\big[1,2\\big)\\) . Thus the difference between successive values is \\(\\big(2-1\\big)/2^{52}\\) . Now consider what the smallest value we can subtract from one and not get a different number, we are now in the exponent range \\(\\big[0.5,\\big)\\) , and thus the difference between successive values is halved. This we can represent a value of (approximately) 0.999999999999999889. Similarly, for larger numbers the exponent range is greater, and thus the distance between successive value is proportionately larger, but there are always \\(2^{52}\\) discrete values available for a given exponent. We'll can define the spacing around any value as follows: \\[ \\delta\\big(x\\big) = 2^{\\lfloor{log_2\\big(x\\big)}\\rfloor - 52} \\] Now consider the assignment from string literals: Python 3.7 . 5 ( default , Nov 20 2019 , 09 : 21 : 52 ) [ GCC 9.2 . 1 20191008 ] on linux Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information . >>> x = 0.4 >>> print ( \" %.18f \" % x ) 0.400000000000000022 >>> x = 0.1 >>> print ( \" %.18f \" % x ) 0.100000000000000006 >>> As you can see neither 0.4 nor 0.1 are represented exactly, and we can be get error bounds: Desired x Representation of x 0.4 \\(0.4 \\pm \\delta\\big(0.4\\big)\\) 0.1 \\(0.1 \\pm \\delta\\big(0.1\\big)\\) Looking at the binary representations of the values we can see that the only difference is in the exponent: \\(\\boxed{0}\\boxed{01111111101}\\boxed{1001100110011001100110011001100110011001100110011010}\\) \\(\\boxed{0}\\boxed{01111111011}\\boxed{1001100110011001100110011001100110011001100110011010}\\) This makes sense since the numbers are exactly two binary orders of magnitude different. Thus the errors in approximating 0.1 must be exactly a quarter of that approximating 0.4.","title":"Epsilon"},{"location":"float/#ordering","text":"+/- zero inf nan","title":"Ordering"},{"location":"float/#references","text":"What every computer scientist should know about floating-point: https://www.itu.dk/~sestoft/bachelor/IEEE754_article.pdf","title":"References"},{"location":"sourdough/","text":"Sourdough Bread \u00b6 Refreshing the Mother \u00b6 Making the preferment \u00b6 (aka spoge) Making the dough \u00b6 Autolyse \u00b6 Knead \u00b6 Fold and First Proof \u00b6 Shape \u00b6 Overnight Retardation \u00b6 Bake \u00b6 Eat \u00b6","title":"Sourdough Bread"},{"location":"sourdough/#sourdough-bread","text":"","title":"Sourdough Bread"},{"location":"sourdough/#refreshing-the-mother","text":"","title":"Refreshing the Mother"},{"location":"sourdough/#making-the-preferment","text":"(aka spoge)","title":"Making the preferment"},{"location":"sourdough/#making-the-dough","text":"","title":"Making the dough"},{"location":"sourdough/#autolyse","text":"","title":"Autolyse"},{"location":"sourdough/#knead","text":"","title":"Knead"},{"location":"sourdough/#fold-and-first-proof","text":"","title":"Fold and First Proof"},{"location":"sourdough/#shape","text":"","title":"Shape"},{"location":"sourdough/#overnight-retardation","text":"","title":"Overnight Retardation"},{"location":"sourdough/#bake","text":"","title":"Bake"},{"location":"sourdough/#eat","text":"","title":"Eat"},{"location":"output/fib_generator_table/","text":"n F(n) Time(ms) 10 55 0.0114441 20 6765 0.00596046 30 832040 0.0138283 40 102334155 0.0114441 50 12586269025 0.0162125 60 1548008755920 0.0150204 70 190392490709135 0.015974 80 23416728348467685 0.0178814 90 2880067194370816120 0.0200272","title":"Fib generator table"},{"location":"output/fib_recursive_table/","text":"n F(n) Calls Time(ms) 10 55 177 0.159979 15 610 1973 1.31249 20 6765 21891 13.5508 25 75025 242785 141.565 30 832040 2692537 1680.88","title":"Fib recursive table"}]}