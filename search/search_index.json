{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Research Reproducibility Ramblings \u00b6 Articles looking at aspects of and issues related to research software development, from data to software and algorithms, to platforms and hardware, with a particular focus on reproducibility. Anatomy of a successful research software project \u00b6 Bad algorithms \u00b6 Numerical (ir)reproducibility \u00b6 How (not) to supply data \u00b6","title":"Research Reproducibility Ramblings"},{"location":"#research-reproducibility-ramblings","text":"Articles looking at aspects of and issues related to research software development, from data to software and algorithms, to platforms and hardware, with a particular focus on reproducibility.","title":"Research Reproducibility Ramblings"},{"location":"#anatomy-of-a-successful-research-software-project","text":"","title":"Anatomy of a successful research software project"},{"location":"#bad-algorithms","text":"","title":"Bad algorithms"},{"location":"#numerical-irreproducibility","text":"","title":"Numerical (ir)reproducibility"},{"location":"#how-not-to-supply-data","text":"","title":"How (not) to supply data"},{"location":"about/","text":"About me \u00b6 Physicist turned investment banker turned academic researcher. Mission: software and practices that promote reproducibility, reusability and efficiency in academic research Currently: Research Fellow at the University of Leeds, using microsimulation and agent-based models to investigate the interaction between Police resourcing (supply) and crime (demand). Selected open-source projects \u00b6 neworder : a high-performance framework for dynamic microsimulation models humanleague : a microsynthesis package using quasirandom sampling and iterative propotional fitting (IPF) ukcensusapi : UK Census Data queries and downloads from python or R ukpopulation : download, cache, collate, filter, manipulate and extrapolate UK population and household estimates/projections Qualifications \u00b6 Publications \u00b6 Top tip, Viz , c.2002","title":"About me"},{"location":"about/#about-me","text":"Physicist turned investment banker turned academic researcher. Mission: software and practices that promote reproducibility, reusability and efficiency in academic research Currently: Research Fellow at the University of Leeds, using microsimulation and agent-based models to investigate the interaction between Police resourcing (supply) and crime (demand).","title":"About me"},{"location":"about/#selected-open-source-projects","text":"neworder : a high-performance framework for dynamic microsimulation models humanleague : a microsynthesis package using quasirandom sampling and iterative propotional fitting (IPF) ukcensusapi : UK Census Data queries and downloads from python or R ukpopulation : download, cache, collate, filter, manipulate and extrapolate UK population and household estimates/projections","title":"Selected open-source projects"},{"location":"about/#qualifications","text":"","title":"Qualifications"},{"location":"about/#publications","text":"Top tip, Viz , c.2002","title":"Publications"},{"location":"anatomy/","text":"Anatomy of a successful research software project \u00b6 Motivations \u00b6 If you publish research that requires you to write a significant amount of code to produce results, then you probably (hopefully) put the code into source control in a public repository. Many journals now require, or at the very least, encourage this practice. But if you think this alone is enough to make your results reproducible by your peers, then think again. Why is this? Here's a few possible reasons another researcher might not be able to first replicate your results and then go on to use your software to extend the research: it isn't clear how you run/use the code the code isn't packaged and thus difficult to install correctly, if at all the code doesn't work on their machine the code runs, but the results are different the code works - the results are the same - but it's not at all clear how you would go about using the package to advance or modify the published work Even if a researcher successfully replicates your results, uses, and then and cites your software (somehow), subsequent readers of their research will likely confront the same problems. Often, documentation and testing are overlooked or given a low priority because they \"slow you down\". To this, I say more haste less speed . You'll save more time in future development than you'll lose by producing some documentation and a test framework. Whilst all developers test their code as they write it, not all developers keep those tests and run them regularly. How many times have you changed some code that's had unintended consequences in another part of your software, creating a bug that's gone unnoticed because you didn't keep the original tests, and now you have no idea exactly what is broken or quite why it no longer works? It is also important that your software is open to scrutiny by others. This essentially exposes your methodology at a level of detail generally impractical to describe fully in a paper. If there are any flaws in your methodology, or even bugs, then they are open for peer review. Conversely, aspects of your implementation may be of benefit to other researchers, even if the overall package isn't as relevant to them. All of the potential problems listed above can be solved to a lesser or greater extent using free and open tools that integrate into your project. Where I mention a specific product, it's not necessarily a recommendation, it just happens to be the tool I tend to use. And in most cases other (and perhaps better) products are available. Here's how it all fits together: One you integrate these tools into your ecosystem, and once set up require little or no maintenance. They'll save you enormous amounts of time and effort by providing forewarning of any issues, as well as providing a framework enabling developers to collaborate effectively, and users to feed back into the project by reporting bugs or suggesting improvements. Some, or perhaps all, of what follows may be obvious to some readers and it's largely common sense, but I'm describing the methodology from the ground up for completeness, and for those who are new to the concept of providing software research tools to a wide audience. Developer(s) \u00b6 Even if you work on your own, entertain the possibility that someone, somewhere, may one day want to collaborate on the project. Consider also that there may be long periods where you're not actively developing the software, during which you'll likely forget some of the detail of how it works, what the dependencies are, or how you test it's working correctly. This means documentation, even if it's just some brief notes for yourself. But you may as well write it for a wider audience. I've seen plenty of projects on github that I would've like to contribute to, but didn't as there weren't clear instructions on basic usage or instructions on to how to set up a development environment, and a well-documented test framework. If you want collaborators, you're more likely to get them if they are comfortable they can add value without breaking anything. The obvious place to put this documentation is in the repo itself, starting with README.md . Github (for example) renders markdown into nicely (if somewhat spartanly) formatted web pages, is simple to master, and is well worth getting up to speed in if you haven't already: see e.g. markdown cheatsheet . Source control \u00b6 In the corporate world, teams of developers work very closely together, and often exclusively on a single project, and in this context it often makes sense for them to all work directly in the same repo - when things go wrong they are quickly apparent and quickly resolved. In academia, however, the situation is very different: collaborations are much looser and people will be dipping in an out of various projects. Your collaborators may be people you barely know, geographically separated, and (most crucially) not reporting to the same boss as you! The best model for this situation is the \"fork and pull request\" practice. In other words, you control the \"root\" repo for the project, and only you (or a select few) have permission to commit to this repository. If somebody else wants to collaborate, they \"fork\" your repo, essentially taking a copy of it, do their development in their repo, and when ready they submit a \"pull request\" (PR) which invites you to review their changes and decide if you want to merge their contribution into your repo. Github also has tools in place to ensure their changes don't break anything (see Testing ). This practice can also be used within a single repo, with people developing on branches and submitting PRs merging to the master branch. A note here about good practice - it's generally not a good idea to commit directly to master , which should be the golden copy of your code. Development should be carried out on branches and merged to master when complete and fully tested. In practice, when I'm the only developer on a project I tend to commit small and uncontroversial \"atomic\" changes directly to master , but always use branches for larger and/or potentially breaking changes. Once a branch has been merged to master, it is no longer required and should be deleted. Issues and project management \u00b6 Github has the facility to create \"issues\" against a repo which are very useful for reporting bugs and tracking development. Issues can be categorised, assigned to people, assigned a status, assigned to a particular release, and discussed and commented on. Whilst the functionality is reasonably basic compared to other (often paid for) products, it has the advantage of being completely integrated into the project, and there are tools available to enhance the functionality, such as Zenhub , which adds full Agile project management via a browser plugin. For academic projects with a relatively small number of collaborators, this is probably more than sufficient. Commit messages can be automatically linked to specific issues simply by referencing the issue number prefixed with a '#', which is useful for tracking work done on a particular issue. Additionally, you can automatically close issues via a commit that addresses the issue, by including text like \"fixes #47\" or \"closes #47\" in the body. Looking at the issues boards of other repos is a good (rough) guide to see how robust the software is, how many issues are reported by users (a lot could indicate either wide usage or buggy software, or both!) and how responsive the developers are at addressing them. Testing \u00b6 It goes without saying that you should test your code, have a test harness of some form, and append the test code for any new features to the test suite. And obviously you always run the tests before pushing to the github repo, and when you do your CI (see next section ) runs more comprehensive tests. This section is more concerned more about how you go about automating this process, how to test in a wider range of environments, and how to determine if your tests are giving you a good indication of how robust your software is. Continuous integration \u00b6 Simply put, you register with a continuous integration (CI) provider (e.g. travis , appveyor , circleci ) and point them to your github project and specify some configuration for each provider so they know how to build and test your project. Then, every time you commit, the CI will build and test your code for you and tell you if there's a problem. The first thing this does is give you an assurance that your repo is complete and self-contained - if you've forgotten to commit a file, or if there's a dependency you haven't explicitly specified but happens to be installed on your machine, you'll know pretty soon. Ok, so you've been convinced that regression tests are a good thing (if you weren't already), and you're developing a python project on your linux machine, which has python 3.7 on it, and your tests are fairly comprehensive to you're confident it works... but only on linux, and only with python 3.7. This is where CI comes in - you can configure it to build on multiple platforms and multiple software versions. Different CI providers support different platforms so you may well need to register with multiple providers to cover the major platforms, e.g. Windows, OSX and linux. In terms of software versions aim for 3 - e.g. for R, they recommend testing against the previous release, the current release, and the release in development. Obviously if you do find a problem with a specific platform/version, you may then need to get hold of a machine with that configuration to track down the problem, but often a painstaking trawl through the CI log files is enough to pinpoint the problem. Writing configuration files for CI providers (typically in yaml) is something of a dark art. There are plenty of examples to be found on github and elsewhere though. Test coverage \u00b6 As software evolves, the usage patterns change and the code gets changed or added to but often the tests don't. Thus its easy to stumble into a situation where the code that's being tested is not a good representation of the code that's executed by users. Again there are online tools that integrate with your repo and report area of your codebase that aren't being adequately tested. Static analysis \u00b6 No amount of runtime testing can guarantee to weed out all code errors, so it's important to analyse your code for dubious constructs that cause subtle bugs. This is commonly known as static analysis , since its done without actually running the code. For compiled code, this is doubly important, as there could be undefined behaviour lurking in the code which may (by pure luck) appear to work 99% of the time but cause the application to error or even crash seemingly randomly, for example by accidentally reading over the end of an array. As with CI, online tools are available that can be integrated with the github repository and will automatically analyse each commit and report any issues. They also typically grade your code for quality, which you can advertise on your repo: a high grade will again encourage people to trust (and therefore use) your package, and may also help attract contributors. Codacy is one such provider, and it combines static analysis tools for a variety of languages, sometimes with multiple tools for one language. Often static analysis tools will seem overly pedantic and seem to report a lot of false positives, but sometimes they are highlighting a subtle issue that requires some thought to understand. Otherwise, spurious and overly pedantic issues can be suppressed. Versioning and releases \u00b6 So far all we've done is lay some foundations for good software development but havent really got to grips with the problems outlined at the start. Now we can start getting to the point of this methodology. Journal articles generally have a pretty slow turnaround. If you submit a manuscript containing a reference to some software in a github repository, then it will be at least 3 months and possibly a lot longer before that article is published. In the meantime, unless you've completely stopped development, the software in your repo could have changed substantially, and may not even be able to produce the results you described in the article, so it will be very difficult for any other researcher to replicate your results, short of bisecting the revision history, hoping for some informative commit comments, and guessing the approximate date you ran the code that generated the results you published. A time-consuming and potentially fruitless process for anyone. There is a simple solution, you simply version your software, and refer to a specific version in any published work. git has a concept called a tag, which just identifies a specific commit in a repo with a convenient name. In turn you can use this tag to create a release on github, which creates a zipped copy of the code at the tag, and allows you to create some release notes. Examining commit comments and closed issues since the previous release is a good starting point for making release notes. There are a number of different standards for versioning software, perhaps the most common is semantic versioning (aka semver) which is described here . So if you quoted your package and a version in your article, you've gone a long way to making your work reproducible by others. But there are easier and less error-prone ways of installing software than downloading a zip file, extracting it and manually copying it to a particular location. If you want to upgrade or remove it then will you remember exactly what you did to \"install\" it in the first place? Documentation \u00b6 I've already mentioned the README, and this is a great place to introduce your package and put some brief notes on installation and basic usage. But its important also to provide more detailed examples and, ideally, API documentation. Depending on the size/complexity of your package you might be able to put this all in README.md but it probably makes sense to split it over multiple markdown files. Github markdown rendering is fairly spartan and has limited support for things like tables of contents and equations, if your documentation requires them. If you want very professional-looking documentation it is worth looking into rendering packages that convert markdown to styled html, such as Sphinx or MkDocs , which give you things like themes, a search tool, and tables of contents etc, straight out of the box. The generated documentation can be hosted (free) on github as project pages, or externally on sites such as readthedocs . I personally use mkdocs (in fact this site uses it), which is relatively easy to get up and running, supports configurable themes and supports thing like embedding (\\latex) (for equations) and (python) macros to programmatically generate content. Examples \u00b6 Providing worked examples is arguably the most important part of the documentation as its likely to be the first port of call for new users. The documentation macro feature (in e.g. mkdocs ) is particularly useful for examples, as you can pull in the actual code (rather than having to remember to manually copy/paste the example code in the documentation every time it changes). You can even go as far as actually running example code, inserting the output into the documentation as its generated. This has the added bonus of ensuring your example code works with the current version of the package. You are effectively testing your documentation, a process known as doctest . Some languages (e.g R, rust) support this natively. API documentation \u00b6 Once users are actively integrating your package into their work, they will likely want more detail on how to use specific classes or functions. This is where API documentation comes in. You want to automate this as much as you can so it reflects the current API and manually generating it and keeping it up-to-date is a manual, laborious (and thus error-prone) process. Some documentation packages, or plugins for the packages, purport to be able to automatically generate documentation from your package, but you will probably find that your mileage may vary. In the worst case you can write a script to translate the raw package documentation ( docstrings in python) into markdown. Not ideal, but still better than a purely manual process. Package repositories \u00b6 If you want users to be able to install your software easily, which translates pretty much to \"if you want users\", then you're going to need to package your software and upload it to a public repository. How straightforward this process is depends very much on the repository. In my experience, python package index (PyPI) is the easiest and CRAN (the R repository) the most onerous. PyPI seemingly accepts pretty much anything (including broken packages), whereas CRAN is extremely strict about the package contents and documentation. In between you have things like conda-forge, which will test the package installs on a number of different platforms before it accepts it. Obviously it's imperative that you ensure that you have fully tested the version you are publishing. Container repositories \u00b6 An alternative way of delivering software is to provide it in the form of a container, a.k.a. a docker image. This is in essence a lightweight but full (linux) operating system image pre-installed with your software and all its dependencies. This can be extremely useful if your package has a web API, i.e. an \"app service\" and allows it to be easily and quickly deployed almost anywhere. It is also useful for providing an environment containing ready-to-run examples that aren't part of the installed package. You provide a recipe in the form of a Dockerfile which is essentially a sequence of commands on how to install - from a base OS - all your package's dependencies, build it, set up an environment, and then run it. Another advantage is if you your software requires large datasets from various sources, you can provide a pre-packaged docker volume containing all the data (as long as its publicly available and you have got any necessary permissions to replicate it, of course). Docker hub is one such repository, which (as always) integrates with github and can be configured to automatically generate images for you, including tags (and thus releases). Citations \u00b6 Citations nowadays include a unique Document Origin Identifier (DOI), and this by no means applies only to academic papers. It's also used for both software and data. Thus, assigning a DOI to a specific release of your software constitutes a reference which anyone should be able to trace. What happens if you publish a paper referencing your software, including a version, and then subsequently move the repo elsewhere or even delete it, or pull the release from public repositories? This will leave readers of your article unable to find your work. The beauty of a DOI (and the provider thereof) is that it doesn't matter (so much). Zenodo is a service that (you guessed it) integrates with github and takes a complete snapshot copy of your software, and thus persists even if the original repo or package no longer exists. I would suggest you make it as easy as possible for people to cite you, providing a bibtex-style reference and/or a link to Zenodo (or similar) in your documentation can help. After all, it's people (including yourself, see below ) citing your software that makes it maintaining it worthwhile. Licencing \u00b6 Another aspect of acknowledgement your work is others using your source code, as opposed to your software. This is where licencing comes in. It is important you understand your rights as a a developer of open-source software, and also the rights of users of your software. By licencing your package you protect yourself from public liability and also (typically) legally require anyone to acknowledge you if they base their own software on yours. Most open source licences are quite permissive, even to the point where a third party is allowed to sell your software, as long as your were acknowledged. Github, for example, provides a number of licence templates you can select from, select the one which best suits your situation. Communication \u00b6 For promotion \u00b6 Users and practitioners need to be aware that your software exists. A simple and relatively easy way of promoting it is to publish an article in, for example, the Journal of Open Source Software or the Journal of Open Research Software , neither of which require a full paper, just simply a brief summary of what the package does. Their review process focussed on the software and if you've followed the recommendations here you should have little trouble being accepted. The impact, however, may not be great (who trawls there journals in the hope a package they could use suddenly appears?), so if you can also cite it yourself in an applied paper this will help greatly. Likewise, conferences are a good place to plug your development work, as well as simply encouraging collaborators and colleagues to use (and then cite) it. For development \u00b6 After configuring the tools in your ecosystem to listen for changes in repo, and respond automatically to them, they will typically email you if there's a problem. These days this isn't so reliable as most people are bombarded by emails and read few of them. Perhaps a better way is to use a messaging service (such as slack , as always others are available). Slack can be used as a central point for any issues coming from the services around your package, and of course can also be a place where developers communicate. Slack has many custom integrations which you can add to your channel, such as being able to create and amend github issues directly, and where custom integrations are not available you cant fairly easily create your own via webhooks. The slack channel(s) can also be a good place for communication with end users, which brings me on to... Support \u00b6 It's difficult to know how many people actively use your package (short of incorporating spyware into it). Whilst package repositories will produce a count (or rate) of downloads, I suspect at least some are bots, and of the others, downloading is one thing but actually using is another. But lets assume you have active users, and you have good documentation. You may find that a lot of the support questions could be answered by simply reading the documentation (who's not guilty of this?), or are not specific enough for you to replicate the problem they report. Some form of triage is useful if you get a lot of support questions. You can use an \"issue template\" where users must complete certain questions, describing in detail what they were trying to do, what went wrong, and on which version and platform. This deters people who realise they should look again at the documentation, and stops people raising vague issues along the lines of \"I tried using X but it didn't work\", that simply don't contain enough information to replicate. At the very least, you should make it clear that you cannot/will not help anyone unless they can come up with a reproducible example. After all, how do you fix a bug if you can't actually replicate the bug? Takeaways \u00b6 This may seem like an awful lot of effort to go to, but it does save potentially a lot of future headaches, and it doesn't need to all be done at once. Start with CI, and add other services/features as and when they become useful. Good practice in software- and data-driven research is becoming ever more important and is being taken more and more seriously by more traditional academics. The growth of Research Software Engineering as a discipline in it's own right speaks to this. If I had to summarise what I've written into some very brief key lessons, they would be: if you want other people to use and cite your software, you have do do a lot more than just writing the software, but there is payback all the tools you need are there, you just need to figure out how to use them (if you don't know already) automate the sh!t out of everything you can. It will be less error prone and save you (potentially lots of) time going forward. Scripts are your friend anticipate failure, test everything. The sooner you know something is broken, the better Room for improvement \u00b6 In what I've described, the developer has the ability to release directly from their own development environment to public package repositories. This is error-prone and open to abuse. I could accidentally (or deliberately) create a public release of my package that's not in source control and thus circumvent all the good work around testing and versioning. A better approach would be to release via continuous deployment (CD), a logical extension of CI. In the commercial world there's typically a clear segregation of roles (dev, infrastructure, production/users) but in academia you are often all 3 of them. Implementing CD would at least eliminate accidents, another approach would be for package repositories to only accept releases from verified sources, e.g. releases created on github. In practice \u00b6 To see (most of) this in action \"in the wild\", take a look around the neworder project.","title":"Anatomy of a successful research software project"},{"location":"anatomy/#anatomy-of-a-successful-research-software-project","text":"","title":"Anatomy of a successful research software project"},{"location":"anatomy/#motivations","text":"If you publish research that requires you to write a significant amount of code to produce results, then you probably (hopefully) put the code into source control in a public repository. Many journals now require, or at the very least, encourage this practice. But if you think this alone is enough to make your results reproducible by your peers, then think again. Why is this? Here's a few possible reasons another researcher might not be able to first replicate your results and then go on to use your software to extend the research: it isn't clear how you run/use the code the code isn't packaged and thus difficult to install correctly, if at all the code doesn't work on their machine the code runs, but the results are different the code works - the results are the same - but it's not at all clear how you would go about using the package to advance or modify the published work Even if a researcher successfully replicates your results, uses, and then and cites your software (somehow), subsequent readers of their research will likely confront the same problems. Often, documentation and testing are overlooked or given a low priority because they \"slow you down\". To this, I say more haste less speed . You'll save more time in future development than you'll lose by producing some documentation and a test framework. Whilst all developers test their code as they write it, not all developers keep those tests and run them regularly. How many times have you changed some code that's had unintended consequences in another part of your software, creating a bug that's gone unnoticed because you didn't keep the original tests, and now you have no idea exactly what is broken or quite why it no longer works? It is also important that your software is open to scrutiny by others. This essentially exposes your methodology at a level of detail generally impractical to describe fully in a paper. If there are any flaws in your methodology, or even bugs, then they are open for peer review. Conversely, aspects of your implementation may be of benefit to other researchers, even if the overall package isn't as relevant to them. All of the potential problems listed above can be solved to a lesser or greater extent using free and open tools that integrate into your project. Where I mention a specific product, it's not necessarily a recommendation, it just happens to be the tool I tend to use. And in most cases other (and perhaps better) products are available. Here's how it all fits together: One you integrate these tools into your ecosystem, and once set up require little or no maintenance. They'll save you enormous amounts of time and effort by providing forewarning of any issues, as well as providing a framework enabling developers to collaborate effectively, and users to feed back into the project by reporting bugs or suggesting improvements. Some, or perhaps all, of what follows may be obvious to some readers and it's largely common sense, but I'm describing the methodology from the ground up for completeness, and for those who are new to the concept of providing software research tools to a wide audience.","title":"Motivations"},{"location":"anatomy/#developers","text":"Even if you work on your own, entertain the possibility that someone, somewhere, may one day want to collaborate on the project. Consider also that there may be long periods where you're not actively developing the software, during which you'll likely forget some of the detail of how it works, what the dependencies are, or how you test it's working correctly. This means documentation, even if it's just some brief notes for yourself. But you may as well write it for a wider audience. I've seen plenty of projects on github that I would've like to contribute to, but didn't as there weren't clear instructions on basic usage or instructions on to how to set up a development environment, and a well-documented test framework. If you want collaborators, you're more likely to get them if they are comfortable they can add value without breaking anything. The obvious place to put this documentation is in the repo itself, starting with README.md . Github (for example) renders markdown into nicely (if somewhat spartanly) formatted web pages, is simple to master, and is well worth getting up to speed in if you haven't already: see e.g. markdown cheatsheet .","title":"Developer(s)"},{"location":"anatomy/#source-control","text":"In the corporate world, teams of developers work very closely together, and often exclusively on a single project, and in this context it often makes sense for them to all work directly in the same repo - when things go wrong they are quickly apparent and quickly resolved. In academia, however, the situation is very different: collaborations are much looser and people will be dipping in an out of various projects. Your collaborators may be people you barely know, geographically separated, and (most crucially) not reporting to the same boss as you! The best model for this situation is the \"fork and pull request\" practice. In other words, you control the \"root\" repo for the project, and only you (or a select few) have permission to commit to this repository. If somebody else wants to collaborate, they \"fork\" your repo, essentially taking a copy of it, do their development in their repo, and when ready they submit a \"pull request\" (PR) which invites you to review their changes and decide if you want to merge their contribution into your repo. Github also has tools in place to ensure their changes don't break anything (see Testing ). This practice can also be used within a single repo, with people developing on branches and submitting PRs merging to the master branch. A note here about good practice - it's generally not a good idea to commit directly to master , which should be the golden copy of your code. Development should be carried out on branches and merged to master when complete and fully tested. In practice, when I'm the only developer on a project I tend to commit small and uncontroversial \"atomic\" changes directly to master , but always use branches for larger and/or potentially breaking changes. Once a branch has been merged to master, it is no longer required and should be deleted.","title":"Source control"},{"location":"anatomy/#issues-and-project-management","text":"Github has the facility to create \"issues\" against a repo which are very useful for reporting bugs and tracking development. Issues can be categorised, assigned to people, assigned a status, assigned to a particular release, and discussed and commented on. Whilst the functionality is reasonably basic compared to other (often paid for) products, it has the advantage of being completely integrated into the project, and there are tools available to enhance the functionality, such as Zenhub , which adds full Agile project management via a browser plugin. For academic projects with a relatively small number of collaborators, this is probably more than sufficient. Commit messages can be automatically linked to specific issues simply by referencing the issue number prefixed with a '#', which is useful for tracking work done on a particular issue. Additionally, you can automatically close issues via a commit that addresses the issue, by including text like \"fixes #47\" or \"closes #47\" in the body. Looking at the issues boards of other repos is a good (rough) guide to see how robust the software is, how many issues are reported by users (a lot could indicate either wide usage or buggy software, or both!) and how responsive the developers are at addressing them.","title":"Issues and project management"},{"location":"anatomy/#testing","text":"It goes without saying that you should test your code, have a test harness of some form, and append the test code for any new features to the test suite. And obviously you always run the tests before pushing to the github repo, and when you do your CI (see next section ) runs more comprehensive tests. This section is more concerned more about how you go about automating this process, how to test in a wider range of environments, and how to determine if your tests are giving you a good indication of how robust your software is.","title":"Testing"},{"location":"anatomy/#continuous-integration","text":"Simply put, you register with a continuous integration (CI) provider (e.g. travis , appveyor , circleci ) and point them to your github project and specify some configuration for each provider so they know how to build and test your project. Then, every time you commit, the CI will build and test your code for you and tell you if there's a problem. The first thing this does is give you an assurance that your repo is complete and self-contained - if you've forgotten to commit a file, or if there's a dependency you haven't explicitly specified but happens to be installed on your machine, you'll know pretty soon. Ok, so you've been convinced that regression tests are a good thing (if you weren't already), and you're developing a python project on your linux machine, which has python 3.7 on it, and your tests are fairly comprehensive to you're confident it works... but only on linux, and only with python 3.7. This is where CI comes in - you can configure it to build on multiple platforms and multiple software versions. Different CI providers support different platforms so you may well need to register with multiple providers to cover the major platforms, e.g. Windows, OSX and linux. In terms of software versions aim for 3 - e.g. for R, they recommend testing against the previous release, the current release, and the release in development. Obviously if you do find a problem with a specific platform/version, you may then need to get hold of a machine with that configuration to track down the problem, but often a painstaking trawl through the CI log files is enough to pinpoint the problem. Writing configuration files for CI providers (typically in yaml) is something of a dark art. There are plenty of examples to be found on github and elsewhere though.","title":"Continuous integration"},{"location":"anatomy/#test-coverage","text":"As software evolves, the usage patterns change and the code gets changed or added to but often the tests don't. Thus its easy to stumble into a situation where the code that's being tested is not a good representation of the code that's executed by users. Again there are online tools that integrate with your repo and report area of your codebase that aren't being adequately tested.","title":"Test coverage"},{"location":"anatomy/#static-analysis","text":"No amount of runtime testing can guarantee to weed out all code errors, so it's important to analyse your code for dubious constructs that cause subtle bugs. This is commonly known as static analysis , since its done without actually running the code. For compiled code, this is doubly important, as there could be undefined behaviour lurking in the code which may (by pure luck) appear to work 99% of the time but cause the application to error or even crash seemingly randomly, for example by accidentally reading over the end of an array. As with CI, online tools are available that can be integrated with the github repository and will automatically analyse each commit and report any issues. They also typically grade your code for quality, which you can advertise on your repo: a high grade will again encourage people to trust (and therefore use) your package, and may also help attract contributors. Codacy is one such provider, and it combines static analysis tools for a variety of languages, sometimes with multiple tools for one language. Often static analysis tools will seem overly pedantic and seem to report a lot of false positives, but sometimes they are highlighting a subtle issue that requires some thought to understand. Otherwise, spurious and overly pedantic issues can be suppressed.","title":"Static analysis"},{"location":"anatomy/#versioning-and-releases","text":"So far all we've done is lay some foundations for good software development but havent really got to grips with the problems outlined at the start. Now we can start getting to the point of this methodology. Journal articles generally have a pretty slow turnaround. If you submit a manuscript containing a reference to some software in a github repository, then it will be at least 3 months and possibly a lot longer before that article is published. In the meantime, unless you've completely stopped development, the software in your repo could have changed substantially, and may not even be able to produce the results you described in the article, so it will be very difficult for any other researcher to replicate your results, short of bisecting the revision history, hoping for some informative commit comments, and guessing the approximate date you ran the code that generated the results you published. A time-consuming and potentially fruitless process for anyone. There is a simple solution, you simply version your software, and refer to a specific version in any published work. git has a concept called a tag, which just identifies a specific commit in a repo with a convenient name. In turn you can use this tag to create a release on github, which creates a zipped copy of the code at the tag, and allows you to create some release notes. Examining commit comments and closed issues since the previous release is a good starting point for making release notes. There are a number of different standards for versioning software, perhaps the most common is semantic versioning (aka semver) which is described here . So if you quoted your package and a version in your article, you've gone a long way to making your work reproducible by others. But there are easier and less error-prone ways of installing software than downloading a zip file, extracting it and manually copying it to a particular location. If you want to upgrade or remove it then will you remember exactly what you did to \"install\" it in the first place?","title":"Versioning and releases"},{"location":"anatomy/#documentation","text":"I've already mentioned the README, and this is a great place to introduce your package and put some brief notes on installation and basic usage. But its important also to provide more detailed examples and, ideally, API documentation. Depending on the size/complexity of your package you might be able to put this all in README.md but it probably makes sense to split it over multiple markdown files. Github markdown rendering is fairly spartan and has limited support for things like tables of contents and equations, if your documentation requires them. If you want very professional-looking documentation it is worth looking into rendering packages that convert markdown to styled html, such as Sphinx or MkDocs , which give you things like themes, a search tool, and tables of contents etc, straight out of the box. The generated documentation can be hosted (free) on github as project pages, or externally on sites such as readthedocs . I personally use mkdocs (in fact this site uses it), which is relatively easy to get up and running, supports configurable themes and supports thing like embedding (\\latex) (for equations) and (python) macros to programmatically generate content.","title":"Documentation"},{"location":"anatomy/#examples","text":"Providing worked examples is arguably the most important part of the documentation as its likely to be the first port of call for new users. The documentation macro feature (in e.g. mkdocs ) is particularly useful for examples, as you can pull in the actual code (rather than having to remember to manually copy/paste the example code in the documentation every time it changes). You can even go as far as actually running example code, inserting the output into the documentation as its generated. This has the added bonus of ensuring your example code works with the current version of the package. You are effectively testing your documentation, a process known as doctest . Some languages (e.g R, rust) support this natively.","title":"Examples"},{"location":"anatomy/#api-documentation","text":"Once users are actively integrating your package into their work, they will likely want more detail on how to use specific classes or functions. This is where API documentation comes in. You want to automate this as much as you can so it reflects the current API and manually generating it and keeping it up-to-date is a manual, laborious (and thus error-prone) process. Some documentation packages, or plugins for the packages, purport to be able to automatically generate documentation from your package, but you will probably find that your mileage may vary. In the worst case you can write a script to translate the raw package documentation ( docstrings in python) into markdown. Not ideal, but still better than a purely manual process.","title":"API documentation"},{"location":"anatomy/#package-repositories","text":"If you want users to be able to install your software easily, which translates pretty much to \"if you want users\", then you're going to need to package your software and upload it to a public repository. How straightforward this process is depends very much on the repository. In my experience, python package index (PyPI) is the easiest and CRAN (the R repository) the most onerous. PyPI seemingly accepts pretty much anything (including broken packages), whereas CRAN is extremely strict about the package contents and documentation. In between you have things like conda-forge, which will test the package installs on a number of different platforms before it accepts it. Obviously it's imperative that you ensure that you have fully tested the version you are publishing.","title":"Package repositories"},{"location":"anatomy/#container-repositories","text":"An alternative way of delivering software is to provide it in the form of a container, a.k.a. a docker image. This is in essence a lightweight but full (linux) operating system image pre-installed with your software and all its dependencies. This can be extremely useful if your package has a web API, i.e. an \"app service\" and allows it to be easily and quickly deployed almost anywhere. It is also useful for providing an environment containing ready-to-run examples that aren't part of the installed package. You provide a recipe in the form of a Dockerfile which is essentially a sequence of commands on how to install - from a base OS - all your package's dependencies, build it, set up an environment, and then run it. Another advantage is if you your software requires large datasets from various sources, you can provide a pre-packaged docker volume containing all the data (as long as its publicly available and you have got any necessary permissions to replicate it, of course). Docker hub is one such repository, which (as always) integrates with github and can be configured to automatically generate images for you, including tags (and thus releases).","title":"Container repositories"},{"location":"anatomy/#citations","text":"Citations nowadays include a unique Document Origin Identifier (DOI), and this by no means applies only to academic papers. It's also used for both software and data. Thus, assigning a DOI to a specific release of your software constitutes a reference which anyone should be able to trace. What happens if you publish a paper referencing your software, including a version, and then subsequently move the repo elsewhere or even delete it, or pull the release from public repositories? This will leave readers of your article unable to find your work. The beauty of a DOI (and the provider thereof) is that it doesn't matter (so much). Zenodo is a service that (you guessed it) integrates with github and takes a complete snapshot copy of your software, and thus persists even if the original repo or package no longer exists. I would suggest you make it as easy as possible for people to cite you, providing a bibtex-style reference and/or a link to Zenodo (or similar) in your documentation can help. After all, it's people (including yourself, see below ) citing your software that makes it maintaining it worthwhile.","title":"Citations"},{"location":"anatomy/#licencing","text":"Another aspect of acknowledgement your work is others using your source code, as opposed to your software. This is where licencing comes in. It is important you understand your rights as a a developer of open-source software, and also the rights of users of your software. By licencing your package you protect yourself from public liability and also (typically) legally require anyone to acknowledge you if they base their own software on yours. Most open source licences are quite permissive, even to the point where a third party is allowed to sell your software, as long as your were acknowledged. Github, for example, provides a number of licence templates you can select from, select the one which best suits your situation.","title":"Licencing"},{"location":"anatomy/#communication","text":"","title":"Communication"},{"location":"anatomy/#for-promotion","text":"Users and practitioners need to be aware that your software exists. A simple and relatively easy way of promoting it is to publish an article in, for example, the Journal of Open Source Software or the Journal of Open Research Software , neither of which require a full paper, just simply a brief summary of what the package does. Their review process focussed on the software and if you've followed the recommendations here you should have little trouble being accepted. The impact, however, may not be great (who trawls there journals in the hope a package they could use suddenly appears?), so if you can also cite it yourself in an applied paper this will help greatly. Likewise, conferences are a good place to plug your development work, as well as simply encouraging collaborators and colleagues to use (and then cite) it.","title":"For promotion"},{"location":"anatomy/#for-development","text":"After configuring the tools in your ecosystem to listen for changes in repo, and respond automatically to them, they will typically email you if there's a problem. These days this isn't so reliable as most people are bombarded by emails and read few of them. Perhaps a better way is to use a messaging service (such as slack , as always others are available). Slack can be used as a central point for any issues coming from the services around your package, and of course can also be a place where developers communicate. Slack has many custom integrations which you can add to your channel, such as being able to create and amend github issues directly, and where custom integrations are not available you cant fairly easily create your own via webhooks. The slack channel(s) can also be a good place for communication with end users, which brings me on to...","title":"For development"},{"location":"anatomy/#support","text":"It's difficult to know how many people actively use your package (short of incorporating spyware into it). Whilst package repositories will produce a count (or rate) of downloads, I suspect at least some are bots, and of the others, downloading is one thing but actually using is another. But lets assume you have active users, and you have good documentation. You may find that a lot of the support questions could be answered by simply reading the documentation (who's not guilty of this?), or are not specific enough for you to replicate the problem they report. Some form of triage is useful if you get a lot of support questions. You can use an \"issue template\" where users must complete certain questions, describing in detail what they were trying to do, what went wrong, and on which version and platform. This deters people who realise they should look again at the documentation, and stops people raising vague issues along the lines of \"I tried using X but it didn't work\", that simply don't contain enough information to replicate. At the very least, you should make it clear that you cannot/will not help anyone unless they can come up with a reproducible example. After all, how do you fix a bug if you can't actually replicate the bug?","title":"Support"},{"location":"anatomy/#takeaways","text":"This may seem like an awful lot of effort to go to, but it does save potentially a lot of future headaches, and it doesn't need to all be done at once. Start with CI, and add other services/features as and when they become useful. Good practice in software- and data-driven research is becoming ever more important and is being taken more and more seriously by more traditional academics. The growth of Research Software Engineering as a discipline in it's own right speaks to this. If I had to summarise what I've written into some very brief key lessons, they would be: if you want other people to use and cite your software, you have do do a lot more than just writing the software, but there is payback all the tools you need are there, you just need to figure out how to use them (if you don't know already) automate the sh!t out of everything you can. It will be less error prone and save you (potentially lots of) time going forward. Scripts are your friend anticipate failure, test everything. The sooner you know something is broken, the better","title":"Takeaways"},{"location":"anatomy/#room-for-improvement","text":"In what I've described, the developer has the ability to release directly from their own development environment to public package repositories. This is error-prone and open to abuse. I could accidentally (or deliberately) create a public release of my package that's not in source control and thus circumvent all the good work around testing and versioning. A better approach would be to release via continuous deployment (CD), a logical extension of CI. In the commercial world there's typically a clear segregation of roles (dev, infrastructure, production/users) but in academia you are often all 3 of them. Implementing CD would at least eliminate accidents, another approach would be for package repositories to only accept releases from verified sources, e.g. releases created on github.","title":"Room for improvement"},{"location":"anatomy/#in-practice","text":"To see (most of) this in action \"in the wild\", take a look around the neworder project.","title":"In practice"},{"location":"bad-algorithms/","text":"Bad algorithms \u00b6 I want to have some fun(?) implementing insanely slow algorithms for simple operations, but first here's a classic example of how not to implement an algorithm to set the scene. Exponential complexity \u00b6 The Fibonacci series is an integer sequence beginning \\(0, 1\\) where every successive digit is the sum of the previous two: \\[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55...\\] The definition \\[F\\big(n\\big) = \\left\\{\\begin{matrix} & n & n \\in {0,1} \\\\ & F\\big(n-1\\big) + F\\big(n-2\\big) & n > 1 \\end{matrix}\\right.\\] lends itself directly to a recursive implementation, e.g. in python: def fib_recursive ( n ): if n < 2 : return n return fib_recursive ( n - 1 ) + fib_recursive ( n - 2 ) For each call to fib_recursive up to two further calls to the function are made and thus the call stack will grow exponentially. For large \\(n\\) this will cause the program to run very slooooow if it doesn't run out of resources entirely and crash. This table illustrates the execution time for a small range of \\(n\\) : n F(n) Time(ms) 10 55 0.0851154 15 610 0.911951 20 6765 9.92441 25 75025 111.743 30 832040 1224.36 In graphical form it's clear that execution time in terms of \\(n\\) is linear on a log scale and thus growing exponentially : If you compute the slope, it's about ~1.618, a.k.a the Golden ratio \\(\\phi\\) which just happens to be the value the that the ratio of successive values in the sequence tends to. As you can see, for even fairly small \\(n\\) the function is taking prohibitively long to run. The number of times the function is actually called for a given value of \\(n\\) is a close relation of the sequence itself (unsurprisingly): \\[C\\big(n\\big) = \\left\\{\\begin{matrix} & 1 & n \\in {0,1} \\\\ & 1 + C\\big(n-1\\big) + C\\big(n-2\\big) & n > 1 \\end{matrix}\\right.\\] which is larger than \\(F\\) but grows at exactly the same rate, \\(\\phi\\) . In algorithmic complexity terms, this is known as exponential time complexity and should be avoided at all costs. The point I'm trying to make here is that the obvious (but perhaps naiive) implementation of a mathematical function is not always the best approach, and a good (research or otherwise) software engineer should know this. So let's try again. Python has a feature called generators which are ideal for computing sequences. The generator holds hold some state, and the next time you call it, it just picks up where it left off (the yield statement - this construct is known as a coroutine ). You could implement is like this: def fibgen (): a , b = 0 , 1 while True : yield a a , b = a + b , a def fib_generator ( n ): fg = fibgen () for _ in range ( n ): next ( fg ) return next ( fg ) And the execution time is vastly improved: n F(n) Time(ms) 10 55 0.00524521 20 6765 0.00333786 30 832040 0.00429153 40 102334155 0.00476837 50 12586269025 0.00572205 60 1548008755920 0.00691414 70 190392490709135 0.00762939 80 23416728348467685 0.00858307 90 2880067194370816120 0.00977516 The data is a little noisier this time because the execution time is so much faster, but there's a clear linear trend now: So, we've started with a worse-than-polynomial \\(O(\\phi^n)\\) algorithm and turned it into an \\(O(n)\\) one. Great! Worse-than-exponential algorithms \u00b6 You don't hear much about such things, I suppose largely because they're utterly impractical. Apart from travelling salesman-type problems which have a brute-force \\(O(n!)\\) complexity, I haven't really come across any problems that require such algorithms, but I'm sure plenty must exist. So, perversely I decided to explore how I could contrive to solve simple problems with hopelessly inefficient algorithms and try and work out how long operations would take using them. Obviously actually running, and even testing, them is largely impractical. Hyperoperations \u00b6 Starting two integers \\(a\\) and \\(b\\) , we formulate arithmetic operations on them as a chain of binary operations starting with the basic operation of incrementing (\"succession\"): Level Operation 0 Succession \\((a, b) \\rightarrow a + 1\\) 1 Addition \\((a, b) \\rightarrow a + b\\) 2 Multiplication \\((a, b) \\rightarrow ab\\) 3 Exponentiation \\((a, b) \\rightarrow a^b \\equiv a \\uparrow b\\) 4 Tetration \\((a, b) \\rightarrow \\underbrace{a^{a^{\\unicode{x22F0}^a}}}_b \\equiv a \\uparrow \\uparrow b\\) 5 Pentation \\((a, b) \\rightarrow a \\uparrow^3 b\\) 6 Hexation \\((a, b) \\rightarrow a \\uparrow^4 b\\) So each level is just doing \\(b\\) iterations of the previous level. And you can keep going forever but you need some unusual mathematical notation. The table above introduces Knuth's up-arrow notation. I can code up all of these operations in terms of iterated calls to the succession function: def suc ( n ): return n + 1 def add ( m , n ): for i in range ( n ): m = suc ( m ) return m def mul ( m , n ): m_ = m for i in range ( 1 , n ): m = add ( m , m_ ) return m def pow ( m , n ): m_ = m for i in range ( 1 , n ): m = mul ( m , m_ ) return m def tet ( m , n ): m_ = m for i in range ( 1 , n ): m = pow ( m_ , m ) # note args switched return m def pen ( m , n ): m_ = m for i in range ( 1 , n ): m = tet ( m_ , m ) return m def hex ( m , n ): m_ = m for i in range ( 1 , n ): m = pen ( m_ , m ) return m which gives addition linear complexity, multiplication quadratic, exponentiation exponential, and then we get into unknown territory with pentation and hexation. I'm not sure why I bothered, as I'm not going to be able to run them anyway. There's also the amusing side effect that adding 1 to 1000000 is much quicker than vice versa. And, since the only arithmetic operation in all of this is succession, the execution time will be proportional to the value of the result. This is good because in most cases it won't be possible to actually perform the computation, but I can still get a good idea of how long it would take. Addition \u00b6 The results for addition, unsurprisingly, show a clear linear trend, so it's \\(O(n)\\) : Multiplication \u00b6 This time with the \\(y\\) axis as the square root of the execution time, again we see the expected linear trend, so we've gone from the inbuilt \\(O(1)\\) to \\(O(n^2)\\) : Exponentiation \u00b6 And for exponentiation, you can see the runtime is increasing rapidly with \\(n\\) : n pow(n,n) Time(ms) 4 256 0.310421 5 3125 2.46119 6 46656 30.9212 7 823543 311.033 8 16777216 4973.39 9 387420489 112083 and since the number of operations grows with \\(n^n\\) , we plot the execution time against \\(\\ln(t)/\\ln(n)\\) : Already the performance is worse than the brute-force Travelling Salesman at \\(O(n^n) > O(n!)\\) and is trying my patience, but now I've worked out my algorithms give the right answer and perform as badly as expected, and also that this laptop can do about 3 million succession operations per second. So how does tetration perform? Hyperexponential complexity \u00b6 We can think of the previous example as \"hyperquadratic\" complexity \\(O(n\\uparrow\\uparrow2)\\) , analgous to quadratic complexity in the family of polynomial complexities. What about tetration? Here's some (extrapolated) execution times: \\(2^{2^{2^2}} = 2\\uparrow\\uparrow4 = 65\\,536\\) took about 20ms to calculate. \\(3^{3^3} = 3\\uparrow\\uparrow3 = 7\\,625\\,597\\,484\\,987\\) will take approximately 29 days to compute. \\(4^{4^4} = 4\\uparrow\\uparrow3 \\approx 1.34 \\times 10^{154} \\) will take around \\(\\mathbf{10^{130}}\\) times the age of the universe to compute. But if you're in a hurry, python -c \"print(4**4**4)\" will give you the answer instantaneously \ud83d\ude09. I'd better stop there. I've replaced perfectly good constant-time algorithms with pathologically inefficient linear, quadratic, exponential (and worse) ones. Utterly pointless, but sort of interesting all the same. I'll get my coat... If you want to see a mind-boggling example of a hyperoperation at a level that is itself an iterated hyperoperation, check out Graham's number .","title":"Bad algorithms"},{"location":"bad-algorithms/#bad-algorithms","text":"I want to have some fun(?) implementing insanely slow algorithms for simple operations, but first here's a classic example of how not to implement an algorithm to set the scene.","title":"Bad algorithms"},{"location":"bad-algorithms/#exponential-complexity","text":"The Fibonacci series is an integer sequence beginning \\(0, 1\\) where every successive digit is the sum of the previous two: \\[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55...\\] The definition \\[F\\big(n\\big) = \\left\\{\\begin{matrix} & n & n \\in {0,1} \\\\ & F\\big(n-1\\big) + F\\big(n-2\\big) & n > 1 \\end{matrix}\\right.\\] lends itself directly to a recursive implementation, e.g. in python: def fib_recursive ( n ): if n < 2 : return n return fib_recursive ( n - 1 ) + fib_recursive ( n - 2 ) For each call to fib_recursive up to two further calls to the function are made and thus the call stack will grow exponentially. For large \\(n\\) this will cause the program to run very slooooow if it doesn't run out of resources entirely and crash. This table illustrates the execution time for a small range of \\(n\\) : n F(n) Time(ms) 10 55 0.0851154 15 610 0.911951 20 6765 9.92441 25 75025 111.743 30 832040 1224.36 In graphical form it's clear that execution time in terms of \\(n\\) is linear on a log scale and thus growing exponentially : If you compute the slope, it's about ~1.618, a.k.a the Golden ratio \\(\\phi\\) which just happens to be the value the that the ratio of successive values in the sequence tends to. As you can see, for even fairly small \\(n\\) the function is taking prohibitively long to run. The number of times the function is actually called for a given value of \\(n\\) is a close relation of the sequence itself (unsurprisingly): \\[C\\big(n\\big) = \\left\\{\\begin{matrix} & 1 & n \\in {0,1} \\\\ & 1 + C\\big(n-1\\big) + C\\big(n-2\\big) & n > 1 \\end{matrix}\\right.\\] which is larger than \\(F\\) but grows at exactly the same rate, \\(\\phi\\) . In algorithmic complexity terms, this is known as exponential time complexity and should be avoided at all costs. The point I'm trying to make here is that the obvious (but perhaps naiive) implementation of a mathematical function is not always the best approach, and a good (research or otherwise) software engineer should know this. So let's try again. Python has a feature called generators which are ideal for computing sequences. The generator holds hold some state, and the next time you call it, it just picks up where it left off (the yield statement - this construct is known as a coroutine ). You could implement is like this: def fibgen (): a , b = 0 , 1 while True : yield a a , b = a + b , a def fib_generator ( n ): fg = fibgen () for _ in range ( n ): next ( fg ) return next ( fg ) And the execution time is vastly improved: n F(n) Time(ms) 10 55 0.00524521 20 6765 0.00333786 30 832040 0.00429153 40 102334155 0.00476837 50 12586269025 0.00572205 60 1548008755920 0.00691414 70 190392490709135 0.00762939 80 23416728348467685 0.00858307 90 2880067194370816120 0.00977516 The data is a little noisier this time because the execution time is so much faster, but there's a clear linear trend now: So, we've started with a worse-than-polynomial \\(O(\\phi^n)\\) algorithm and turned it into an \\(O(n)\\) one. Great!","title":"Exponential complexity"},{"location":"bad-algorithms/#worse-than-exponential-algorithms","text":"You don't hear much about such things, I suppose largely because they're utterly impractical. Apart from travelling salesman-type problems which have a brute-force \\(O(n!)\\) complexity, I haven't really come across any problems that require such algorithms, but I'm sure plenty must exist. So, perversely I decided to explore how I could contrive to solve simple problems with hopelessly inefficient algorithms and try and work out how long operations would take using them. Obviously actually running, and even testing, them is largely impractical.","title":"Worse-than-exponential algorithms"},{"location":"bad-algorithms/#hyperoperations","text":"Starting two integers \\(a\\) and \\(b\\) , we formulate arithmetic operations on them as a chain of binary operations starting with the basic operation of incrementing (\"succession\"): Level Operation 0 Succession \\((a, b) \\rightarrow a + 1\\) 1 Addition \\((a, b) \\rightarrow a + b\\) 2 Multiplication \\((a, b) \\rightarrow ab\\) 3 Exponentiation \\((a, b) \\rightarrow a^b \\equiv a \\uparrow b\\) 4 Tetration \\((a, b) \\rightarrow \\underbrace{a^{a^{\\unicode{x22F0}^a}}}_b \\equiv a \\uparrow \\uparrow b\\) 5 Pentation \\((a, b) \\rightarrow a \\uparrow^3 b\\) 6 Hexation \\((a, b) \\rightarrow a \\uparrow^4 b\\) So each level is just doing \\(b\\) iterations of the previous level. And you can keep going forever but you need some unusual mathematical notation. The table above introduces Knuth's up-arrow notation. I can code up all of these operations in terms of iterated calls to the succession function: def suc ( n ): return n + 1 def add ( m , n ): for i in range ( n ): m = suc ( m ) return m def mul ( m , n ): m_ = m for i in range ( 1 , n ): m = add ( m , m_ ) return m def pow ( m , n ): m_ = m for i in range ( 1 , n ): m = mul ( m , m_ ) return m def tet ( m , n ): m_ = m for i in range ( 1 , n ): m = pow ( m_ , m ) # note args switched return m def pen ( m , n ): m_ = m for i in range ( 1 , n ): m = tet ( m_ , m ) return m def hex ( m , n ): m_ = m for i in range ( 1 , n ): m = pen ( m_ , m ) return m which gives addition linear complexity, multiplication quadratic, exponentiation exponential, and then we get into unknown territory with pentation and hexation. I'm not sure why I bothered, as I'm not going to be able to run them anyway. There's also the amusing side effect that adding 1 to 1000000 is much quicker than vice versa. And, since the only arithmetic operation in all of this is succession, the execution time will be proportional to the value of the result. This is good because in most cases it won't be possible to actually perform the computation, but I can still get a good idea of how long it would take.","title":"Hyperoperations"},{"location":"bad-algorithms/#addition","text":"The results for addition, unsurprisingly, show a clear linear trend, so it's \\(O(n)\\) :","title":"Addition"},{"location":"bad-algorithms/#multiplication","text":"This time with the \\(y\\) axis as the square root of the execution time, again we see the expected linear trend, so we've gone from the inbuilt \\(O(1)\\) to \\(O(n^2)\\) :","title":"Multiplication"},{"location":"bad-algorithms/#exponentiation","text":"And for exponentiation, you can see the runtime is increasing rapidly with \\(n\\) : n pow(n,n) Time(ms) 4 256 0.310421 5 3125 2.46119 6 46656 30.9212 7 823543 311.033 8 16777216 4973.39 9 387420489 112083 and since the number of operations grows with \\(n^n\\) , we plot the execution time against \\(\\ln(t)/\\ln(n)\\) : Already the performance is worse than the brute-force Travelling Salesman at \\(O(n^n) > O(n!)\\) and is trying my patience, but now I've worked out my algorithms give the right answer and perform as badly as expected, and also that this laptop can do about 3 million succession operations per second. So how does tetration perform?","title":"Exponentiation"},{"location":"bad-algorithms/#hyperexponential-complexity","text":"We can think of the previous example as \"hyperquadratic\" complexity \\(O(n\\uparrow\\uparrow2)\\) , analgous to quadratic complexity in the family of polynomial complexities. What about tetration? Here's some (extrapolated) execution times: \\(2^{2^{2^2}} = 2\\uparrow\\uparrow4 = 65\\,536\\) took about 20ms to calculate. \\(3^{3^3} = 3\\uparrow\\uparrow3 = 7\\,625\\,597\\,484\\,987\\) will take approximately 29 days to compute. \\(4^{4^4} = 4\\uparrow\\uparrow3 \\approx 1.34 \\times 10^{154} \\) will take around \\(\\mathbf{10^{130}}\\) times the age of the universe to compute. But if you're in a hurry, python -c \"print(4**4**4)\" will give you the answer instantaneously \ud83d\ude09. I'd better stop there. I've replaced perfectly good constant-time algorithms with pathologically inefficient linear, quadratic, exponential (and worse) ones. Utterly pointless, but sort of interesting all the same. I'll get my coat... If you want to see a mind-boggling example of a hyperoperation at a level that is itself an iterated hyperoperation, check out Graham's number .","title":"Hyperexponential complexity"},{"location":"blockchain-auth/","text":"Blockchain authentication \u00b6 Cryptography, of which authentication is a part, is a thing where you don't ever roll your own. If you do, people will just laugh at you. (After they've hacked you, probably). At the risk of people pointing at me and laughing, here goes... Crypto background \u00b6 Cryptographic hashes \u00b6 Blockchains commonly use the SHA256 hashing algorithm, which converts input of any length into a 32-byte hash. To put it into context, that means there are \\( 2^{256} \\approx 10^{77} \\) possible hashes, which is on a par with the number of atoms in the visible universe . Now while collisions are possible , since there are an infinite number of inputs and only a finite (but large) number of outputs, they are to all intents and purposes so improbable as to be negligible. The other important point is that a cryptographic hash is a one-way function: there is no way to recover the input from the output, short of guessing until you get it right, which is utterly impractical given the numbers above. Public-key cryptography \u00b6 ECDSA is a method of authenticating an entity. It works like this: you generate a keypair consisting of a public and a private key. You give the former to your peers, and you keep the private one secret. When you send a message to someone, you digitally sign it using your private key, and when the recipient receives it they checks the signature against the public key you supplied earlier. If it verifies, they can be certain the message came from you. If you lose your private key, you lose your identity, and if someone steals it, they've stolen your identity, so it's important to keep it safe. Like SHA-256, private keys are 32 bytes long so guessing them is not an option. Blockchain background \u00b6 Here's a very brief summary of how the blockchain does it's thing: it's a distributed database of transactions served on a peer-to-peer network, maintained by miners who are rewarded for preserving its integrity it's immutable, transactions can't be changed once they've been confirmed it's cryptographically secure, the proceeds of transactions can only be \"spent\" by the those who have the relevant private key each transaction is in itself anonymous * as its identified as an address which is an encoded hash of the owner's public key. * whilst it's not possible to recover a public key from an address, if you have a chain of transactions (as you typically will), the owner's identity can quite easily be revealed by times and locations and recipients of their transactions. Think of it like this. You shred your bank statements, but one of the shreds blows away in the wind and contains all the information relating to a single transaction. If somebody finds it and can associate this shred with you, then they effectively know your entire banking history. \"Smart\" contracts \u00b6 Motivations \u00b6 Example Application \u00b6 I have a Raspberry Pi with a \"Sense HAT\" which contains sensors for temperature, pressure and humidity. I've written a little API so that clients can request this information over http, but the ambient conditions in my office are top secret information so I don't want everyone knowing them. I've got another Raspberry Pi that wants to know what the ambient conditions are (2 metres away), but since this is highly sensitive information it wants to ensure that the response it gets is genuine. My Raspberry Pis have agreed (bilaterally) to share this data between them, but don't fully trust each other (to not be hacked) so each want to be able to unilaterally (and permanently) stop any communications if they suspect foul play. I'm also concerned that they could be hacked, so I'd I could probab","title":"Blockchain authentication"},{"location":"blockchain-auth/#blockchain-authentication","text":"Cryptography, of which authentication is a part, is a thing where you don't ever roll your own. If you do, people will just laugh at you. (After they've hacked you, probably). At the risk of people pointing at me and laughing, here goes...","title":"Blockchain authentication"},{"location":"blockchain-auth/#crypto-background","text":"","title":"Crypto background"},{"location":"blockchain-auth/#cryptographic-hashes","text":"Blockchains commonly use the SHA256 hashing algorithm, which converts input of any length into a 32-byte hash. To put it into context, that means there are \\( 2^{256} \\approx 10^{77} \\) possible hashes, which is on a par with the number of atoms in the visible universe . Now while collisions are possible , since there are an infinite number of inputs and only a finite (but large) number of outputs, they are to all intents and purposes so improbable as to be negligible. The other important point is that a cryptographic hash is a one-way function: there is no way to recover the input from the output, short of guessing until you get it right, which is utterly impractical given the numbers above.","title":"Cryptographic hashes"},{"location":"blockchain-auth/#public-key-cryptography","text":"ECDSA is a method of authenticating an entity. It works like this: you generate a keypair consisting of a public and a private key. You give the former to your peers, and you keep the private one secret. When you send a message to someone, you digitally sign it using your private key, and when the recipient receives it they checks the signature against the public key you supplied earlier. If it verifies, they can be certain the message came from you. If you lose your private key, you lose your identity, and if someone steals it, they've stolen your identity, so it's important to keep it safe. Like SHA-256, private keys are 32 bytes long so guessing them is not an option.","title":"Public-key cryptography"},{"location":"blockchain-auth/#blockchain-background","text":"Here's a very brief summary of how the blockchain does it's thing: it's a distributed database of transactions served on a peer-to-peer network, maintained by miners who are rewarded for preserving its integrity it's immutable, transactions can't be changed once they've been confirmed it's cryptographically secure, the proceeds of transactions can only be \"spent\" by the those who have the relevant private key each transaction is in itself anonymous * as its identified as an address which is an encoded hash of the owner's public key. * whilst it's not possible to recover a public key from an address, if you have a chain of transactions (as you typically will), the owner's identity can quite easily be revealed by times and locations and recipients of their transactions. Think of it like this. You shred your bank statements, but one of the shreds blows away in the wind and contains all the information relating to a single transaction. If somebody finds it and can associate this shred with you, then they effectively know your entire banking history.","title":"Blockchain background"},{"location":"blockchain-auth/#smart-contracts","text":"","title":"\"Smart\" contracts"},{"location":"blockchain-auth/#motivations","text":"","title":"Motivations"},{"location":"blockchain-auth/#example-application","text":"I have a Raspberry Pi with a \"Sense HAT\" which contains sensors for temperature, pressure and humidity. I've written a little API so that clients can request this information over http, but the ambient conditions in my office are top secret information so I don't want everyone knowing them. I've got another Raspberry Pi that wants to know what the ambient conditions are (2 metres away), but since this is highly sensitive information it wants to ensure that the response it gets is genuine. My Raspberry Pis have agreed (bilaterally) to share this data between them, but don't fully trust each other (to not be hacked) so each want to be able to unilaterally (and permanently) stop any communications if they suspect foul play. I'm also concerned that they could be hacked, so I'd I could probab","title":"Example Application"},{"location":"difficult-data/","text":"How (not) to supply data \u00b6 If you provide packaged, versioned, well-documented software as part of your academic output, then you make your work much more easily reproducible (see my previous post ). However, if that software needs some input data to do its thing, then you also need to apply the same principles to the data : somebody with a different dataset wont be able to produce your results, despite having identical software. This applies whether you are providing the data yourself, or if you are using data from a third-party source. If the latter, you of course cite all your data sources. Data is some way behind the curve compared to software: alarm bells would immediately ring if one of your software dependencies was completely unversioned, perhaps not even under source control, and yet data is all-too-often like this. The FAIR principles were established/published in 2016, and in summary state that data should be Findable, Accessible, Interoperable, and Reusable . Data providers can only anticipate how consumers use their data, and can be slow to change from older/traditional formats which may be less suitable for modern use-cases, be they software-led (raw data) or human-led (visualisation). This inevitably leads to some preprocessing being required, which itself needs to be reproducible. It's common to download a spreadsheet, copy and paste some data into a new spreadsheet, apply some formulas, change some names, then save it as a csv file. This is completely opaque and if any misinterpretations or errors creep in then they are hidden: it needs to be transparent. Here's a fairly simple example... Population estimates \u00b6 Let's say we need population projections by for England by local authority, sex and (5-year) age group. We'd like it in some non-proprietary format that we could load straight into (e.g.) python or R and do something with it. A csv file will do just fine for this. ONS produce this data, and here's the 2018 numbers from their website . It's an Excel spreadsheet (despite some very high-profile Excel-related data blunders ): Firstly, as data goes this isn't particularly big, unstructured, or dirty data. It's pretty straightforward, but the question that springs to my mind is: what audience is this format aimed at? It's got all sorts of features like multiple tabs, frozen rows, formatted text, comma-number separators than suggest it's been designed for humans to pore over. Really? But who gets insight directly from huge tables of numbers, surely they'd visualise it more graphically in some way? It seems to be in some no-man's land between user-friendliness and process-friendliness. I wouldn't give it a very high rating against the FAIR principles. To get the exact dataset I want I could do the following: delete the first six rows of the Males worksheet delete all the rows where the AGE GROUP column value is \"All ages\", otherwise I'll double-count the numbers delete all the rows where the AREA column value is a country, a region or a county, to avoid more double-counting create a new column called SEX and fill it with (e.g.) \"M\" remove the comma-separator formatting from the numbers and the \"freeze columns\" setting copy the remaining contents into a new worksheet repeat all the steps above in the Females worksheet, but adding (e.g.) \"F\" in the new SEX column copy this to the new sheet, appending the male data save the new worksheet as a csv file Despite this data not being particularly messy or unstructured, I have to go though some fairly tedious, manual and error-prone steps to get the data exactly as I need it. And if I'm using this data to generate some published results, do I really want people to have to repeat these steps to replicate my work? I could detail the manual preprocessing steps in the text, but it detracts somewhat from the flow of the paper. Another possibility is I could publish the preprocessed data and reference it in my paper. But if it transpires there is an error in the original data, which gets corrected, or updated, then neither I nor my readers get the correction. Perhaps a better solution would be to automate the steps I took, including downloading the data, and packages like pandas (for python) are perfect for this, although there's a still a few hoops to jump though: import pandas as pd import requests from pathlib import Path def download ( url ): # reading url directly with pd.read_excel(url) gives 403 # but using requests works fine (must be user-agent string) response = requests . get ( url ) if response . status_code != 200 : print ( \" %d error\" ) return None return response . content which is a generic function I could use purely for fetching just about any raw dataset from a URL. Then I need another function, specific to this dataset, to actually process the data: def preprocess ( raw_data , output_file ): dataset = pd . DataFrame () for tab in [ \"Males\" , \"Females\" ]: data = pd . read_excel ( raw_data , sheet_name = tab , header = 6 ) # remove duplicate age counts data = data [ data [ \"AGE GROUP\" ] != \"All ages\" ] # remove non-LAD counts data = data [ data [ \"CODE\" ] . str . match ( r '^E0[6789]*' )] # add sex column data [ \"SEX\" ] = tab [ 0 ] # append data dataset = dataset . append ( data ) # cache data dataset . to_csv ( output_file , index = False ) and putting this together, we code some logic that efficiently caches the dataset after initially downloading and preprocessing, then returns a DataFrame : # 2018 SNPP 5y url = \"https://www.ons.gov.uk/file?uri= %2f peoplepopulationandcommunity %2f populationandmigration %2f populationprojections %2f datasets %2f localauthoritiesinenglandtable2 %2f 2018based/table2.xls\" cached_data = Path ( \"./england_snpp_2018.csv\" ) if not cached_data . is_file (): preprocess ( download ( url ), cached_data ) dataset = pd . read_csv ( cached_data ) print ( dataset . head ()) which produces CODE AREA AGE GROUP 2018 2019 2020 2021 2022 2023 2024 ... 2035 2036 2037 2038 2039 2040 2041 2042 2043 SEX 0 E06000047 County Durham 0-4 13849 13526.5 13343.5 13121.8 12976.9 12939.8 12930.1 ... 13351.0 13453.3 13561.5 13672.8 13785.2 13894.1 13992.6 14075.1 14137.3 M 1 E06000047 County Durham 5-9 15443 15626.6 15524.8 15315.7 15085.3 14677.6 14335.6 ... 13665.4 13707.3 13764.3 13837.1 13921.7 14015.8 14119.1 14229.4 14344.3 M 2 E06000047 County Durham 10-14 14610 14997.3 15386.8 15795.2 15967.3 16157.0 16320.8 ... 14227.7 14206.5 14206.9 14219.8 14234.1 14259.6 14298.2 14352.6 14423.7 M 3 E06000047 County Durham 15-19 15022 14936.7 15001.9 15290.5 15812.4 16310.0 16692.7 ... 16627.9 16339.9 16130.7 15999.2 15975.2 15930.3 15898.4 15891.0 15898.2 M 4 E06000047 County Durham 20-24 18599 18613.4 18657.8 18538.9 18394.5 18291.3 18241.3 ... 21551.0 21395.9 21109.8 20755.1 20381.3 20095.5 19787.6 19558.4 19415.6 M [5 rows x 30 columns] Which is great, I can just ensure that this code is in the repo that I cite in my work. APIs \u00b6 Many data providers provide an API , which allows users to directly query a data provider to get the specific subset they require from a large data, and often allows for customisable output formats. A good example of this is NomisWeb , which provide UK population (particularly census) data from various sources, including ONS data. Their website provides a \"wizard\" allowing users to construct a query to get a specific dataset. For large datasets (e.g. census data), this is far more efficient than downloading a bulk dataset then filtering for the (small) specific subset you want. Using the Nomisweb query builder I can construct this url: https://www.nomisweb.co.uk/api/v01/dataset/NM_31_1.data.tsv?geography=1811939329...1811939332,1811939334...1811939336,1811939338...1811939497,1811939499...1811939501,1811939503,1811939505...1811939507,1811939509...1811939517,1811939519,1811939520,1811939524...1811939570,1811939575...1811939599,1811939601...1811939628,1811939630...1811939634,1811939636...1811939647,1811939649,1811939655...1811939664,1811939667...1811939680,1811939682,1811939683,1811939685,1811939687...1811939704,1811939707,1811939708,1811939710,1811939712...1811939717,1811939719,1811939720,1811939722...1811939730,1811939757...1811939767&date=latestMINUS1&sex=5,6&age=1...19&measures=20100 which gives me a very similar dataset to the one in the previous section, and I can then amend my code to download (and potentially cache) the data as and when I need it. I can also get rid of the preprocessing code, which simplifies things. Data persistence \u00b6 The trouble is that neither of the two approaches address a fundamental problem that if you are not the source of the data, you are not in control of it. Your reproducibility will be compromised if the provider moves the data (e.g. to a different URL) the provider changes the format of the data the provider amends the values in data, for whatever reason Thankfully there is a solution, but it's one that only the data providers themselves can implement: the Digital Object Identifier ( DOI ) system. It's already pretty standard in academic circles, and uses a persistent identifier to provide a citable, recoverable reference to an (immutable?) digital asset that is commonly a paper, software, or a dataset. And by providing a DOI for your data, you are making it easier for people to cite you. If your data providers don't already use the DOI system, please encourage them to do so. Finally, the UK government are looking into it so hopefully this should filter through to the ONS at some point. Third-party organisations such as zenodo can be used to store the content referenced by the DOI. The future? \u00b6 There's still a problem: you may have a DOI, but what if the data itself gets deleted, lost or hacked (i.e. modified maliciously)? To me, Blockchain technologies seem to be a perfect solution for reproducible data: a blockchain is an immutable, persistent, distributed database which would be ideal for storing DOIs, and either the data itself (or a cryptographic hash of it, and a URL). Smart contracts could be used to encode any preprocessing steps you need to do to the raw data, or as an alternative to an API, and your software would need only to execute the contract to get the exact dataset it requires. Immutability is guaranteed, so local cacheing isn't liable to get out of step with the source data. A technology like interplanetary file system IPFS could provide the scalability to store the enormous amount of data, potentially making your locally-cached copy of the data part of the database itself. Finally, to make this work smoothly, some form of incentivisation for participants may be required, perhaps something along the lines of H-index? Manifesto for data providers \u00b6 All the effort you have expended to make your software reproducible is wasted if you don't apply the same principles to the data it consumes. This can be tricky when the data is from a third party. You could (should?) name-and-shame them in your work, but perhaps more productive would be to encourage them to adopt some or all of these guidelines: version your datasets with a DOI try to anticipate the (changing) needs of your consumers (ask them!) don't use a proprietary data format. Excel should not be the default tool for data science! make the data easily consumable by code without undue preprocessing (you can always provide a human-friendly visualisation tool as well) consider any dataset you publish to be immutable: if it needs corrections or amendments, treat this as a brand new dataset don't change the location (i.e. url) of the data without some form of notification APIs are better than static links if you have the data to justify it and the resources to implement one consult the GO-FAIR community's guidance on data stewardship .","title":"How (not) to supply data"},{"location":"difficult-data/#how-not-to-supply-data","text":"If you provide packaged, versioned, well-documented software as part of your academic output, then you make your work much more easily reproducible (see my previous post ). However, if that software needs some input data to do its thing, then you also need to apply the same principles to the data : somebody with a different dataset wont be able to produce your results, despite having identical software. This applies whether you are providing the data yourself, or if you are using data from a third-party source. If the latter, you of course cite all your data sources. Data is some way behind the curve compared to software: alarm bells would immediately ring if one of your software dependencies was completely unversioned, perhaps not even under source control, and yet data is all-too-often like this. The FAIR principles were established/published in 2016, and in summary state that data should be Findable, Accessible, Interoperable, and Reusable . Data providers can only anticipate how consumers use their data, and can be slow to change from older/traditional formats which may be less suitable for modern use-cases, be they software-led (raw data) or human-led (visualisation). This inevitably leads to some preprocessing being required, which itself needs to be reproducible. It's common to download a spreadsheet, copy and paste some data into a new spreadsheet, apply some formulas, change some names, then save it as a csv file. This is completely opaque and if any misinterpretations or errors creep in then they are hidden: it needs to be transparent. Here's a fairly simple example...","title":"How (not) to supply data"},{"location":"difficult-data/#population-estimates","text":"Let's say we need population projections by for England by local authority, sex and (5-year) age group. We'd like it in some non-proprietary format that we could load straight into (e.g.) python or R and do something with it. A csv file will do just fine for this. ONS produce this data, and here's the 2018 numbers from their website . It's an Excel spreadsheet (despite some very high-profile Excel-related data blunders ): Firstly, as data goes this isn't particularly big, unstructured, or dirty data. It's pretty straightforward, but the question that springs to my mind is: what audience is this format aimed at? It's got all sorts of features like multiple tabs, frozen rows, formatted text, comma-number separators than suggest it's been designed for humans to pore over. Really? But who gets insight directly from huge tables of numbers, surely they'd visualise it more graphically in some way? It seems to be in some no-man's land between user-friendliness and process-friendliness. I wouldn't give it a very high rating against the FAIR principles. To get the exact dataset I want I could do the following: delete the first six rows of the Males worksheet delete all the rows where the AGE GROUP column value is \"All ages\", otherwise I'll double-count the numbers delete all the rows where the AREA column value is a country, a region or a county, to avoid more double-counting create a new column called SEX and fill it with (e.g.) \"M\" remove the comma-separator formatting from the numbers and the \"freeze columns\" setting copy the remaining contents into a new worksheet repeat all the steps above in the Females worksheet, but adding (e.g.) \"F\" in the new SEX column copy this to the new sheet, appending the male data save the new worksheet as a csv file Despite this data not being particularly messy or unstructured, I have to go though some fairly tedious, manual and error-prone steps to get the data exactly as I need it. And if I'm using this data to generate some published results, do I really want people to have to repeat these steps to replicate my work? I could detail the manual preprocessing steps in the text, but it detracts somewhat from the flow of the paper. Another possibility is I could publish the preprocessed data and reference it in my paper. But if it transpires there is an error in the original data, which gets corrected, or updated, then neither I nor my readers get the correction. Perhaps a better solution would be to automate the steps I took, including downloading the data, and packages like pandas (for python) are perfect for this, although there's a still a few hoops to jump though: import pandas as pd import requests from pathlib import Path def download ( url ): # reading url directly with pd.read_excel(url) gives 403 # but using requests works fine (must be user-agent string) response = requests . get ( url ) if response . status_code != 200 : print ( \" %d error\" ) return None return response . content which is a generic function I could use purely for fetching just about any raw dataset from a URL. Then I need another function, specific to this dataset, to actually process the data: def preprocess ( raw_data , output_file ): dataset = pd . DataFrame () for tab in [ \"Males\" , \"Females\" ]: data = pd . read_excel ( raw_data , sheet_name = tab , header = 6 ) # remove duplicate age counts data = data [ data [ \"AGE GROUP\" ] != \"All ages\" ] # remove non-LAD counts data = data [ data [ \"CODE\" ] . str . match ( r '^E0[6789]*' )] # add sex column data [ \"SEX\" ] = tab [ 0 ] # append data dataset = dataset . append ( data ) # cache data dataset . to_csv ( output_file , index = False ) and putting this together, we code some logic that efficiently caches the dataset after initially downloading and preprocessing, then returns a DataFrame : # 2018 SNPP 5y url = \"https://www.ons.gov.uk/file?uri= %2f peoplepopulationandcommunity %2f populationandmigration %2f populationprojections %2f datasets %2f localauthoritiesinenglandtable2 %2f 2018based/table2.xls\" cached_data = Path ( \"./england_snpp_2018.csv\" ) if not cached_data . is_file (): preprocess ( download ( url ), cached_data ) dataset = pd . read_csv ( cached_data ) print ( dataset . head ()) which produces CODE AREA AGE GROUP 2018 2019 2020 2021 2022 2023 2024 ... 2035 2036 2037 2038 2039 2040 2041 2042 2043 SEX 0 E06000047 County Durham 0-4 13849 13526.5 13343.5 13121.8 12976.9 12939.8 12930.1 ... 13351.0 13453.3 13561.5 13672.8 13785.2 13894.1 13992.6 14075.1 14137.3 M 1 E06000047 County Durham 5-9 15443 15626.6 15524.8 15315.7 15085.3 14677.6 14335.6 ... 13665.4 13707.3 13764.3 13837.1 13921.7 14015.8 14119.1 14229.4 14344.3 M 2 E06000047 County Durham 10-14 14610 14997.3 15386.8 15795.2 15967.3 16157.0 16320.8 ... 14227.7 14206.5 14206.9 14219.8 14234.1 14259.6 14298.2 14352.6 14423.7 M 3 E06000047 County Durham 15-19 15022 14936.7 15001.9 15290.5 15812.4 16310.0 16692.7 ... 16627.9 16339.9 16130.7 15999.2 15975.2 15930.3 15898.4 15891.0 15898.2 M 4 E06000047 County Durham 20-24 18599 18613.4 18657.8 18538.9 18394.5 18291.3 18241.3 ... 21551.0 21395.9 21109.8 20755.1 20381.3 20095.5 19787.6 19558.4 19415.6 M [5 rows x 30 columns] Which is great, I can just ensure that this code is in the repo that I cite in my work.","title":"Population estimates"},{"location":"difficult-data/#apis","text":"Many data providers provide an API , which allows users to directly query a data provider to get the specific subset they require from a large data, and often allows for customisable output formats. A good example of this is NomisWeb , which provide UK population (particularly census) data from various sources, including ONS data. Their website provides a \"wizard\" allowing users to construct a query to get a specific dataset. For large datasets (e.g. census data), this is far more efficient than downloading a bulk dataset then filtering for the (small) specific subset you want. Using the Nomisweb query builder I can construct this url: https://www.nomisweb.co.uk/api/v01/dataset/NM_31_1.data.tsv?geography=1811939329...1811939332,1811939334...1811939336,1811939338...1811939497,1811939499...1811939501,1811939503,1811939505...1811939507,1811939509...1811939517,1811939519,1811939520,1811939524...1811939570,1811939575...1811939599,1811939601...1811939628,1811939630...1811939634,1811939636...1811939647,1811939649,1811939655...1811939664,1811939667...1811939680,1811939682,1811939683,1811939685,1811939687...1811939704,1811939707,1811939708,1811939710,1811939712...1811939717,1811939719,1811939720,1811939722...1811939730,1811939757...1811939767&date=latestMINUS1&sex=5,6&age=1...19&measures=20100 which gives me a very similar dataset to the one in the previous section, and I can then amend my code to download (and potentially cache) the data as and when I need it. I can also get rid of the preprocessing code, which simplifies things.","title":"APIs"},{"location":"difficult-data/#data-persistence","text":"The trouble is that neither of the two approaches address a fundamental problem that if you are not the source of the data, you are not in control of it. Your reproducibility will be compromised if the provider moves the data (e.g. to a different URL) the provider changes the format of the data the provider amends the values in data, for whatever reason Thankfully there is a solution, but it's one that only the data providers themselves can implement: the Digital Object Identifier ( DOI ) system. It's already pretty standard in academic circles, and uses a persistent identifier to provide a citable, recoverable reference to an (immutable?) digital asset that is commonly a paper, software, or a dataset. And by providing a DOI for your data, you are making it easier for people to cite you. If your data providers don't already use the DOI system, please encourage them to do so. Finally, the UK government are looking into it so hopefully this should filter through to the ONS at some point. Third-party organisations such as zenodo can be used to store the content referenced by the DOI.","title":"Data persistence"},{"location":"difficult-data/#the-future","text":"There's still a problem: you may have a DOI, but what if the data itself gets deleted, lost or hacked (i.e. modified maliciously)? To me, Blockchain technologies seem to be a perfect solution for reproducible data: a blockchain is an immutable, persistent, distributed database which would be ideal for storing DOIs, and either the data itself (or a cryptographic hash of it, and a URL). Smart contracts could be used to encode any preprocessing steps you need to do to the raw data, or as an alternative to an API, and your software would need only to execute the contract to get the exact dataset it requires. Immutability is guaranteed, so local cacheing isn't liable to get out of step with the source data. A technology like interplanetary file system IPFS could provide the scalability to store the enormous amount of data, potentially making your locally-cached copy of the data part of the database itself. Finally, to make this work smoothly, some form of incentivisation for participants may be required, perhaps something along the lines of H-index?","title":"The future?"},{"location":"difficult-data/#manifesto-for-data-providers","text":"All the effort you have expended to make your software reproducible is wasted if you don't apply the same principles to the data it consumes. This can be tricky when the data is from a third party. You could (should?) name-and-shame them in your work, but perhaps more productive would be to encourage them to adopt some or all of these guidelines: version your datasets with a DOI try to anticipate the (changing) needs of your consumers (ask them!) don't use a proprietary data format. Excel should not be the default tool for data science! make the data easily consumable by code without undue preprocessing (you can always provide a human-friendly visualisation tool as well) consider any dataset you publish to be immutable: if it needs corrections or amendments, treat this as a brand new dataset don't change the location (i.e. url) of the data without some form of notification APIs are better than static links if you have the data to justify it and the resources to implement one consult the GO-FAIR community's guidance on data stewardship .","title":"Manifesto for data providers"},{"location":"float/","text":"Floating-point (ir)reproducibility \u00b6 I'm going to show that even if you have identical data, software and OS, you still can't guarantee reproducibility across instances, at least when it comes to floating-point calculations. XKCD sums it up quite nicely: Any software developer who writes numerical code needs to have some understanding of how floating-point numbers are represented, because they are a major source of software bugs and issues. Some people have a tendency to either assume the computer can't possibly get simple maths (slightly) wrong, or dismiss floating-point issues as rounding errors. Firstly, consider this motivating example of a very simple floating-point expression: Python 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> 0.5-0.4-0.1 -2.7755575615628914e-17 Feel free to try this in your language/platform of choice, chances are you will get the same wrong answer. So why can't a computer get such a simple sum correct? So you can assume it's a small and systematic (i.e easily reproducible) error and live with it, but if ( for example ) you have derivatives traders screaming at you because their risk numbers are all over the place and they can't hedge their positions, you can't just accept it. The problem is, floating-point errors don't always stay small, and they aren't always easily reproducible. I strongly recommend reading What Every Computer Scientist Should Know About Floating-Point Arithmetic . Now I'll try to explain the non-zero answer above. The IEEE754 Standard \u00b6 Here we will only consider the double precision format, as it's by far the most widely used. Single, half and quad precision all suffer from the same issues. Double-precision floating-point numbers are represented in 64 bits as follows: a sign bit s , with zero representing positive and 1 negative. an 11-bit exponent e a 52-bit mantissa m The value 1 looks like this in binary: \\[\\underbrace{\\boxed{0}}_s\\underbrace{\\boxed{01111111111}}_e\\underbrace{\\boxed{0000000000000000000000000000000000000000000000000000}}_m\\] In other words, \\[ x = \\big(-1\\big)^{s}.2^{e-1023}.(1 + m/(2^{52})) \\] The exponent bits need some explanation. It is biased by 1023, in other words a bit value of 1023 represents an exponent of zero. It also has two special values that are interpreted differently, namely 0 (subnormals) and 2047 (infinity and NaN), which we won't cover here. A value of 1 represents an exponent of \\(2^{-1022}\\) or approximately \\(2.23\\times10^{-308}\\) and a value of 2046 is \\(2^{1023}\\) or approximately \\(8.99\\times10^{307}\\) . Thus the format can represent number over a range of well over 600 decimal orders of magnitude. The mantissa is interpreted as a binary number in the range \\(\\big[1,2\\big)\\) , with the leading 1 implied. The bits thus represent the fraction after the decimal point. This is why double precision is often referred to as having 53 bits of precision when the length of the mantissa is actually 52. Something of a moot point when one of those bits is fixed. The fact the mantissa is binary means that only rational numbers that have a denominator that's a power of two can be represented in a finite number of digits. This contrasts with base 10, which can represent rational numbers with a denominator that's a combination of powers of 2 and 5, since these are the prime factors of 10. So going back to the example above, we can see that neither the exact decimals \\(0.4 = 2/5\\) nor \\(0.1 = 1/(2\\times5)\\) can be represented exactly in binary, but \\(0.5=1/2\\) can. Here are the bit representations of the numbers in our expression: n bits 0.5 \\(\\boxed{0}\\boxed{01111111110}\\boxed{0000000000000000000000000000000000000000000000000000}\\) 0.4 \\(\\boxed{0}\\boxed{01111111101}\\boxed{1001100110011001100110011001100110011001100110011010}\\) 0.1 \\(\\boxed{1}\\boxed{01111111011}\\boxed{1001100110011001100110011001100110011001100110011010}\\) What's interesting is that for the second two, only the exponent is different (by 2). The mantissas are identical, and this makes perfect sense when you consider that 0.4 is exactly 4 times 0.1. So if we reorder the operations, Python 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> 0.5-(0.4+0.1) 0.0 we get the right answer. If you're that interested you can do it manually by first shifting the mantissa bits of 0.1 to the right 2 places and then add the two numbers (don't forget the implicit leading 1s) - the rounding errors cancel (to 52 bit precision) and you get exactly 0.5. Sadly this kind of intervention - reordering the expression based on the values - isn't going be a very practical generic solution, but it gives some insight and understanding of what's going on under the hood. Reproducibility \u00b6 Algorithms \u00b6 Typical operations that are \"numerically unstable\" include, but are in no way limited to computing a running sum of numbers - as the running total gets larger, so do the rounding errors. See Kahan summation for a solution to this. finite differencing - dividing small differences between numbers by another small number matrix inversions/solvers - for the reasons above Platform \u00b6 Ok, so you understand the limitations of floating point, but surely identical versions of your code will give the same (perhaps slightly wrong) answer wherever you run it? Wrong. Firstly the platform (i.e. OS) has an impact. This is typically due to the runtime libraries that your code relies on, and for numeric reproducibility we're talking about the maths library (e.g. libm.so ) and specifically how transcendental functions such as exp , log , etc are implemented. The IEEE754 standard doesn't (to my knowledge) mandate for full 52-bit accuracy (due to the performance tradeoff?) and so their accuracy is typically not guaranteed to be the full 52-bit width of the mantissa. This means that different implementations can give slightly different results. And even a difference in the least significant bit of the result can easily bubble up into a much larger error. I've seen it happen many times. Hardware \u00b6 Basic arithmetic operations aren't performed by a runtime library, they are implemented extremely efficiently in hardware . Traditionally (on x86) this was done by a x87 floating point coprocessor but is now largely done using vectorised SIMD ( single-instruction-multiple-data ) instructions. Things to note: the x87 uses an 80-bit internal representation of a double and thus can perform more accurate (internal) computations. SIMD floating-point instructions can operate simultaneously on multiple values but do not have the extra precision of the x87. SIMD implementations have gone from a 16-byte vector width (e.g. SSE2) to 32 (e.g. AVX) and even 64 (AVX512) on some hardware platforms. Newer SIMD implementations contain hardware implementations of some transcendental functions (e.g. exp , sqrt ), as well as reciprocals and fused multiply-add operations. The wider the vector width you use, the faster your code will run, typically. Many maths libraries (e.g. Intel's MKL ) contain multiple implementations of algorithms optimised for different architectures and will detect what hardware support is available, and use the best available, unless you explicitly force it to use a specific SIMD architecture. So there's potentially a hardware-software interaction that you may not be aware of. I'm going to replicate the behaviour of such a library using this C++ code that computes the dot product of a large vector: #include <vector> #include <algorithm> #include <iostream> #include <iomanip> double dot ( const double * a , const double * b , size_t n ) { double x = 0.0 ; // tell the compiler to unroll this loop if it can #pragma omp simd reduction(+:x) for ( size_t i = 0 ; i < n ; ++ i ) { x += a [ i ] * b [ i ]; } return x / n ; } int main () { size_t size = 512 * 1024 ; double h = 1.0 / size ; std :: vector < double > a ; size_t i = 0 ; // a values decreasing linearly from 1 to zero std :: generate_n ( std :: back_inserter ( a ), size , [ & ]{ return 1.0 - i ++* h ; }); // compute the inner product to self and display the result to full precision double x0 = dot ( a . data (), a . data (), size ); std :: cout << std :: setprecision ( 16 ) << x0 << std :: endl ; } The C++ compiler handily allows us to control what floating point instructions to use, so I can mimic the behaviour of a third-party maths library that selects the architecture at runtime: #!/bin/bash # use the non-vectorised floating-point unit g++ -O3 -g src/float.cpp -mfpmath = 387 -o bin/float-387 && echo -n x87: ; bin/float-387 # use 2 double width vector instructions g++ -O3 -g src/float.cpp -fopenmp -msse4.2 -o bin/float-sse4.2 && echo -n SSE4.2: ; bin/float-sse4.2 # use 4 double width vector instructions g++ -O3 -g src/float.cpp -fopenmp -mavx2 -o bin/float-avx2 && echo -n AVX2: ; bin/float-avx2 Here's the output: x87:0.3333342870082561 SSE4.2:0.3333342870067009 AVX2:0.3333342870071102 Why the differences \u00b6 This code is simply multiplying and adding. The problem really comes from the adding: as the running sum gets larger, successively adding small values involves greater and greater rounding errors. In the vectorised versions, the loop is unrolled and two or four running sums are maintained (which are themselves summed at the end) so the order of addition is different, and thus rounding errors are different. The differences are admittedly small, but if these were intermediate results, they could easily propagate to large differences in the end result. Interestingly, whilst the x87 implementation is potentially the most accurate (due to it's 80-bit internals) it's actually the least accurate because it's single running sum is prone to more rounding errors. True story \u00b6 The \"screaming derivatives trader\" was real (ok slightly exaggerated), but we were sometimes getting nonsensical risk numbers from a valuation model that was a nonlinear optimisation (requiring matrix inversions) of an objective function that was itself a Monte-Carlo simulation, and sensitivities calculated by finite-differencing results collated from various nodes on a large compute grid. And the point, as pointed out by XKCD, is that small errors in algorithms like these can accumulate into large ones in the results. Eventually, we managed to reproduce the problem and pinpoint two specific machines on the grid that consistently reproduced the problem. Trouble was, not only were they both running identical versions of the software, they were both running the same OS, and they even had identical hardware. The only difference was that one was virtualised, the other \"bare metal\". We were already (painfully) aware that hardware with different SIMD instruction sets could give different results, and that our grid had a range of cores of varying ages, and that we were using a highly optimised third party maths library that would use the best-available SIMD instructions on the hardware, unless otherwise directed, and I thought I had already solved this by forcing the library it to only use SIMD instructions that all of our cores supported (SSSE3 if I recall correctly). It turned out that the virtualisation layer was incorrectly reporting what SIMD architecture was on the machine, and so our (actually my) code didn't think it needed to force the issue. But when the code ran, it wasn't using the same SIMD implementation as the rest of the grid, and the risk numbers blew up as a result. Actually fixing this was trivial, but getting to this point - where we knew what on earth was going wrong - took a teeny bit more effort! In summary \u00b6 Achieving exact reproducibility in floating-point computations is hard. Reproducibility issues can arise from any of the software, the operating system, or even the hardware , depending on the way your software interacts with it. Using SIMD can give you big floating-point performance improvements but at the expense of not getting exactly the same answers. Often, you may not even know what CPU-level instructions you are using, and tracking down the source of differences can be tricky, so it pays to be aware of potential issues. Containers (docker) and virtual machines are no less as susceptible: whilst they allow you to ensure the software is exactly reproducible, you still have no control over what hardware they are run on.","title":"Floating-point (ir)reproducibity"},{"location":"float/#floating-point-irreproducibility","text":"I'm going to show that even if you have identical data, software and OS, you still can't guarantee reproducibility across instances, at least when it comes to floating-point calculations. XKCD sums it up quite nicely: Any software developer who writes numerical code needs to have some understanding of how floating-point numbers are represented, because they are a major source of software bugs and issues. Some people have a tendency to either assume the computer can't possibly get simple maths (slightly) wrong, or dismiss floating-point issues as rounding errors. Firstly, consider this motivating example of a very simple floating-point expression: Python 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> 0.5-0.4-0.1 -2.7755575615628914e-17 Feel free to try this in your language/platform of choice, chances are you will get the same wrong answer. So why can't a computer get such a simple sum correct? So you can assume it's a small and systematic (i.e easily reproducible) error and live with it, but if ( for example ) you have derivatives traders screaming at you because their risk numbers are all over the place and they can't hedge their positions, you can't just accept it. The problem is, floating-point errors don't always stay small, and they aren't always easily reproducible. I strongly recommend reading What Every Computer Scientist Should Know About Floating-Point Arithmetic . Now I'll try to explain the non-zero answer above.","title":"Floating-point (ir)reproducibility"},{"location":"float/#the-ieee754-standard","text":"Here we will only consider the double precision format, as it's by far the most widely used. Single, half and quad precision all suffer from the same issues. Double-precision floating-point numbers are represented in 64 bits as follows: a sign bit s , with zero representing positive and 1 negative. an 11-bit exponent e a 52-bit mantissa m The value 1 looks like this in binary: \\[\\underbrace{\\boxed{0}}_s\\underbrace{\\boxed{01111111111}}_e\\underbrace{\\boxed{0000000000000000000000000000000000000000000000000000}}_m\\] In other words, \\[ x = \\big(-1\\big)^{s}.2^{e-1023}.(1 + m/(2^{52})) \\] The exponent bits need some explanation. It is biased by 1023, in other words a bit value of 1023 represents an exponent of zero. It also has two special values that are interpreted differently, namely 0 (subnormals) and 2047 (infinity and NaN), which we won't cover here. A value of 1 represents an exponent of \\(2^{-1022}\\) or approximately \\(2.23\\times10^{-308}\\) and a value of 2046 is \\(2^{1023}\\) or approximately \\(8.99\\times10^{307}\\) . Thus the format can represent number over a range of well over 600 decimal orders of magnitude. The mantissa is interpreted as a binary number in the range \\(\\big[1,2\\big)\\) , with the leading 1 implied. The bits thus represent the fraction after the decimal point. This is why double precision is often referred to as having 53 bits of precision when the length of the mantissa is actually 52. Something of a moot point when one of those bits is fixed. The fact the mantissa is binary means that only rational numbers that have a denominator that's a power of two can be represented in a finite number of digits. This contrasts with base 10, which can represent rational numbers with a denominator that's a combination of powers of 2 and 5, since these are the prime factors of 10. So going back to the example above, we can see that neither the exact decimals \\(0.4 = 2/5\\) nor \\(0.1 = 1/(2\\times5)\\) can be represented exactly in binary, but \\(0.5=1/2\\) can. Here are the bit representations of the numbers in our expression: n bits 0.5 \\(\\boxed{0}\\boxed{01111111110}\\boxed{0000000000000000000000000000000000000000000000000000}\\) 0.4 \\(\\boxed{0}\\boxed{01111111101}\\boxed{1001100110011001100110011001100110011001100110011010}\\) 0.1 \\(\\boxed{1}\\boxed{01111111011}\\boxed{1001100110011001100110011001100110011001100110011010}\\) What's interesting is that for the second two, only the exponent is different (by 2). The mantissas are identical, and this makes perfect sense when you consider that 0.4 is exactly 4 times 0.1. So if we reorder the operations, Python 3.8.5 (default, Jul 28 2020, 12:59:40) [GCC 9.3.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> 0.5-(0.4+0.1) 0.0 we get the right answer. If you're that interested you can do it manually by first shifting the mantissa bits of 0.1 to the right 2 places and then add the two numbers (don't forget the implicit leading 1s) - the rounding errors cancel (to 52 bit precision) and you get exactly 0.5. Sadly this kind of intervention - reordering the expression based on the values - isn't going be a very practical generic solution, but it gives some insight and understanding of what's going on under the hood.","title":"The IEEE754 Standard"},{"location":"float/#reproducibility","text":"","title":"Reproducibility"},{"location":"float/#algorithms","text":"Typical operations that are \"numerically unstable\" include, but are in no way limited to computing a running sum of numbers - as the running total gets larger, so do the rounding errors. See Kahan summation for a solution to this. finite differencing - dividing small differences between numbers by another small number matrix inversions/solvers - for the reasons above","title":"Algorithms"},{"location":"float/#platform","text":"Ok, so you understand the limitations of floating point, but surely identical versions of your code will give the same (perhaps slightly wrong) answer wherever you run it? Wrong. Firstly the platform (i.e. OS) has an impact. This is typically due to the runtime libraries that your code relies on, and for numeric reproducibility we're talking about the maths library (e.g. libm.so ) and specifically how transcendental functions such as exp , log , etc are implemented. The IEEE754 standard doesn't (to my knowledge) mandate for full 52-bit accuracy (due to the performance tradeoff?) and so their accuracy is typically not guaranteed to be the full 52-bit width of the mantissa. This means that different implementations can give slightly different results. And even a difference in the least significant bit of the result can easily bubble up into a much larger error. I've seen it happen many times.","title":"Platform"},{"location":"float/#hardware","text":"Basic arithmetic operations aren't performed by a runtime library, they are implemented extremely efficiently in hardware . Traditionally (on x86) this was done by a x87 floating point coprocessor but is now largely done using vectorised SIMD ( single-instruction-multiple-data ) instructions. Things to note: the x87 uses an 80-bit internal representation of a double and thus can perform more accurate (internal) computations. SIMD floating-point instructions can operate simultaneously on multiple values but do not have the extra precision of the x87. SIMD implementations have gone from a 16-byte vector width (e.g. SSE2) to 32 (e.g. AVX) and even 64 (AVX512) on some hardware platforms. Newer SIMD implementations contain hardware implementations of some transcendental functions (e.g. exp , sqrt ), as well as reciprocals and fused multiply-add operations. The wider the vector width you use, the faster your code will run, typically. Many maths libraries (e.g. Intel's MKL ) contain multiple implementations of algorithms optimised for different architectures and will detect what hardware support is available, and use the best available, unless you explicitly force it to use a specific SIMD architecture. So there's potentially a hardware-software interaction that you may not be aware of. I'm going to replicate the behaviour of such a library using this C++ code that computes the dot product of a large vector: #include <vector> #include <algorithm> #include <iostream> #include <iomanip> double dot ( const double * a , const double * b , size_t n ) { double x = 0.0 ; // tell the compiler to unroll this loop if it can #pragma omp simd reduction(+:x) for ( size_t i = 0 ; i < n ; ++ i ) { x += a [ i ] * b [ i ]; } return x / n ; } int main () { size_t size = 512 * 1024 ; double h = 1.0 / size ; std :: vector < double > a ; size_t i = 0 ; // a values decreasing linearly from 1 to zero std :: generate_n ( std :: back_inserter ( a ), size , [ & ]{ return 1.0 - i ++* h ; }); // compute the inner product to self and display the result to full precision double x0 = dot ( a . data (), a . data (), size ); std :: cout << std :: setprecision ( 16 ) << x0 << std :: endl ; } The C++ compiler handily allows us to control what floating point instructions to use, so I can mimic the behaviour of a third-party maths library that selects the architecture at runtime: #!/bin/bash # use the non-vectorised floating-point unit g++ -O3 -g src/float.cpp -mfpmath = 387 -o bin/float-387 && echo -n x87: ; bin/float-387 # use 2 double width vector instructions g++ -O3 -g src/float.cpp -fopenmp -msse4.2 -o bin/float-sse4.2 && echo -n SSE4.2: ; bin/float-sse4.2 # use 4 double width vector instructions g++ -O3 -g src/float.cpp -fopenmp -mavx2 -o bin/float-avx2 && echo -n AVX2: ; bin/float-avx2 Here's the output: x87:0.3333342870082561 SSE4.2:0.3333342870067009 AVX2:0.3333342870071102","title":"Hardware"},{"location":"float/#why-the-differences","text":"This code is simply multiplying and adding. The problem really comes from the adding: as the running sum gets larger, successively adding small values involves greater and greater rounding errors. In the vectorised versions, the loop is unrolled and two or four running sums are maintained (which are themselves summed at the end) so the order of addition is different, and thus rounding errors are different. The differences are admittedly small, but if these were intermediate results, they could easily propagate to large differences in the end result. Interestingly, whilst the x87 implementation is potentially the most accurate (due to it's 80-bit internals) it's actually the least accurate because it's single running sum is prone to more rounding errors.","title":"Why the differences"},{"location":"float/#true-story","text":"The \"screaming derivatives trader\" was real (ok slightly exaggerated), but we were sometimes getting nonsensical risk numbers from a valuation model that was a nonlinear optimisation (requiring matrix inversions) of an objective function that was itself a Monte-Carlo simulation, and sensitivities calculated by finite-differencing results collated from various nodes on a large compute grid. And the point, as pointed out by XKCD, is that small errors in algorithms like these can accumulate into large ones in the results. Eventually, we managed to reproduce the problem and pinpoint two specific machines on the grid that consistently reproduced the problem. Trouble was, not only were they both running identical versions of the software, they were both running the same OS, and they even had identical hardware. The only difference was that one was virtualised, the other \"bare metal\". We were already (painfully) aware that hardware with different SIMD instruction sets could give different results, and that our grid had a range of cores of varying ages, and that we were using a highly optimised third party maths library that would use the best-available SIMD instructions on the hardware, unless otherwise directed, and I thought I had already solved this by forcing the library it to only use SIMD instructions that all of our cores supported (SSSE3 if I recall correctly). It turned out that the virtualisation layer was incorrectly reporting what SIMD architecture was on the machine, and so our (actually my) code didn't think it needed to force the issue. But when the code ran, it wasn't using the same SIMD implementation as the rest of the grid, and the risk numbers blew up as a result. Actually fixing this was trivial, but getting to this point - where we knew what on earth was going wrong - took a teeny bit more effort!","title":"True story"},{"location":"float/#in-summary","text":"Achieving exact reproducibility in floating-point computations is hard. Reproducibility issues can arise from any of the software, the operating system, or even the hardware , depending on the way your software interacts with it. Using SIMD can give you big floating-point performance improvements but at the expense of not getting exactly the same answers. Often, you may not even know what CPU-level instructions you are using, and tracking down the source of differences can be tricky, so it pays to be aware of potential issues. Containers (docker) and virtual machines are no less as susceptible: whilst they allow you to ensure the software is exactly reproducible, you still have no control over what hardware they are run on.","title":"In summary"},{"location":"sourdough/","text":"Sourdough Bread \u00b6 Refreshing the Mother \u00b6 Making the preferment \u00b6 (aka spoge) Making the dough \u00b6 Autolyse \u00b6 Knead \u00b6 Fold and First Proof \u00b6 Shape \u00b6 Overnight Retardation \u00b6 Bake \u00b6 Eat \u00b6","title":"Sourdough Bread"},{"location":"sourdough/#sourdough-bread","text":"","title":"Sourdough Bread"},{"location":"sourdough/#refreshing-the-mother","text":"","title":"Refreshing the Mother"},{"location":"sourdough/#making-the-preferment","text":"(aka spoge)","title":"Making the preferment"},{"location":"sourdough/#making-the-dough","text":"","title":"Making the dough"},{"location":"sourdough/#autolyse","text":"","title":"Autolyse"},{"location":"sourdough/#knead","text":"","title":"Knead"},{"location":"sourdough/#fold-and-first-proof","text":"","title":"Fold and First Proof"},{"location":"sourdough/#shape","text":"","title":"Shape"},{"location":"sourdough/#overnight-retardation","text":"","title":"Overnight Retardation"},{"location":"sourdough/#bake","text":"","title":"Bake"},{"location":"sourdough/#eat","text":"","title":"Eat"},{"location":"output/fib_generator_table/","text":"n F(n) Time(ms) 10 55 0.00524521 20 6765 0.00333786 30 832040 0.00429153 40 102334155 0.00476837 50 12586269025 0.00572205 60 1548008755920 0.00691414 70 190392490709135 0.00762939 80 23416728348467685 0.00858307 90 2880067194370816120 0.00977516","title":"Fib generator table"},{"location":"output/fib_recursive_table/","text":"n F(n) Time(ms) 10 55 0.0851154 15 610 0.911951 20 6765 9.92441 25 75025 111.743 30 832040 1224.36","title":"Fib recursive table"},{"location":"output/hyper_exp_table/","text":"n pow(n,n) Time(ms) 4 256 0.310421 5 3125 2.46119 6 46656 30.9212 7 823543 311.033 8 16777216 4973.39 9 387420489 112083","title":"Hyper exp table"}]}